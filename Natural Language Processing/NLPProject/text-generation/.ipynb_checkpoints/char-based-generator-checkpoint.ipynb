{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import pickle\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import np_utils\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train on Wonderland"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'data/wonderland.txt'\n",
    "with open(filename, 'r') as f:\n",
    "    raw_text = f.read().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_char_int(raw_text):\n",
    "    chars = sorted(list(set(raw_text)))\n",
    "    char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
    "    int_to_char = dict((i, c) for i, c in enumerate(chars))\n",
    "    nchars = len(raw_text)\n",
    "    nunique_chars = len(chars)\n",
    "    print(\"Total characters: \", nchars)\n",
    "    print(\"Total unique characters: \", nunique_chars)\n",
    "    return chars, char_to_int, int_to_char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total characters:  144412\n",
      "Total unique characters:  47\n"
     ]
    }
   ],
   "source": [
    "chars, char_to_int, int_to_char = map_char_int(raw_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use * to represent unknown character\n",
    "char_to_int['*']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19904\n",
      "2609\n"
     ]
    }
   ],
   "source": [
    "# stopWords = set(stopwords.words('english'))\n",
    "# raw_words = word_tokenize(raw_text)\n",
    "# wordsFiltered = []\n",
    " \n",
    "# for w in raw_words:\n",
    "#     if w not in stopWords:\n",
    "#         wordsFiltered.append(w)\n",
    "\n",
    "# print(len(wordsFiltered))\n",
    "# print(len(np.unique(wordsFiltered)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_feature(raw_text, char_to_int, len_seq=100, return_y=True):\n",
    "    nchars = len(raw_text)\n",
    "    nunique_chars = len(char_to_int)\n",
    "    X = []\n",
    "    if return_y: y = []\n",
    "    for i in range(nchars - len_seq):\n",
    "        seq_X = raw_text[i : i + len_seq]\n",
    "        X.append([char_to_int.get(c, 5) for c in seq_X])\n",
    "        if return_y:\n",
    "            label = raw_text[i + len_seq]\n",
    "            y.append(char_to_int[label])\n",
    "    nsamples = len(X)\n",
    "    X = np.reshape(X, (nsamples, len_seq, 1))\n",
    "    X = X / nunique_chars\n",
    "    if return_y: \n",
    "        y = np_utils.to_categorical(y)\n",
    "        return X, y\n",
    "    else:\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(input_shape, output_shape, small=True):\n",
    "    model = Sequential()\n",
    "    if small:\n",
    "        model.add(LSTM(256, input_shape=input_shape))\n",
    "        model.add(Dropout(0.2))\n",
    "    else:\n",
    "        model.add(LSTM(256, input_shape=input_shape, return_sequences=True))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(LSTM(256))\n",
    "        model.add(Dropout(0.2))\n",
    "    model.add(Dense(output_shape, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 256)               264192    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 47)                12079     \n",
      "=================================================================\n",
      "Total params: 276,271\n",
      "Trainable params: 276,271\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "X, y = text_to_feature(raw_text, char_to_int, len_seq=100, return_y=True)\n",
    "input_shape = (X.shape[1], X.shape[2])\n",
    "output_shape = y.shape[1]\n",
    "small_model = make_model(input_shape, output_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the checkpoint\n",
    "filepath=\"build/char-based-weights-{epoch:02d}-{loss:.4f}-simple-small.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "144312/144312 [==============================] - 310s 2ms/step - loss: 2.9326\n",
      "\n",
      "Epoch 00001: loss improved from inf to 2.93260, saving model to build/char-based-weights-01-2.9326-simple-small.hdf5\n",
      "Epoch 2/50\n",
      "144312/144312 [==============================] - 322s 2ms/step - loss: 2.7033\n",
      "\n",
      "Epoch 00002: loss improved from 2.93260 to 2.70334, saving model to build/char-based-weights-02-2.7033-simple-small.hdf5\n",
      "Epoch 3/50\n",
      "144312/144312 [==============================] - 314s 2ms/step - loss: 2.5928\n",
      "\n",
      "Epoch 00003: loss improved from 2.70334 to 2.59277, saving model to build/char-based-weights-03-2.5928-simple-small.hdf5\n",
      "Epoch 4/50\n",
      "144312/144312 [==============================] - 308s 2ms/step - loss: 2.5046\n",
      "\n",
      "Epoch 00004: loss improved from 2.59277 to 2.50462, saving model to build/char-based-weights-04-2.5046-simple-small.hdf5\n",
      "Epoch 5/50\n",
      "144312/144312 [==============================] - 305s 2ms/step - loss: 2.4456\n",
      "\n",
      "Epoch 00005: loss improved from 2.50462 to 2.44563, saving model to build/char-based-weights-05-2.4456-simple-small.hdf5\n",
      "Epoch 6/50\n",
      "144312/144312 [==============================] - 294s 2ms/step - loss: 2.3639\n",
      "\n",
      "Epoch 00006: loss improved from 2.44563 to 2.36391, saving model to build/char-based-weights-06-2.3639-simple-small.hdf5\n",
      "Epoch 7/50\n",
      "144312/144312 [==============================] - 291s 2ms/step - loss: 2.3017\n",
      "\n",
      "Epoch 00007: loss improved from 2.36391 to 2.30173, saving model to build/char-based-weights-07-2.3017-simple-small.hdf5\n",
      "Epoch 8/50\n",
      "144312/144312 [==============================] - 287s 2ms/step - loss: 2.2494\n",
      "\n",
      "Epoch 00008: loss improved from 2.30173 to 2.24936, saving model to build/char-based-weights-08-2.2494-simple-small.hdf5\n",
      "Epoch 9/50\n",
      "144312/144312 [==============================] - 283s 2ms/step - loss: 2.1877\n",
      "\n",
      "Epoch 00009: loss improved from 2.24936 to 2.18768, saving model to build/char-based-weights-09-2.1877-simple-small.hdf5\n",
      "Epoch 10/50\n",
      "144312/144312 [==============================] - 281s 2ms/step - loss: 2.1556\n",
      "\n",
      "Epoch 00010: loss improved from 2.18768 to 2.15559, saving model to build/char-based-weights-10-2.1556-simple-small.hdf5\n",
      "Epoch 11/50\n",
      "144312/144312 [==============================] - 276s 2ms/step - loss: 2.1224\n",
      "\n",
      "Epoch 00011: loss improved from 2.15559 to 2.12243, saving model to build/char-based-weights-11-2.1224-simple-small.hdf5\n",
      "Epoch 12/50\n",
      "144312/144312 [==============================] - 273s 2ms/step - loss: 2.0888\n",
      "\n",
      "Epoch 00012: loss improved from 2.12243 to 2.08882, saving model to build/char-based-weights-12-2.0888-simple-small.hdf5\n",
      "Epoch 13/50\n",
      "144312/144312 [==============================] - 270s 2ms/step - loss: 2.0520\n",
      "\n",
      "Epoch 00013: loss improved from 2.08882 to 2.05196, saving model to build/char-based-weights-13-2.0520-simple-small.hdf5\n",
      "Epoch 14/50\n",
      "144312/144312 [==============================] - 270s 2ms/step - loss: 2.0133\n",
      "\n",
      "Epoch 00014: loss improved from 2.05196 to 2.01325, saving model to build/char-based-weights-14-2.0133-simple-small.hdf5\n",
      "Epoch 15/50\n",
      "144312/144312 [==============================] - 265s 2ms/step - loss: 1.9779\n",
      "\n",
      "Epoch 00015: loss improved from 2.01325 to 1.97787, saving model to build/char-based-weights-15-1.9779-simple-small.hdf5\n",
      "Epoch 16/50\n",
      "144312/144312 [==============================] - 258s 2ms/step - loss: 1.9436\n",
      "\n",
      "Epoch 00016: loss improved from 1.97787 to 1.94362, saving model to build/char-based-weights-16-1.9436-simple-small.hdf5\n",
      "Epoch 17/50\n",
      "144312/144312 [==============================] - 254s 2ms/step - loss: 1.9121\n",
      "\n",
      "Epoch 00017: loss improved from 1.94362 to 1.91211, saving model to build/char-based-weights-17-1.9121-simple-small.hdf5\n",
      "Epoch 18/50\n",
      "144312/144312 [==============================] - 249s 2ms/step - loss: 1.8845\n",
      "\n",
      "Epoch 00018: loss improved from 1.91211 to 1.88448, saving model to build/char-based-weights-18-1.8845-simple-small.hdf5\n",
      "Epoch 19/50\n",
      "144312/144312 [==============================] - 240s 2ms/step - loss: 1.8590\n",
      "\n",
      "Epoch 00019: loss improved from 1.88448 to 1.85902, saving model to build/char-based-weights-19-1.8590-simple-small.hdf5\n",
      "Epoch 20/50\n",
      "144312/144312 [==============================] - 240s 2ms/step - loss: 1.8326\n",
      "\n",
      "Epoch 00020: loss improved from 1.85902 to 1.83262, saving model to build/char-based-weights-20-1.8326-simple-small.hdf5\n",
      "Epoch 21/50\n",
      "144312/144312 [==============================] - 243s 2ms/step - loss: 1.8093\n",
      "\n",
      "Epoch 00021: loss improved from 1.83262 to 1.80935, saving model to build/char-based-weights-21-1.8093-simple-small.hdf5\n",
      "Epoch 22/50\n",
      "144312/144312 [==============================] - 239s 2ms/step - loss: 1.7895\n",
      "\n",
      "Epoch 00022: loss improved from 1.80935 to 1.78945, saving model to build/char-based-weights-22-1.7895-simple-small.hdf5\n",
      "Epoch 23/50\n",
      "144312/144312 [==============================] - 239s 2ms/step - loss: 1.7690\n",
      "\n",
      "Epoch 00023: loss improved from 1.78945 to 1.76897, saving model to build/char-based-weights-23-1.7690-simple-small.hdf5\n",
      "Epoch 24/50\n",
      "144312/144312 [==============================] - 238s 2ms/step - loss: 1.7515\n",
      "\n",
      "Epoch 00024: loss improved from 1.76897 to 1.75155, saving model to build/char-based-weights-24-1.7515-simple-small.hdf5\n",
      "Epoch 25/50\n",
      "144312/144312 [==============================] - 240s 2ms/step - loss: 1.7333\n",
      "\n",
      "Epoch 00025: loss improved from 1.75155 to 1.73333, saving model to build/char-based-weights-25-1.7333-simple-small.hdf5\n",
      "Epoch 26/50\n",
      "144312/144312 [==============================] - 239s 2ms/step - loss: 1.7188\n",
      "\n",
      "Epoch 00026: loss improved from 1.73333 to 1.71876, saving model to build/char-based-weights-26-1.7188-simple-small.hdf5\n",
      "Epoch 27/50\n",
      "144312/144312 [==============================] - 238s 2ms/step - loss: 1.7042\n",
      "\n",
      "Epoch 00027: loss improved from 1.71876 to 1.70416, saving model to build/char-based-weights-27-1.7042-simple-small.hdf5\n",
      "Epoch 28/50\n",
      "144312/144312 [==============================] - 238s 2ms/step - loss: 1.7473\n",
      "\n",
      "Epoch 00028: loss did not improve from 1.70416\n",
      "Epoch 29/50\n",
      "144312/144312 [==============================] - 239s 2ms/step - loss: 1.6695\n",
      "\n",
      "Epoch 00029: loss improved from 1.70416 to 1.66953, saving model to build/char-based-weights-29-1.6695-simple-small.hdf5\n",
      "Epoch 30/50\n",
      "144312/144312 [==============================] - 240s 2ms/step - loss: 1.6506\n",
      "\n",
      "Epoch 00030: loss improved from 1.66953 to 1.65061, saving model to build/char-based-weights-30-1.6506-simple-small.hdf5\n",
      "Epoch 31/50\n",
      "144312/144312 [==============================] - 239s 2ms/step - loss: 1.6620\n",
      "\n",
      "Epoch 00031: loss did not improve from 1.65061\n",
      "Epoch 32/50\n",
      "144312/144312 [==============================] - 240s 2ms/step - loss: 2.8156\n",
      "\n",
      "Epoch 00032: loss did not improve from 1.65061\n",
      "Epoch 33/50\n",
      "144312/144312 [==============================] - 239s 2ms/step - loss: 2.6474\n",
      "\n",
      "Epoch 00033: loss did not improve from 1.65061\n",
      "Epoch 34/50\n",
      "144312/144312 [==============================] - 241s 2ms/step - loss: 2.4851\n",
      "\n",
      "Epoch 00034: loss did not improve from 1.65061\n",
      "Epoch 35/50\n",
      "144312/144312 [==============================] - 239s 2ms/step - loss: 2.2497\n",
      "\n",
      "Epoch 00035: loss did not improve from 1.65061\n",
      "Epoch 36/50\n",
      "144312/144312 [==============================] - 242s 2ms/step - loss: 1.9785\n",
      "\n",
      "Epoch 00036: loss did not improve from 1.65061\n",
      "Epoch 37/50\n",
      "144312/144312 [==============================] - 241s 2ms/step - loss: 1.8888\n",
      "\n",
      "Epoch 00037: loss did not improve from 1.65061\n",
      "Epoch 38/50\n",
      "144312/144312 [==============================] - 241s 2ms/step - loss: 2.6669\n",
      "\n",
      "Epoch 00038: loss did not improve from 1.65061\n",
      "Epoch 39/50\n",
      "144312/144312 [==============================] - 240s 2ms/step - loss: 2.8553\n",
      "\n",
      "Epoch 00039: loss did not improve from 1.65061\n",
      "Epoch 40/50\n",
      "144312/144312 [==============================] - 241s 2ms/step - loss: 2.7181\n",
      "\n",
      "Epoch 00040: loss did not improve from 1.65061\n",
      "Epoch 41/50\n",
      "144312/144312 [==============================] - 240s 2ms/step - loss: 2.6268\n",
      "\n",
      "Epoch 00041: loss did not improve from 1.65061\n",
      "Epoch 42/50\n",
      "144312/144312 [==============================] - 243s 2ms/step - loss: 2.5344\n",
      "\n",
      "Epoch 00042: loss did not improve from 1.65061\n",
      "Epoch 43/50\n",
      "144312/144312 [==============================] - 239s 2ms/step - loss: 2.4353\n",
      "\n",
      "Epoch 00043: loss did not improve from 1.65061\n",
      "Epoch 44/50\n",
      "144312/144312 [==============================] - 240s 2ms/step - loss: 2.3361\n",
      "\n",
      "Epoch 00044: loss did not improve from 1.65061\n",
      "Epoch 45/50\n",
      "144312/144312 [==============================] - 240s 2ms/step - loss: 2.2468\n",
      "\n",
      "Epoch 00045: loss did not improve from 1.65061\n",
      "Epoch 46/50\n",
      "144312/144312 [==============================] - 240s 2ms/step - loss: 2.1613\n",
      "\n",
      "Epoch 00046: loss did not improve from 1.65061\n",
      "Epoch 47/50\n",
      "144312/144312 [==============================] - 240s 2ms/step - loss: 2.0948\n",
      "\n",
      "Epoch 00047: loss did not improve from 1.65061\n",
      "Epoch 48/50\n",
      "144312/144312 [==============================] - 239s 2ms/step - loss: 2.0349\n",
      "\n",
      "Epoch 00048: loss did not improve from 1.65061\n",
      "Epoch 49/50\n",
      "144312/144312 [==============================] - 243s 2ms/step - loss: 1.9828\n",
      "\n",
      "Epoch 00049: loss did not improve from 1.65061\n",
      "Epoch 50/50\n",
      "144312/144312 [==============================] - 241s 2ms/step - loss: 1.9388\n",
      "\n",
      "Epoch 00050: loss did not improve from 1.65061\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f99b26dbe80>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_model.fit(X, y, epochs=50, batch_size=64, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_2 (LSTM)                (None, 100, 256)          264192    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 100, 256)          0         \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 256)               525312    \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 47)                12079     \n",
      "=================================================================\n",
      "Total params: 801,583\n",
      "Trainable params: 801,583\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "144312/144312 [==============================] - 496s 3ms/step - loss: 2.8065\n",
      "\n",
      "Epoch 00001: loss improved from inf to 2.80647, saving model to build/char-based-weights-01-2.8065-simple-big.hdf5\n",
      "Epoch 2/50\n",
      "144312/144312 [==============================] - 493s 3ms/step - loss: 2.4289\n",
      "\n",
      "Epoch 00002: loss improved from 2.80647 to 2.42888, saving model to build/char-based-weights-02-2.4289-simple-big.hdf5\n",
      "Epoch 3/50\n",
      "144312/144312 [==============================] - 495s 3ms/step - loss: 2.2239\n",
      "\n",
      "Epoch 00003: loss improved from 2.42888 to 2.22393, saving model to build/char-based-weights-03-2.2239-simple-big.hdf5\n",
      "Epoch 4/50\n",
      "144312/144312 [==============================] - 491s 3ms/step - loss: 2.0868\n",
      "\n",
      "Epoch 00004: loss improved from 2.22393 to 2.08679, saving model to build/char-based-weights-04-2.0868-simple-big.hdf5\n",
      "Epoch 5/50\n",
      "144312/144312 [==============================] - 495s 3ms/step - loss: 1.9879\n",
      "\n",
      "Epoch 00005: loss improved from 2.08679 to 1.98787, saving model to build/char-based-weights-05-1.9879-simple-big.hdf5\n",
      "Epoch 6/50\n",
      "144312/144312 [==============================] - 495s 3ms/step - loss: 1.9105\n",
      "\n",
      "Epoch 00006: loss improved from 1.98787 to 1.91048, saving model to build/char-based-weights-06-1.9105-simple-big.hdf5\n",
      "Epoch 7/50\n",
      "144312/144312 [==============================] - 493s 3ms/step - loss: 1.8469\n",
      "\n",
      "Epoch 00007: loss improved from 1.91048 to 1.84685, saving model to build/char-based-weights-07-1.8469-simple-big.hdf5\n",
      "Epoch 8/50\n",
      "144312/144312 [==============================] - 493s 3ms/step - loss: 1.7920\n",
      "\n",
      "Epoch 00008: loss improved from 1.84685 to 1.79200, saving model to build/char-based-weights-08-1.7920-simple-big.hdf5\n",
      "Epoch 9/50\n",
      "144312/144312 [==============================] - 498s 3ms/step - loss: 1.7420\n",
      "\n",
      "Epoch 00009: loss improved from 1.79200 to 1.74205, saving model to build/char-based-weights-09-1.7420-simple-big.hdf5\n",
      "Epoch 10/50\n",
      "144312/144312 [==============================] - 500s 3ms/step - loss: 1.7032\n",
      "\n",
      "Epoch 00010: loss improved from 1.74205 to 1.70316, saving model to build/char-based-weights-10-1.7032-simple-big.hdf5\n",
      "Epoch 11/50\n",
      "144312/144312 [==============================] - 491s 3ms/step - loss: 1.6634\n",
      "\n",
      "Epoch 00011: loss improved from 1.70316 to 1.66335, saving model to build/char-based-weights-11-1.6634-simple-big.hdf5\n",
      "Epoch 12/50\n",
      "144312/144312 [==============================] - 481s 3ms/step - loss: 1.6283\n",
      "\n",
      "Epoch 00012: loss improved from 1.66335 to 1.62829, saving model to build/char-based-weights-12-1.6283-simple-big.hdf5\n",
      "Epoch 13/50\n",
      "144312/144312 [==============================] - 485s 3ms/step - loss: 1.5999\n",
      "\n",
      "Epoch 00013: loss improved from 1.62829 to 1.59994, saving model to build/char-based-weights-13-1.5999-simple-big.hdf5\n",
      "Epoch 14/50\n",
      "144312/144312 [==============================] - 515s 4ms/step - loss: 1.5701\n",
      "\n",
      "Epoch 00014: loss improved from 1.59994 to 1.57012, saving model to build/char-based-weights-14-1.5701-simple-big.hdf5\n",
      "Epoch 15/50\n",
      "144312/144312 [==============================] - 553s 4ms/step - loss: 1.5464\n",
      "\n",
      "Epoch 00015: loss improved from 1.57012 to 1.54643, saving model to build/char-based-weights-15-1.5464-simple-big.hdf5\n",
      "Epoch 16/50\n",
      "144312/144312 [==============================] - 549s 4ms/step - loss: 1.5216\n",
      "\n",
      "Epoch 00016: loss improved from 1.54643 to 1.52161, saving model to build/char-based-weights-16-1.5216-simple-big.hdf5\n",
      "Epoch 17/50\n",
      "144312/144312 [==============================] - 556s 4ms/step - loss: 1.4991\n",
      "\n",
      "Epoch 00017: loss improved from 1.52161 to 1.49907, saving model to build/char-based-weights-17-1.4991-simple-big.hdf5\n",
      "Epoch 18/50\n",
      "144312/144312 [==============================] - 548s 4ms/step - loss: 1.4784\n",
      "\n",
      "Epoch 00018: loss improved from 1.49907 to 1.47845, saving model to build/char-based-weights-18-1.4784-simple-big.hdf5\n",
      "Epoch 19/50\n",
      "144312/144312 [==============================] - 545s 4ms/step - loss: 1.4643\n",
      "\n",
      "Epoch 00019: loss improved from 1.47845 to 1.46431, saving model to build/char-based-weights-19-1.4643-simple-big.hdf5\n",
      "Epoch 20/50\n",
      "144312/144312 [==============================] - 540s 4ms/step - loss: 1.4433\n",
      "\n",
      "Epoch 00020: loss improved from 1.46431 to 1.44333, saving model to build/char-based-weights-20-1.4433-simple-big.hdf5\n",
      "Epoch 21/50\n",
      "144312/144312 [==============================] - 544s 4ms/step - loss: 1.4276\n",
      "\n",
      "Epoch 00021: loss improved from 1.44333 to 1.42759, saving model to build/char-based-weights-21-1.4276-simple-big.hdf5\n",
      "Epoch 22/50\n",
      "144312/144312 [==============================] - 544s 4ms/step - loss: 1.4150\n",
      "\n",
      "Epoch 00022: loss improved from 1.42759 to 1.41504, saving model to build/char-based-weights-22-1.4150-simple-big.hdf5\n",
      "Epoch 23/50\n",
      "144312/144312 [==============================] - 540s 4ms/step - loss: 1.4006\n",
      "\n",
      "Epoch 00023: loss improved from 1.41504 to 1.40062, saving model to build/char-based-weights-23-1.4006-simple-big.hdf5\n",
      "Epoch 24/50\n",
      "144312/144312 [==============================] - 536s 4ms/step - loss: 1.3899\n",
      "\n",
      "Epoch 00024: loss improved from 1.40062 to 1.38992, saving model to build/char-based-weights-24-1.3899-simple-big.hdf5\n",
      "Epoch 25/50\n",
      "144312/144312 [==============================] - 537s 4ms/step - loss: 1.3738\n",
      "\n",
      "Epoch 00025: loss improved from 1.38992 to 1.37382, saving model to build/char-based-weights-25-1.3738-simple-big.hdf5\n",
      "Epoch 26/50\n",
      "144312/144312 [==============================] - 537s 4ms/step - loss: 1.3666\n",
      "\n",
      "Epoch 00026: loss improved from 1.37382 to 1.36664, saving model to build/char-based-weights-26-1.3666-simple-big.hdf5\n",
      "Epoch 27/50\n",
      "144312/144312 [==============================] - 536s 4ms/step - loss: 1.3573\n",
      "\n",
      "Epoch 00027: loss improved from 1.36664 to 1.35727, saving model to build/char-based-weights-27-1.3573-simple-big.hdf5\n",
      "Epoch 28/50\n",
      "144312/144312 [==============================] - 535s 4ms/step - loss: 1.3433\n",
      "\n",
      "Epoch 00028: loss improved from 1.35727 to 1.34327, saving model to build/char-based-weights-28-1.3433-simple-big.hdf5\n",
      "Epoch 29/50\n",
      "144312/144312 [==============================] - 534s 4ms/step - loss: 1.3347\n",
      "\n",
      "Epoch 00029: loss improved from 1.34327 to 1.33466, saving model to build/char-based-weights-29-1.3347-simple-big.hdf5\n",
      "Epoch 30/50\n",
      "144312/144312 [==============================] - 527s 4ms/step - loss: 1.3256\n",
      "\n",
      "Epoch 00030: loss improved from 1.33466 to 1.32555, saving model to build/char-based-weights-30-1.3256-simple-big.hdf5\n",
      "Epoch 31/50\n",
      "144312/144312 [==============================] - 514s 4ms/step - loss: 1.3208\n",
      "\n",
      "Epoch 00031: loss improved from 1.32555 to 1.32081, saving model to build/char-based-weights-31-1.3208-simple-big.hdf5\n",
      "Epoch 32/50\n",
      "144312/144312 [==============================] - 530s 4ms/step - loss: 1.3134\n",
      "\n",
      "Epoch 00032: loss improved from 1.32081 to 1.31341, saving model to build/char-based-weights-32-1.3134-simple-big.hdf5\n",
      "Epoch 33/50\n",
      "144312/144312 [==============================] - 521s 4ms/step - loss: 1.3096\n",
      "\n",
      "Epoch 00033: loss improved from 1.31341 to 1.30957, saving model to build/char-based-weights-33-1.3096-simple-big.hdf5\n",
      "Epoch 34/50\n",
      "144312/144312 [==============================] - 515s 4ms/step - loss: 1.2995\n",
      "\n",
      "Epoch 00034: loss improved from 1.30957 to 1.29947, saving model to build/char-based-weights-34-1.2995-simple-big.hdf5\n",
      "Epoch 35/50\n",
      "144312/144312 [==============================] - 511s 4ms/step - loss: 1.2937\n",
      "\n",
      "Epoch 00035: loss improved from 1.29947 to 1.29373, saving model to build/char-based-weights-35-1.2937-simple-big.hdf5\n",
      "Epoch 36/50\n",
      "144312/144312 [==============================] - 503s 3ms/step - loss: 1.2909\n",
      "\n",
      "Epoch 00036: loss improved from 1.29373 to 1.29088, saving model to build/char-based-weights-36-1.2909-simple-big.hdf5\n",
      "Epoch 37/50\n",
      "144312/144312 [==============================] - 495s 3ms/step - loss: 1.2857\n",
      "\n",
      "Epoch 00037: loss improved from 1.29088 to 1.28570, saving model to build/char-based-weights-37-1.2857-simple-big.hdf5\n",
      "Epoch 38/50\n",
      "144312/144312 [==============================] - 492s 3ms/step - loss: 1.2850\n",
      "\n",
      "Epoch 00038: loss improved from 1.28570 to 1.28502, saving model to build/char-based-weights-38-1.2850-simple-big.hdf5\n",
      "Epoch 39/50\n",
      "144312/144312 [==============================] - 493s 3ms/step - loss: 1.2718\n",
      "\n",
      "Epoch 00039: loss improved from 1.28502 to 1.27181, saving model to build/char-based-weights-39-1.2718-simple-big.hdf5\n",
      "Epoch 40/50\n",
      "144312/144312 [==============================] - 490s 3ms/step - loss: 1.2756\n",
      "\n",
      "Epoch 00040: loss did not improve from 1.27181\n",
      "Epoch 41/50\n",
      "144312/144312 [==============================] - 490s 3ms/step - loss: 1.2728\n",
      "\n",
      "Epoch 00041: loss did not improve from 1.27181\n",
      "Epoch 42/50\n",
      "144312/144312 [==============================] - 491s 3ms/step - loss: 1.2646\n",
      "\n",
      "Epoch 00042: loss improved from 1.27181 to 1.26456, saving model to build/char-based-weights-42-1.2646-simple-big.hdf5\n",
      "Epoch 43/50\n",
      "144312/144312 [==============================] - 491s 3ms/step - loss: 1.2613\n",
      "\n",
      "Epoch 00043: loss improved from 1.26456 to 1.26133, saving model to build/char-based-weights-43-1.2613-simple-big.hdf5\n",
      "Epoch 44/50\n",
      "144312/144312 [==============================] - 491s 3ms/step - loss: 1.2622\n",
      "\n",
      "Epoch 00044: loss did not improve from 1.26133\n",
      "Epoch 45/50\n",
      "144312/144312 [==============================] - 490s 3ms/step - loss: 1.2528\n",
      "\n",
      "Epoch 00045: loss improved from 1.26133 to 1.25277, saving model to build/char-based-weights-45-1.2528-simple-big.hdf5\n",
      "Epoch 46/50\n",
      "144312/144312 [==============================] - 488s 3ms/step - loss: 1.2560\n",
      "\n",
      "Epoch 00046: loss did not improve from 1.25277\n",
      "Epoch 47/50\n",
      "144312/144312 [==============================] - 492s 3ms/step - loss: 1.2569\n",
      "\n",
      "Epoch 00047: loss did not improve from 1.25277\n",
      "Epoch 48/50\n",
      "144312/144312 [==============================] - 487s 3ms/step - loss: 1.2468\n",
      "\n",
      "Epoch 00048: loss improved from 1.25277 to 1.24684, saving model to build/char-based-weights-48-1.2468-simple-big.hdf5\n",
      "Epoch 49/50\n",
      "144312/144312 [==============================] - 490s 3ms/step - loss: 1.2501\n",
      "\n",
      "Epoch 00049: loss did not improve from 1.24684\n",
      "Epoch 50/50\n",
      "144312/144312 [==============================] - 491s 3ms/step - loss: 1.2453\n",
      "\n",
      "Epoch 00050: loss improved from 1.24684 to 1.24532, saving model to build/char-based-weights-50-1.2453-simple-big.hdf5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f99a82fee10>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "big_model = make_model(input_shape, output_shape, small=False)\n",
    "# define the checkpoint\n",
    "filepath=\"build/char-based-weights-{epoch:02d}-{loss:.4f}-simple-big.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]\n",
    "big_model.fit(X, y, epochs=50, batch_size=64, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train on news snippet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'data/nyt-snippet-subsets.txt'\n",
    "with open(filename, 'r') as f:\n",
    "    raw_text = f.read().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total characters:  144462\n",
      "Total unique characters:  61\n"
     ]
    }
   ],
   "source": [
    "chars, char_to_int, int_to_char = map_char_int(raw_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sleep deprivation is linked to behavioral and mental health problems and car accident risk, experts \n"
     ]
    }
   ],
   "source": [
    "print(raw_text[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = text_to_feature(raw_text, char_to_int, len_seq=100, return_y=True)\n",
    "input_shape = (X.shape[1], X.shape[2])\n",
    "output_shape = y.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_2 (LSTM)                (None, 256)               264192    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 61)                15677     \n",
      "=================================================================\n",
      "Total params: 279,869\n",
      "Trainable params: 279,869\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "144362/144362 [==============================] - 287s 2ms/step - loss: 2.9321\n",
      "\n",
      "Epoch 00001: loss improved from inf to 2.93208, saving model to build/char-based-weights-01-2.9321-complex-big.hdf5\n",
      "Epoch 2/50\n",
      "144362/144362 [==============================] - 286s 2ms/step - loss: 2.8166\n",
      "\n",
      "Epoch 00002: loss improved from 2.93208 to 2.81655, saving model to build/char-based-weights-02-2.8166-complex-big.hdf5\n",
      "Epoch 3/50\n",
      "144362/144362 [==============================] - 284s 2ms/step - loss: 2.7769\n",
      "\n",
      "Epoch 00003: loss improved from 2.81655 to 2.77693, saving model to build/char-based-weights-03-2.7769-complex-big.hdf5\n",
      "Epoch 4/50\n",
      "144362/144362 [==============================] - 274s 2ms/step - loss: 2.7431\n",
      "\n",
      "Epoch 00004: loss improved from 2.77693 to 2.74307, saving model to build/char-based-weights-04-2.7431-complex-big.hdf5\n",
      "Epoch 5/50\n",
      "144362/144362 [==============================] - 279s 2ms/step - loss: 2.7113\n",
      "\n",
      "Epoch 00005: loss improved from 2.74307 to 2.71131, saving model to build/char-based-weights-05-2.7113-complex-big.hdf5\n",
      "Epoch 6/50\n",
      "144362/144362 [==============================] - 277s 2ms/step - loss: 2.6829\n",
      "\n",
      "Epoch 00006: loss improved from 2.71131 to 2.68292, saving model to build/char-based-weights-06-2.6829-complex-big.hdf5\n",
      "Epoch 7/50\n",
      "144362/144362 [==============================] - 269s 2ms/step - loss: 2.6526\n",
      "\n",
      "Epoch 00007: loss improved from 2.68292 to 2.65260, saving model to build/char-based-weights-07-2.6526-complex-big.hdf5\n",
      "Epoch 8/50\n",
      "144362/144362 [==============================] - 269s 2ms/step - loss: 2.6233\n",
      "\n",
      "Epoch 00008: loss improved from 2.65260 to 2.62329, saving model to build/char-based-weights-08-2.6233-complex-big.hdf5\n",
      "Epoch 9/50\n",
      "144362/144362 [==============================] - 268s 2ms/step - loss: 2.5946\n",
      "\n",
      "Epoch 00009: loss improved from 2.62329 to 2.59461, saving model to build/char-based-weights-09-2.5946-complex-big.hdf5\n",
      "Epoch 10/50\n",
      "144362/144362 [==============================] - 266s 2ms/step - loss: 2.5676\n",
      "\n",
      "Epoch 00010: loss improved from 2.59461 to 2.56760, saving model to build/char-based-weights-10-2.5676-complex-big.hdf5\n",
      "Epoch 11/50\n",
      "144362/144362 [==============================] - 264s 2ms/step - loss: 2.5382\n",
      "\n",
      "Epoch 00011: loss improved from 2.56760 to 2.53823, saving model to build/char-based-weights-11-2.5382-complex-big.hdf5\n",
      "Epoch 12/50\n",
      "144362/144362 [==============================] - 263s 2ms/step - loss: 2.5096\n",
      "\n",
      "Epoch 00012: loss improved from 2.53823 to 2.50960, saving model to build/char-based-weights-12-2.5096-complex-big.hdf5\n",
      "Epoch 13/50\n",
      "144362/144362 [==============================] - 257s 2ms/step - loss: 2.4792\n",
      "\n",
      "Epoch 00013: loss improved from 2.50960 to 2.47916, saving model to build/char-based-weights-13-2.4792-complex-big.hdf5\n",
      "Epoch 14/50\n",
      "144362/144362 [==============================] - 255s 2ms/step - loss: 2.4526\n",
      "\n",
      "Epoch 00014: loss improved from 2.47916 to 2.45264, saving model to build/char-based-weights-14-2.4526-complex-big.hdf5\n",
      "Epoch 15/50\n",
      "144362/144362 [==============================] - 248s 2ms/step - loss: 2.4225\n",
      "\n",
      "Epoch 00015: loss improved from 2.45264 to 2.42246, saving model to build/char-based-weights-15-2.4225-complex-big.hdf5\n",
      "Epoch 16/50\n",
      "144362/144362 [==============================] - 243s 2ms/step - loss: 2.3952\n",
      "\n",
      "Epoch 00016: loss improved from 2.42246 to 2.39517, saving model to build/char-based-weights-16-2.3952-complex-big.hdf5\n",
      "Epoch 17/50\n",
      "144362/144362 [==============================] - 240s 2ms/step - loss: 2.3686\n",
      "\n",
      "Epoch 00017: loss improved from 2.39517 to 2.36857, saving model to build/char-based-weights-17-2.3686-complex-big.hdf5\n",
      "Epoch 18/50\n",
      "144362/144362 [==============================] - 241s 2ms/step - loss: 2.3416\n",
      "\n",
      "Epoch 00018: loss improved from 2.36857 to 2.34158, saving model to build/char-based-weights-18-2.3416-complex-big.hdf5\n",
      "Epoch 19/50\n",
      "144362/144362 [==============================] - 245s 2ms/step - loss: 2.3172\n",
      "\n",
      "Epoch 00019: loss improved from 2.34158 to 2.31721, saving model to build/char-based-weights-19-2.3172-complex-big.hdf5\n",
      "Epoch 20/50\n",
      "144362/144362 [==============================] - 243s 2ms/step - loss: 2.2944\n",
      "\n",
      "Epoch 00020: loss improved from 2.31721 to 2.29443, saving model to build/char-based-weights-20-2.2944-complex-big.hdf5\n",
      "Epoch 21/50\n",
      "144362/144362 [==============================] - 239s 2ms/step - loss: 2.2707\n",
      "\n",
      "Epoch 00021: loss improved from 2.29443 to 2.27072, saving model to build/char-based-weights-21-2.2707-complex-big.hdf5\n",
      "Epoch 22/50\n",
      "144362/144362 [==============================] - 249s 2ms/step - loss: 2.2503\n",
      "\n",
      "Epoch 00022: loss improved from 2.27072 to 2.25032, saving model to build/char-based-weights-22-2.2503-complex-big.hdf5\n",
      "Epoch 23/50\n",
      "144362/144362 [==============================] - 246s 2ms/step - loss: 2.2294\n",
      "\n",
      "Epoch 00023: loss improved from 2.25032 to 2.22939, saving model to build/char-based-weights-23-2.2294-complex-big.hdf5\n",
      "Epoch 24/50\n",
      "144362/144362 [==============================] - 249s 2ms/step - loss: 2.2135\n",
      "\n",
      "Epoch 00024: loss improved from 2.22939 to 2.21354, saving model to build/char-based-weights-24-2.2135-complex-big.hdf5\n",
      "Epoch 25/50\n",
      "144362/144362 [==============================] - 248s 2ms/step - loss: 2.1949\n",
      "\n",
      "Epoch 00025: loss improved from 2.21354 to 2.19493, saving model to build/char-based-weights-25-2.1949-complex-big.hdf5\n",
      "Epoch 26/50\n",
      "144362/144362 [==============================] - 249s 2ms/step - loss: 2.1780\n",
      "\n",
      "Epoch 00026: loss improved from 2.19493 to 2.17799, saving model to build/char-based-weights-26-2.1780-complex-big.hdf5\n",
      "Epoch 27/50\n",
      "144362/144362 [==============================] - 242s 2ms/step - loss: 2.1617\n",
      "\n",
      "Epoch 00027: loss improved from 2.17799 to 2.16172, saving model to build/char-based-weights-27-2.1617-complex-big.hdf5\n",
      "Epoch 28/50\n",
      "144362/144362 [==============================] - 244s 2ms/step - loss: 2.1517\n",
      "\n",
      "Epoch 00028: loss improved from 2.16172 to 2.15172, saving model to build/char-based-weights-28-2.1517-complex-big.hdf5\n",
      "Epoch 29/50\n",
      "144362/144362 [==============================] - 245s 2ms/step - loss: 2.1298\n",
      "\n",
      "Epoch 00029: loss improved from 2.15172 to 2.12980, saving model to build/char-based-weights-29-2.1298-complex-big.hdf5\n",
      "Epoch 30/50\n",
      "144362/144362 [==============================] - 247s 2ms/step - loss: 2.1241\n",
      "\n",
      "Epoch 00030: loss improved from 2.12980 to 2.12414, saving model to build/char-based-weights-30-2.1241-complex-big.hdf5\n",
      "Epoch 31/50\n",
      "144362/144362 [==============================] - 247s 2ms/step - loss: 2.1124\n",
      "\n",
      "Epoch 00031: loss improved from 2.12414 to 2.11244, saving model to build/char-based-weights-31-2.1124-complex-big.hdf5\n",
      "Epoch 32/50\n",
      "144362/144362 [==============================] - 249s 2ms/step - loss: 2.1021\n",
      "\n",
      "Epoch 00032: loss improved from 2.11244 to 2.10208, saving model to build/char-based-weights-32-2.1021-complex-big.hdf5\n",
      "Epoch 33/50\n",
      "144362/144362 [==============================] - 242s 2ms/step - loss: 2.0898\n",
      "\n",
      "Epoch 00033: loss improved from 2.10208 to 2.08977, saving model to build/char-based-weights-33-2.0898-complex-big.hdf5\n",
      "Epoch 34/50\n",
      "144362/144362 [==============================] - 243s 2ms/step - loss: 2.0772\n",
      "\n",
      "Epoch 00034: loss improved from 2.08977 to 2.07723, saving model to build/char-based-weights-34-2.0772-complex-big.hdf5\n",
      "Epoch 35/50\n",
      "144362/144362 [==============================] - 238s 2ms/step - loss: 2.0667\n",
      "\n",
      "Epoch 00035: loss improved from 2.07723 to 2.06671, saving model to build/char-based-weights-35-2.0667-complex-big.hdf5\n",
      "Epoch 36/50\n",
      "144362/144362 [==============================] - 242s 2ms/step - loss: 2.0576\n",
      "\n",
      "Epoch 00036: loss improved from 2.06671 to 2.05758, saving model to build/char-based-weights-36-2.0576-complex-big.hdf5\n",
      "Epoch 37/50\n",
      "144362/144362 [==============================] - 247s 2ms/step - loss: 2.0472\n",
      "\n",
      "Epoch 00037: loss improved from 2.05758 to 2.04718, saving model to build/char-based-weights-37-2.0472-complex-big.hdf5\n",
      "Epoch 38/50\n",
      "144362/144362 [==============================] - 248s 2ms/step - loss: 2.0382\n",
      "\n",
      "Epoch 00038: loss improved from 2.04718 to 2.03815, saving model to build/char-based-weights-38-2.0382-complex-big.hdf5\n",
      "Epoch 39/50\n",
      "144362/144362 [==============================] - 246s 2ms/step - loss: 2.0291\n",
      "\n",
      "Epoch 00039: loss improved from 2.03815 to 2.02912, saving model to build/char-based-weights-39-2.0291-complex-big.hdf5\n",
      "Epoch 40/50\n",
      "144362/144362 [==============================] - 247s 2ms/step - loss: 2.0206\n",
      "\n",
      "Epoch 00040: loss improved from 2.02912 to 2.02060, saving model to build/char-based-weights-40-2.0206-complex-big.hdf5\n",
      "Epoch 41/50\n",
      "144362/144362 [==============================] - 244s 2ms/step - loss: 2.0107\n",
      "\n",
      "Epoch 00041: loss improved from 2.02060 to 2.01073, saving model to build/char-based-weights-41-2.0107-complex-big.hdf5\n",
      "Epoch 42/50\n",
      "144362/144362 [==============================] - 248s 2ms/step - loss: 2.0053\n",
      "\n",
      "Epoch 00042: loss improved from 2.01073 to 2.00532, saving model to build/char-based-weights-42-2.0053-complex-big.hdf5\n",
      "Epoch 43/50\n",
      "144362/144362 [==============================] - 250s 2ms/step - loss: 1.9923\n",
      "\n",
      "Epoch 00043: loss improved from 2.00532 to 1.99228, saving model to build/char-based-weights-43-1.9923-complex-big.hdf5\n",
      "Epoch 44/50\n",
      "144362/144362 [==============================] - 248s 2ms/step - loss: 1.9919\n",
      "\n",
      "Epoch 00044: loss improved from 1.99228 to 1.99193, saving model to build/char-based-weights-44-1.9919-complex-big.hdf5\n",
      "Epoch 45/50\n",
      "144362/144362 [==============================] - 247s 2ms/step - loss: 1.9816\n",
      "\n",
      "Epoch 00045: loss improved from 1.99193 to 1.98160, saving model to build/char-based-weights-45-1.9816-complex-big.hdf5\n",
      "Epoch 46/50\n",
      "144362/144362 [==============================] - 247s 2ms/step - loss: 1.9760\n",
      "\n",
      "Epoch 00046: loss improved from 1.98160 to 1.97604, saving model to build/char-based-weights-46-1.9760-complex-big.hdf5\n",
      "Epoch 47/50\n",
      "144362/144362 [==============================] - 247s 2ms/step - loss: 1.9700\n",
      "\n",
      "Epoch 00047: loss improved from 1.97604 to 1.97002, saving model to build/char-based-weights-47-1.9700-complex-big.hdf5\n",
      "Epoch 48/50\n",
      "144362/144362 [==============================] - 246s 2ms/step - loss: 1.9602\n",
      "\n",
      "Epoch 00048: loss improved from 1.97002 to 1.96017, saving model to build/char-based-weights-48-1.9602-complex-big.hdf5\n",
      "Epoch 49/50\n",
      "144362/144362 [==============================] - 249s 2ms/step - loss: 1.9565\n",
      "\n",
      "Epoch 00049: loss improved from 1.96017 to 1.95647, saving model to build/char-based-weights-49-1.9565-complex-big.hdf5\n",
      "Epoch 50/50\n",
      "144362/144362 [==============================] - 247s 2ms/step - loss: 1.9461\n",
      "\n",
      "Epoch 00050: loss improved from 1.95647 to 1.94609, saving model to build/char-based-weights-50-1.9461-complex-big.hdf5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f336412e518>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_model = make_model(input_shape, output_shape)\n",
    "filepath=\"build/char-based-weights-{epoch:02d}-{loss:.4f}-complex-small.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]\n",
    "small_model.fit(X, y, epochs=50, batch_size=64, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_3 (LSTM)                (None, 100, 256)          264192    \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 100, 256)          0         \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 256)               525312    \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 61)                15677     \n",
      "=================================================================\n",
      "Total params: 805,181\n",
      "Trainable params: 805,181\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "144362/144362 [==============================] - 508s 4ms/step - loss: 2.8770\n",
      "\n",
      "Epoch 00001: loss improved from inf to 2.87695, saving model to build/char-based-weights-01-2.8770-complex-big.hdf5\n",
      "Epoch 2/50\n",
      "144362/144362 [==============================] - 505s 3ms/step - loss: 2.6869\n",
      "\n",
      "Epoch 00002: loss improved from 2.87695 to 2.68688, saving model to build/char-based-weights-02-2.6869-complex-big.hdf5\n",
      "Epoch 3/50\n",
      "144362/144362 [==============================] - 503s 3ms/step - loss: 2.5799\n",
      "\n",
      "Epoch 00003: loss improved from 2.68688 to 2.57991, saving model to build/char-based-weights-03-2.5799-complex-big.hdf5\n",
      "Epoch 4/50\n",
      "144362/144362 [==============================] - 503s 3ms/step - loss: 2.4880\n",
      "\n",
      "Epoch 00004: loss improved from 2.57991 to 2.48803, saving model to build/char-based-weights-04-2.4880-complex-big.hdf5\n",
      "Epoch 5/50\n",
      "144362/144362 [==============================] - 504s 3ms/step - loss: 2.4126\n",
      "\n",
      "Epoch 00005: loss improved from 2.48803 to 2.41260, saving model to build/char-based-weights-05-2.4126-complex-big.hdf5\n",
      "Epoch 6/50\n",
      "144362/144362 [==============================] - 503s 3ms/step - loss: 2.3502\n",
      "\n",
      "Epoch 00006: loss improved from 2.41260 to 2.35021, saving model to build/char-based-weights-06-2.3502-complex-big.hdf5\n",
      "Epoch 7/50\n",
      "144362/144362 [==============================] - 506s 4ms/step - loss: 2.2913\n",
      "\n",
      "Epoch 00007: loss improved from 2.35021 to 2.29127, saving model to build/char-based-weights-07-2.2913-complex-big.hdf5\n",
      "Epoch 8/50\n",
      "144362/144362 [==============================] - 502s 3ms/step - loss: 2.2431\n",
      "\n",
      "Epoch 00008: loss improved from 2.29127 to 2.24313, saving model to build/char-based-weights-08-2.2431-complex-big.hdf5\n",
      "Epoch 9/50\n",
      "144362/144362 [==============================] - 494s 3ms/step - loss: 2.1956\n",
      "\n",
      "Epoch 00009: loss improved from 2.24313 to 2.19557, saving model to build/char-based-weights-09-2.1956-complex-big.hdf5\n",
      "Epoch 10/50\n",
      "144362/144362 [==============================] - 481s 3ms/step - loss: 2.1540\n",
      "\n",
      "Epoch 00010: loss improved from 2.19557 to 2.15397, saving model to build/char-based-weights-10-2.1540-complex-big.hdf5\n",
      "Epoch 11/50\n",
      "144362/144362 [==============================] - 534s 4ms/step - loss: 2.1167\n",
      "\n",
      "Epoch 00011: loss improved from 2.15397 to 2.11667, saving model to build/char-based-weights-11-2.1167-complex-big.hdf5\n",
      "Epoch 12/50\n",
      "144362/144362 [==============================] - 541s 4ms/step - loss: 2.0799\n",
      "\n",
      "Epoch 00012: loss improved from 2.11667 to 2.07990, saving model to build/char-based-weights-12-2.0799-complex-big.hdf5\n",
      "Epoch 13/50\n",
      "144362/144362 [==============================] - 535s 4ms/step - loss: 2.0487\n",
      "\n",
      "Epoch 00013: loss improved from 2.07990 to 2.04868, saving model to build/char-based-weights-13-2.0487-complex-big.hdf5\n",
      "Epoch 14/50\n",
      "144362/144362 [==============================] - 537s 4ms/step - loss: 2.0198\n",
      "\n",
      "Epoch 00014: loss improved from 2.04868 to 2.01979, saving model to build/char-based-weights-14-2.0198-complex-big.hdf5\n",
      "Epoch 15/50\n",
      "144362/144362 [==============================] - 528s 4ms/step - loss: 1.9906\n",
      "\n",
      "Epoch 00015: loss improved from 2.01979 to 1.99059, saving model to build/char-based-weights-15-1.9906-complex-big.hdf5\n",
      "Epoch 16/50\n",
      "144362/144362 [==============================] - 527s 4ms/step - loss: 1.9661\n",
      "\n",
      "Epoch 00016: loss improved from 1.99059 to 1.96615, saving model to build/char-based-weights-16-1.9661-complex-big.hdf5\n",
      "Epoch 17/50\n",
      "144362/144362 [==============================] - 529s 4ms/step - loss: 1.9441\n",
      "\n",
      "Epoch 00017: loss improved from 1.96615 to 1.94406, saving model to build/char-based-weights-17-1.9441-complex-big.hdf5\n",
      "Epoch 18/50\n",
      "144362/144362 [==============================] - 529s 4ms/step - loss: 1.9171\n",
      "\n",
      "Epoch 00018: loss improved from 1.94406 to 1.91715, saving model to build/char-based-weights-18-1.9171-complex-big.hdf5\n",
      "Epoch 19/50\n",
      "144362/144362 [==============================] - 531s 4ms/step - loss: 1.8982\n",
      "\n",
      "Epoch 00019: loss improved from 1.91715 to 1.89824, saving model to build/char-based-weights-19-1.8982-complex-big.hdf5\n",
      "Epoch 20/50\n",
      "144362/144362 [==============================] - 521s 4ms/step - loss: 1.8774\n",
      "\n",
      "Epoch 00020: loss improved from 1.89824 to 1.87736, saving model to build/char-based-weights-20-1.8774-complex-big.hdf5\n",
      "Epoch 21/50\n",
      "144362/144362 [==============================] - 518s 4ms/step - loss: 1.8579\n",
      "\n",
      "Epoch 00021: loss improved from 1.87736 to 1.85790, saving model to build/char-based-weights-21-1.8579-complex-big.hdf5\n",
      "Epoch 22/50\n",
      "144362/144362 [==============================] - 516s 4ms/step - loss: 1.8409\n",
      "\n",
      "Epoch 00022: loss improved from 1.85790 to 1.84089, saving model to build/char-based-weights-22-1.8409-complex-big.hdf5\n",
      "Epoch 23/50\n",
      "144362/144362 [==============================] - 534s 4ms/step - loss: 1.8262\n",
      "\n",
      "Epoch 00023: loss improved from 1.84089 to 1.82619, saving model to build/char-based-weights-23-1.8262-complex-big.hdf5\n",
      "Epoch 24/50\n",
      "144362/144362 [==============================] - 521s 4ms/step - loss: 1.8117\n",
      "\n",
      "Epoch 00024: loss improved from 1.82619 to 1.81168, saving model to build/char-based-weights-24-1.8117-complex-big.hdf5\n",
      "Epoch 25/50\n",
      "144362/144362 [==============================] - 526s 4ms/step - loss: 1.7961\n",
      "\n",
      "Epoch 00025: loss improved from 1.81168 to 1.79605, saving model to build/char-based-weights-25-1.7961-complex-big.hdf5\n",
      "Epoch 26/50\n",
      "144362/144362 [==============================] - 525s 4ms/step - loss: 1.7844\n",
      "\n",
      "Epoch 00026: loss improved from 1.79605 to 1.78438, saving model to build/char-based-weights-26-1.7844-complex-big.hdf5\n",
      "Epoch 27/50\n",
      "144362/144362 [==============================] - 522s 4ms/step - loss: 1.7679\n",
      "\n",
      "Epoch 00027: loss improved from 1.78438 to 1.76787, saving model to build/char-based-weights-27-1.7679-complex-big.hdf5\n",
      "Epoch 28/50\n",
      "144362/144362 [==============================] - 522s 4ms/step - loss: 1.7565\n",
      "\n",
      "Epoch 00028: loss improved from 1.76787 to 1.75654, saving model to build/char-based-weights-28-1.7565-complex-big.hdf5\n",
      "Epoch 29/50\n",
      "144362/144362 [==============================] - 525s 4ms/step - loss: 1.7437\n",
      "\n",
      "Epoch 00029: loss improved from 1.75654 to 1.74369, saving model to build/char-based-weights-29-1.7437-complex-big.hdf5\n",
      "Epoch 30/50\n",
      "144362/144362 [==============================] - 515s 4ms/step - loss: 1.7377\n",
      "\n",
      "Epoch 00030: loss improved from 1.74369 to 1.73770, saving model to build/char-based-weights-30-1.7377-complex-big.hdf5\n",
      "Epoch 31/50\n",
      "144362/144362 [==============================] - 518s 4ms/step - loss: 1.7257\n",
      "\n",
      "Epoch 00031: loss improved from 1.73770 to 1.72573, saving model to build/char-based-weights-31-1.7257-complex-big.hdf5\n",
      "Epoch 32/50\n",
      "144362/144362 [==============================] - 521s 4ms/step - loss: 1.7158\n",
      "\n",
      "Epoch 00032: loss improved from 1.72573 to 1.71575, saving model to build/char-based-weights-32-1.7158-complex-big.hdf5\n",
      "Epoch 33/50\n",
      "144362/144362 [==============================] - 511s 4ms/step - loss: 1.7051\n",
      "\n",
      "Epoch 00033: loss improved from 1.71575 to 1.70514, saving model to build/char-based-weights-33-1.7051-complex-big.hdf5\n",
      "Epoch 34/50\n",
      "144362/144362 [==============================] - 516s 4ms/step - loss: 1.6983\n",
      "\n",
      "Epoch 00034: loss improved from 1.70514 to 1.69831, saving model to build/char-based-weights-34-1.6983-complex-big.hdf5\n",
      "Epoch 35/50\n",
      "144362/144362 [==============================] - 525s 4ms/step - loss: 1.6892\n",
      "\n",
      "Epoch 00035: loss improved from 1.69831 to 1.68924, saving model to build/char-based-weights-35-1.6892-complex-big.hdf5\n",
      "Epoch 36/50\n",
      "144362/144362 [==============================] - 509s 4ms/step - loss: 1.8709\n",
      "\n",
      "Epoch 00036: loss did not improve from 1.68924\n",
      "Epoch 37/50\n",
      "144362/144362 [==============================] - 492s 3ms/step - loss: 1.6785\n",
      "\n",
      "Epoch 00037: loss improved from 1.68924 to 1.67846, saving model to build/char-based-weights-37-1.6785-complex-big.hdf5\n",
      "Epoch 38/50\n",
      "144362/144362 [==============================] - 475s 3ms/step - loss: 1.6687\n",
      "\n",
      "Epoch 00038: loss improved from 1.67846 to 1.66867, saving model to build/char-based-weights-38-1.6687-complex-big.hdf5\n",
      "Epoch 39/50\n",
      "144362/144362 [==============================] - 493s 3ms/step - loss: 1.6635\n",
      "\n",
      "Epoch 00039: loss improved from 1.66867 to 1.66345, saving model to build/char-based-weights-39-1.6635-complex-big.hdf5\n",
      "Epoch 40/50\n",
      "144362/144362 [==============================] - 503s 3ms/step - loss: 1.6576\n",
      "\n",
      "Epoch 00040: loss improved from 1.66345 to 1.65758, saving model to build/char-based-weights-40-1.6576-complex-big.hdf5\n",
      "Epoch 41/50\n",
      "144362/144362 [==============================] - 497s 3ms/step - loss: 1.6578\n",
      "\n",
      "Epoch 00041: loss did not improve from 1.65758\n",
      "Epoch 42/50\n",
      "144362/144362 [==============================] - 503s 3ms/step - loss: 1.6545\n",
      "\n",
      "Epoch 00042: loss improved from 1.65758 to 1.65454, saving model to build/char-based-weights-42-1.6545-complex-big.hdf5\n",
      "Epoch 43/50\n",
      "144362/144362 [==============================] - 482s 3ms/step - loss: 1.6430\n",
      "\n",
      "Epoch 00043: loss improved from 1.65454 to 1.64299, saving model to build/char-based-weights-43-1.6430-complex-big.hdf5\n",
      "Epoch 44/50\n",
      "144362/144362 [==============================] - 511s 4ms/step - loss: 1.6358\n",
      "\n",
      "Epoch 00044: loss improved from 1.64299 to 1.63580, saving model to build/char-based-weights-44-1.6358-complex-big.hdf5\n",
      "Epoch 45/50\n",
      "144362/144362 [==============================] - 518s 4ms/step - loss: 1.6357\n",
      "\n",
      "Epoch 00045: loss improved from 1.63580 to 1.63573, saving model to build/char-based-weights-45-1.6357-complex-big.hdf5\n",
      "Epoch 46/50\n",
      "144362/144362 [==============================] - 506s 4ms/step - loss: 1.6311\n",
      "\n",
      "Epoch 00046: loss improved from 1.63573 to 1.63111, saving model to build/char-based-weights-46-1.6311-complex-big.hdf5\n",
      "Epoch 47/50\n",
      "144362/144362 [==============================] - 504s 3ms/step - loss: 1.6284\n",
      "\n",
      "Epoch 00047: loss improved from 1.63111 to 1.62844, saving model to build/char-based-weights-47-1.6284-complex-big.hdf5\n",
      "Epoch 48/50\n",
      "144362/144362 [==============================] - 510s 4ms/step - loss: 1.6195\n",
      "\n",
      "Epoch 00048: loss improved from 1.62844 to 1.61946, saving model to build/char-based-weights-48-1.6195-complex-big.hdf5\n",
      "Epoch 49/50\n",
      "144362/144362 [==============================] - 512s 4ms/step - loss: 1.6157\n",
      "\n",
      "Epoch 00049: loss improved from 1.61946 to 1.61574, saving model to build/char-based-weights-49-1.6157-complex-big.hdf5\n",
      "Epoch 50/50\n",
      "144362/144362 [==============================] - 512s 4ms/step - loss: 1.6119\n",
      "\n",
      "Epoch 00050: loss improved from 1.61574 to 1.61193, saving model to build/char-based-weights-50-1.6119-complex-big.hdf5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f335466ce10>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "big_model = make_model(input_shape, output_shape, small=False)\n",
    "filepath=\"build/char-based-weights-{epoch:02d}-{loss:.4f}-complex-big.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]\n",
    "big_model.fit(X, y, epochs=50, batch_size=64, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check point"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's generate some texts using trained models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total characters:  144412\n",
      "Total unique characters:  47\n",
      "Total characters:  144462\n",
      "Total unique characters:  61\n"
     ]
    }
   ],
   "source": [
    "filename = 'data/wonderland.txt'\n",
    "with open(filename, 'r') as f:\n",
    "    raw_simple_text = f.read().lower()\n",
    "chars_simple, char_to_int_simple, int_to_char_simple = map_char_int(raw_simple_text)\n",
    "\n",
    "filename = 'data/nyt-snippet-subsets.txt'\n",
    "with open(filename, 'r') as f:\n",
    "    raw_complex_text = f.read().lower()\n",
    "chars_complex, char_to_int_complex, int_to_char_complex = map_char_int(raw_complex_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load model weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 256)               264192    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 47)                12079     \n",
      "=================================================================\n",
      "Total params: 276,271\n",
      "Trainable params: 276,271\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_2 (LSTM)                (None, 100, 256)          264192    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 100, 256)          0         \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 256)               525312    \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 47)                12079     \n",
      "=================================================================\n",
      "Total params: 801,583\n",
      "Trainable params: 801,583\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_4 (LSTM)                (None, 256)               264192    \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 61)                15677     \n",
      "=================================================================\n",
      "Total params: 279,869\n",
      "Trainable params: 279,869\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_5 (LSTM)                (None, 100, 256)          264192    \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 100, 256)          0         \n",
      "_________________________________________________________________\n",
      "lstm_6 (LSTM)                (None, 256)               525312    \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 61)                15677     \n",
      "=================================================================\n",
      "Total params: 805,181\n",
      "Trainable params: 805,181\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "X, y = text_to_feature(raw_simple_text, char_to_int_simple, len_seq=100, return_y=True)\n",
    "input_shape = (X.shape[1], X.shape[2])\n",
    "output_shape = y.shape[1]\n",
    "filename = \"build/char-based-weights-30-1.6506-simple-small.hdf5\"\n",
    "simple_small_model = make_model(input_shape, output_shape)\n",
    "simple_small_model.load_weights(filename)\n",
    "simple_small_model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "filename = \"build/char-based-weights-50-1.2453-simple-big.hdf5\"\n",
    "simple_big_model = make_model(input_shape, output_shape, False)\n",
    "simple_big_model.load_weights(filename)\n",
    "simple_big_model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "X, y = text_to_feature(raw_complex_text, char_to_int_complex, len_seq=100, return_y=True)\n",
    "input_shape = (X.shape[1], X.shape[2])\n",
    "output_shape = y.shape[1]\n",
    "filename = \"build/char-based-weights-50-1.9461-complex-small.hdf5\"\n",
    "complex_small_model = make_model(input_shape, output_shape)\n",
    "complex_small_model.load_weights(filename)\n",
    "complex_small_model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "filename = \"build/char-based-weights-50-1.6119-complex-big.hdf5\"\n",
    "complex_big_model = make_model(input_shape, output_shape, False)\n",
    "complex_big_model.load_weights(filename)\n",
    "complex_big_model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(raw_text, model, char_to_int, int_to_char):\n",
    "    \"\"\"Generates texts given a raw text of length with 100\"\"\"\n",
    "    seq_X = [char_to_int.get(char, 0) for char in raw_text]\n",
    "    nunique_chars = len(char_to_int)\n",
    "    results = []\n",
    "    results.append(raw_text)\n",
    "    result = ''\n",
    "    while result != '.' and len(results) < 300:\n",
    "        X = np.reshape(seq_X, (1, len(seq_X), 1))\n",
    "        X = X / nunique_chars\n",
    "        prediction = model.predict(X, verbose=0)\n",
    "        index = np.argmax(prediction)\n",
    "        result = int_to_char[index]\n",
    "        results.append(result)\n",
    "        seq_X.append(index)\n",
    "        seq_X = seq_X[1:]\n",
    "    return ''.join(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sentence(raw_sentence, max_len=100, pad_str=' '):\n",
    "    l = len(raw_sentence)\n",
    "    if l < max_len:\n",
    "        pad_len = max_len - l\n",
    "        pad_strings = pad_str * pad_len\n",
    "        return pad_strings + raw_sentence\n",
    "    else:\n",
    "        return raw_sentence[:max_len]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "CNN = 'Occupy Boston protesters remained firmly entrenched in a downtown city square early Friday after a midnight deadline passed for them to clear out or face eviction.  Demonstrators cleared trash and some of the more than 100 tents from the area, but most stayed put. Police presence was light around Dewey Square. Superior Court Judge Frances A. McIntyre ruled Wednesday that demonstrators\\' First Amendment rights do not extend to seizing and holding areas on which they sit. Authorities are \\\"obligated by law to preserve Dewey Square as a space open to the public,\\\" McIntyre added. Boston Mayor Thomas M. Menino, who in the past has appeared to tolerate his city\\'s chapter of the nationwide movement, recently signaled that the park could be a safety hazard as winter weather rolls in. \\\"We\\'re asking them to leave, according to their own will and volition,\\\" Menino told CNN affiliate WCVB. \\\"After that, we\\'ll make decisions about how we\\'ll clear off the site in the future.\\\" Protesters have been encamped in the square since late September.  On Thursday, police could be seen handing out fliers to protesters in an apparent effort to inform them that they soon had to leave.  \\\"It\\'s like telling you to get out of your apartment by midnight,\\\" protester Philip O\\'Connell told the affiliate station. By Thursday evening, comments on an Occupy Boston Twitter feed were focused on the impending deadline.  \\\"Some tents may fold, but an idea cannot be evicted,\\\" read one post.  \\\"We have occupied Dewey Square because Wall Street has occupied our government,\\\" read another. In October, 129 people were arrested during a demonstration in which protesters allegedly blocked traffic and refused to disperse, according to police spokesman Eddy Chrispin. They were arrested mostly for \\\"unlawful assembly and trespassing,\\\" he said.  The movement, which first sprang up in a Lower Manhattan park, seeks to highlight what it sees as corruption and growing income disparities between the nation\\'s richest 1% and the rest of the country. In a move similar to McIntyre\\'s ruling, a New York Supreme Court announced last month that Occupy protesters would be allowed to return to Zuccotti Park -- considered a home base for demonstrators -- but would be restricted from camping overnight.  Police in riot gear cleared them out in the early-morning hours in mid-November, a move that attorneys for the demonstrators have said was unlawful. Thousands later deluged the city\\'s financial district in a show of strength echoed nationwide as part of the group\\'s so-called \\\"mass day of action.\\\" Scores were arrested across the city, and several police officers were reported injured, as thousands of others gathered in places such as St. Louis, San Francisco, Denver and Milwaukee to respond to the \\\"day of action\\\" plea. Last month, police in Los Angeles and Philadelphia dismantled tents and arrested Occupy protesters who refused to leave city areas.  In Philadelphia, police arrested 52 people after scuffles broke out when authorities ordered some protesters to clear the street.  Encampments have largely remained in a handful of cities, including San Francisco; Asheville, North Carolina; Oklahoma City; and Washington, according to media reports and group websites. '\n",
    "USAToday = 'Shopping carts sit outside the Wal-Mart store in Mayfield Hts., Ohio., on Nov. 14, 2011. The retailer offered few details about the investigation in a quarterly report filed Thursday with the Securities and Exchange Commission. The company said that it opened the investigation after reviewing policies, procedures and internal controls tied to its global anti-corruption program. It said that it is taking \\\"appropriate remedial measures.\\\" \\\"We are taking a deep look at our policies and procedures in every country in which we operate,\\\" said Wal-Mart spokesman Dave Tovar in an email. \\\"As a result of information obtained during that review and from other sources, we have begun an internal investigation related to compliance with the\\\" Foreign Corrupt Practice Act. All companies doing business overseas must comply with the U.S. Foreign Corrupt Practices Act, which broadly deals with bribery and accounting rules. Wal-Mart Stores (WMT) has hired outside lawyers and other advisers and has notified the U.S. Department of Justice and Securities and Exchange Commission. \\\"Our investigation is currently focused on discrete incidents in specific areas,\\\" Tovar said. \\\"We intend to keep federal authorities apprised of what we learn.\\\" The issue will not have a material impact on business, the company said. The world\\'s largest retailer  which operates in 28 countries, including China, Mexico and Japan  brought in $419 billion in revenue last year. Company shares fell 31 cents or 0.5% to $58.29 on Friday. '\n",
    "TheGreenMile = 'This happened in 1932, when the state penitentiary was still at Cold Mountain. And the electric chair was there, too, of course.   The inmates made jokes about the chair the way people always make jokes about things that frighten them but can\\'t be gotten away from. They called it Old Sparky, or the Big Juicy. They made cracks about the Power bill, and how Warden Moores would cook his Thanksgiving dinner that fall, with his wife, Melinda, too sick to cook.   But for the ones who actually had to sit down in that chair, the humor went out of the situation in a hurry I presided over seventy-eight executions during my time at Cold Mountain (that\\'s one figure I\\'ve never been confused about; I\\'ll remember it on my deathbed), and I think that, for most of those men, the truth of what was happening to them finally hit all the way home when their ankles were being damped to the stout oak of \\\"Old Sparky\\'s\\\" legs. The realization came then (you would see it rising in their eyes, a kind of cold dismay) that their, own legs had finished their careers. The blood still ran in them, the muscles were still strong, but they were finished, all the same; they were never going to walk another country mile or dance with a girl at a barn-raising. Old Sparky\\'s clients came to a knowledge of their deaths from the ankles up. There was a black silk bag that went over their heads after they had finished their rambling and mostly disjointed last remarks. It was supposed to be for them, but I always thought it was really for us, to keep us from seeing the awful tide of dismay in their eyes as they realized they were going to die with their knees bent.   There was no death row at Cold Mountain, only E Block, set apart from the other four and about a quarter their size, brick instead of wood, with a horrible bare metal roof that glared in the summer sun like a delirious eyeball. Six cells inside, three on each side of a wide center aisle, each almost twice as big as the cells in the other four blocks. Singles, too. Great accommodations for a prison (especially in the thirties), but the inmates would have traded for cells in any of the other four. Believe me, they would have traded.   There was never a time during my years as block superintendent when all six cells were occupied at one time -- thank God for small favors. Four was the most, mixed black and white (at Cold Mountain, there was no segregation among the walking dead), and that was a little piece of hell. One was a woman, Beverly McCall. She was black as the ace of spades and as beautiful as the sin you never had nerve enough to commit. She put up with six years of her husband beating her, but wouldn\\'t put up with his creeping around for a single day. On the evening after she found out he was cheating, she stood waiting for the unfortunate Lester McCall, known to his pals (and, presumably, to his extremely short-term mistress) as Cutter, at the top of the stairs leading to the apartment over his barber shop. She waited until he got his overcoat half off, then dropped his cheating guts onto his tu-tone shoes. Used one of Cutter\\'s own razors to do it. Two nights before she was due to sit in Old Sparky, she called me to her cell and said she had been visited by her African spirit-father in a dream. He told her to discard her slave-name and to die under her free name, Matuomi. That was her request, that her deathwarrant should be read under the name of Beverly Matuomi. I guess her spirit-father didn\\'t give her any first name, or one she could make out, anyhow. I said yes, okay, fine. One thing those years serving as the bull-goose screw taught me was never to refuse the condemned unless I absolutely had to. In the case of Beverly Matuomi, it made no difference, anyway. The governor called the next day around three in the afternoon, commuting her sentence to life in the Grassy Valley Penal Facility for Women -- all penal and no penis, we used to say back then. I was glad to see Bev\\'s round ass going left instead of right when she got to the duty desk, let me tell you. Thirty-five years or so later -- had to be at least thirty-five -- I saw that name on the obituary page of the paper, under a picture of a skinny-faced black lady with a cloud of white hair and glasses with rhinestones at the comers. It was Beverly. She\\'d spent the last ten years of her life a free woman, the obituary said, and had rescued the small-town library of Raines Falls pretty much single-handed. She had also taught Sunday school and had been much loved in that little backwater. LIBRARIAN DIES OF HEART FAILURE, the headline said, and below that, in smaller type, almost as an afterthought: Served Over Two Decades in Prison for Murder. Only the eyes, wide and blazing behind the glasses with the rhinestones at the comers, were the same. They were the eyes of a woman who even at seventy-whatever would not hesitate to pluck a safety razor from its blue jar of disinfectant, if the urge seemed pressing. You know murderers, even if they finish up as old lady librarians in dozey little towns. At least you do if you\\'ve spent as much time minding murderers as I did. There was only one time I ever had a question about the nature of my job. That, I reckon, is why I\\'m writing this.   The wide corridor up the center of E Block was floored with linoleum the color of tired old limes, and so what was called the Last Mile at other prisons was called the Green Mile at Cold Mountain. It ran, I guess, sixty long paces from south to north, bottom to top. At the bottom was the restraint room. At the top end was a T-junction. A left turn meant life -- if you called what went on in the sunbaked exercise yard life, and many did; many lived it for years, with no apparent ill effects. Thieves and arsonists and sex criminals, all talking their talk and walking their walk and making their little deals.   A right turn, though -- that was different. First you went into my office (where the carpet was also green, a thing I kept meaning to change and not getting around to), and crossed in front of my desk, which was flanked by the American flag on the left and the state flag on the right. On the far side were two doors. One led into the small W.C. that I and the E Block guards (sometimes even Warden Moores) used; the other opened on a kind of storage shed. This was where you ended up when you walked the Green Mile.   It was a small door -- I had to duck my head when I went through, and John Coffey actually had to sit and scoot. You came out on a little landing, then went down three cement steps to a board floor. It was a miserable room without heat and with a metal roof, just like the one on the block to which it was an adjunct. It was cold enough in there to see your breath during the winter, and stifling in the summer. At the execution of Elmer Manfred -- in July or August of \\'30, that one was, I believe -- we had nine witnesses pass out.   On the left side of the storage shed -- again -- there was life. Tools (all locked down in frames crisscrossed with chains, as if they were carbine rifles instead of spades and pickaxes), dry goods, sacks of seeds for spring planting in the prison gardens, boxes of toilet paper, pallets cross-loaded with blanks for the prison plate-shop...even bags of lime for marking out the baseball diamond and the football gridiron -- the cons played in what was known as The Pasture, and fall afternoons were greatly looked forward to at Cold Mountain.   On the right -- once again -- death. Old Sparky his ownself, sitting up on a plank platform at the southeast comer of the storeroom, stout oak legs, broad oak arms that had absorbed the terrorized sweat of scores of men in the last few minutes of their lives, and the metal cap, usually hung jauntily on the back of the chair, like some robot kid\\'s beanie in a Buck Rogers comic-strip. A cord ran from it and through a gasket-circled hole in the cinderblock wall behind the chair. Off to one side was a galvanized tin bucket. If you looked inside it, you would see a circle of sponge, cut just right to fit the metal cap. Before executions, it was soaked in brine to better conduct the charge of direct-current electricity that ran through the wire, through the sponge, and into the condemned man\\'s brain.'\n",
    "HarryPotter = 'October arrived, spreading a damp chill over the grounds and into the castle. Madam Pomfrey, the nurse, was kept busy by a sudden spate of colds among the staff and students. Her Pepperup potion worked instantly, though it left the drinker smoking at the ears for several hours afterward. Ginny Weasley, who had been looking pale, was bullied into taking some by Percy. The steam pouring from under her vivid hair gave the impression that her whole head was on fire.  Raindrops the size of bullets thundered on the castle windows for days on end; the lake rose, the flower beds turned into muddy streams, and Hagrid \\'s pumpkins swelled to the size of garden sheds. Oliver Wood \\'s enthusiasm for regular training sessions, however, was not dampened, which was why Harry was to be found, late one stormy Saturday afternoon a few days before Halloween, returning to Gryffindor Tower, drenched to the skin and splattered with mud.  Even aside from the rain and wind it hadn \\'t been a happy practice session. Fred and George, who had been spying on the Slytherin team, had seen for themselves the speed of those new Nimbus Two Thousand and Ones. They reported that the Slytherin team was no more than seven greenish blurs, shooting through the air like missiles.  As Harry squelched along the deserted corridor he came across somebody who looked just as preoccupied as he was. Nearly Headless Nick, the ghost of Gryffindor Tower, was staring morosely out of a window, muttering under his breath,  \\\" don \\'t fulfill their requirements half an inch, if that \\\"   \\\"Hello, Nick, \\\" said Harry.   \\\"Hello, hello, \\\" said Nearly Headless Nick, starting and looking round. He wore a dashing, plumed hat on his long curly hair, and a tunic with a ruff, which concealed the fact that his neck was almost completely severed. He was pale as smoke, and Harry could see right through him to the dark sky and torrential rain outside.   \\\"You look troubled, young Potter, \\\" said Nick, folding a transparent letter as he spoke and tucking it inside his doublet.   \\\"So do you, \\\" said Harry.   \\\"Ah, \\\" Nearly Headless Nick waved an elegant hand,  \\\"a matter of no importance. It \\'s not as though I really wanted to join. Thought I \\'d apply, but apparently I  \\'don \\'t fulfill requirements \\' - \\\"  In spite of his airy tone, there was a look of great bitterness on his face.   \\\"But you would think, wouldn \\'t you, \\\" he erupted suddenly, pulling the letter back out of his pocket,  \\\"that getting hit forty-five times in the neck with a blunt axe would qualify you to join the Headless Hunt? \\\"   \\\"Oh - yes, \\\" said Harry, who was obviously supposed to agree.   \\\"I mean, nobody wishes more than I do that it had all been quick and clean, and my head had come off properly, I mean, it would have saved me a great deal of pain and ridicule. However - \\\" Nearly Headless Nick shook his letter open and read furiously:  \\\" \\'We can only accept huntsmen whose heads have parted company with their bodies. You will appreciate that it would be impossible otherwise for members to participate in hunt activities such as Horseback Head-Juggling and Head Polo. It is with the greatest regret, therefore, that I must inform you that you do not fulfill our requirements. With very best wishes, Sir Patrick Delaney-Podmore. \\' \\\"  Fuming, Nearly Headless Nick stuffed the letter away.   \\\"Half an inch of skin and sinew holding my neck on, Harry! Most people would think that \\'s good and beheaded, but oh, no, it \\'s not enough for Sir Properly Decapitated-Podmore. \\\"  Nearly Headless Nick took several deep breaths and then said, in a far calmer tone,  \\\"So - what \\'s bothering you? Anything I can do? \\\"   \\\"No, \\\" said Harry.  \\\"Not unless you know where we can get seven free Nimbus Two Thousand and Ones for our match against Sly - \\\"  The rest of Harry \\'s sentence was drowned out by a high-pitched mewling from somewhere near his ankles. He looked down and found himself gazing into a pair of lamp-like yellow eyes. It was Mrs. Norris, the skeletal gray cat who was used by the caretaker, Argus Filch, as a sort of deputy in his endless battle against students.   \\\"You \\'d better get out of here, Harry, \\\" said Nick quickly.  \\\"Filch isn \\'t in a good mood - he \\'s got the flu and some third years accidentally plastered frog brains all over the ceiling in dungeon five. He \\'s been cleaning all morning, and if he sees you dripping mud all over the place - \\\"   \\\"Right, \\\" said Harry, backing away from the accusing stare of Mrs. Norris, but not quickly enough. Drawn to the spot by the mysterious power that seemed to connect him with his foul cat, Argus Filch burst suddenly through a tapestry to Harry \\'s right, wheezing and looking wildly about for the rule-breaker. There was a thick tartan scarf bound around his head, and his nose was unusually purple.   \\\"Filth! \\\" he shouted, his jowls aquiver, his eyes popping alarmingly as he pointed at the muddy puddle that had dripped from Harry \\'s Quidditch robes.  \\\"Mess and muck everywhere! I \\'ve had enough of it, I tell you! Follow me, Potter! \\\"  So Harry waved a gloomy good-bye to Nearly Headless Nick and followed Filch back downstairs, doubling the number of muddy footprints on the floor.  Harry had never been inside Filch \\'s office before; it was a place most students avoided. The room was dingy and windowless, lit by a single oil lamp dangling from the low ceiling. A faint smell of fried fish lingered about the place. Wooden filing cabinets stood around the walls; from their labels, Harry could see that they contained details of every pupil Filch had ever punished. Fred and George Weasley had an entire drawer to themselves. A highly polished collection of chains and manacles hung on the wall behind Filch \\'s desk. It was common knowledge that he was always begging Dumbledore to let him suspend students by their ankles from the ceiling.  Filch grabbed a quill from a pot on his desk and began shuffling around looking for parchment.   \\\"Dung, \\\" he muttered furiously,  \\\"great sizzling dragon bogies frog brains rat intestines I \\'ve had enough of it make an example where \\'s the form yes \\\"  He retrieved a large roll of parchment from his desk drawer and stretched it out in front of him, dipping his long black quill into the ink pot.   \\\"Name Harry Potter. Crime \\\"   \\\"It was only a bit of mud! \\\" said Harry.   \\\"It \\'s only a bit of mud to you, boy, but to me it \\'s an extra hour scrubbing! \\\" shouted Filch, a drip shivering unpleasantly at the end of his bulbous nose.  \\\"Crime befouling the castle suggested sentence \\\"  Dabbing at his streaming nose, Filch squinted unpleasantly at Harry who waited with bated breath for his sentence to fall.  But as Filch lowered his quill, there was a great BANG! on the ceiling of the office, which made the oil lamp rattle.   \\\"PEEVES! \\\" Filch roared, flinging down his quill in a transport of rage.  \\\"I \\'ll have you this time, I \\'ll have you! \\\"  And without a backward glance at Harry, Filch ran flat-footed from the office, Mrs. Norris streaking alongside him.  Peeves was the school poltergeist, a grinning, airborne menace who lived to cause havoc and distress. Harry didn \\'t much like Peeves, but couldn \\'t help feeling grateful for his timing. Hopefully, whatever Peeves had done (and it sounded as though he \\'d wrecked something very big this time) would distract Filch from Harry.  Thinking that he should probably wait for Filch to come back, Harry sank into a moth-eaten chair next to the desk. There was only one thing on it apart from his half-completed form: a large, glossy, purple envelope with silver lettering on the front. With a quick glance at the door to check that Filch wasn \\'t on his way back, Harry picked up the envelope and read: kwikspell A Correspondence Course in Beginners \\' Magic  p>Intrigued, Harry flicked the envelope open and pulled out the sheaf of parchment inside. More curly silver writing on the front page said: Feel out of step in the world of modern magic? Find yourself making excuses not to perform simple spells? Ever been taunted for your woeful wandwork? There is an answer! Kwikspell is an all-new, fail-safe, quick-result, easy-learn course. Hundreds of witches and wizards have benefited from the Kwikspell method! Madam Z. Nettles of Topsham writes:  \\\"I had no memory for incantations and my potions were a family joke! Now, after a Kwikspell course, I am the center of attention at parties and friends beg for the recipe of my Scintillation Solution! \\\" Warlock D. J. Prod of Didsbury says:  \\\"My wife used to sneer at my feeble charms, but one month into your fabulous Kwikspell course and I succeeded in turning her into a yak! Thank you, Kwikspell! \\\"    Fascinated, Harry thumbed through the rest of the envelope \\'s contents. Why on earth did Filch want a Kwikspell course? Did this mean he wasn \\'t a proper wizard? Harry was just reading  \\\"Lesson One: Holding Your Wand (Some Useful Tips) \\\" when shuffling footsteps outside told him Filch was coming back. Stuffing the parchment back into the envelope, Harry threw it back onto the desk just as the door opened.  Filch was looking triumphant.   \\\"That vanishing cabinet was extremely valuable! \\\" he was saying gleefully to Mrs. Norris.  \\\"We \\'ll have Peeves out this time, my sweet - \\\"  His eyes fell on Harry and then darted to the Kwikspell envelope, which, Harry realized too late, was lying two feet away from where it had started.  Filch \\'s pasty face went brick red. Harry braced himself for a tidal wave of fury. Filch hobbled across to his desk, snatched up the envelope, and threw it into a drawer.   \\\"Have you - did you read -? \\\" he sputtered.   \\\"No, \\\" Harry lied quickly.  Filch \\'s knobbly hands were twisting together.   \\\"If I thought you \\'d read my private - not that it \\'s mine - for a friend - be that as it may - however - \\\"  Harry was staring at him, alarmed; Filch had never looked madder. His eyes were popping, a tic was going in one of his pouchy cheeks, and the tartan scarf didn \\'t help.   \\\"Very well - go - and don \\'t breathe a word - not that - however, if you didn \\'t read - go now, I have to write up Peeves \\' report - go - \\\"  Amazed at his luck, Harry sped out of the office, up the corridor, and back upstairs. To escape from Filch \\'s office without punishment was probably some kind of school record.   \\\"Harry! Harry! Did it work? \\\"  Nearly Headless Nick came gliding out of a classroom. Behind him, Harry could see the wreckage of a large black-and-gold cabinet that appeared to have been dropped from a great height.   \\\"I persuaded Peeves to crash it right over Filch \\'s office, \\\" said Nick eagerly.  \\\"Thought it might distract him - \\\"   \\\"Was that you? \\\" said Harry gratefully.  \\\"Yeah, it worked, I didn \\'t even get detention. Thanks, Nick! \\\"  They set off up the corridor together. Nearly Headless Nick, Harry noticed, was still holding Sir Patrick \\'s rejection letter.   \\\"I wish there was something I could do for you about the Headless Hunt, \\\" Harry said.    Nearly Headless Nick stopped in his tracks and Harry walked right through him. He wished he hadn \\'t; it was like stepping through an icy shower.   \\\"But there is something you could do for me, \\\" said Nick excitedly.  \\\"Harry - would I be asking too much - but no, you wouldn \\'t want - \\\"   \\\"What is it? \\\" said Harry.   \\\"Well, this Halloween will be my five hundredth deathday, \\\" said Nearly Headless Nick, drawing himself up and looking dignified.   \\\"Oh, \\\" said Harry, not sure whether he should look sorry or happy about this.  \\\"Right. \\\"   \\\"I \\'m holding a party down in one of the roomier dungeons. Friends will be coming from all over the country. It would be such an honor if you would attend. Mr. Weasley and Miss Granger would be most welcome, too, of course - but I daresay you \\'d rather go to the school feast? \\\" He watched Harry on tenterhooks.   \\\"No, \\\" said Harry quickly,  \\\"I \\'ll come - \\\"   \\\"My dear boy! Harry Potter, at my deathday party! And \\\" - he hesitated, looking excited -  \\\"do you think you could possibly mention to Sir Patrick how very frightening and impressive you find me? \\\"   \\\"Of - of course, \\\" said Harry.  Nearly Headless Nick beamed at him.  \\\"A deathday party? \\\" said Hermione keenly when Harry had changed at last and joined her and Ron in the common room.  \\\"I bet there aren \\'t many living people who can say they \\'ve been to one of those - it \\'ll be fascinating! \\\"   \\\"Why would anyone want to celebrate the day they died? \\\" said Ron, who was halfway through his Potions homework and grumpy.  \\\"Sounds dead depressing to me. \\\"  Rain was still lashing the windows, which were now inky black, but inside all looked bright and cheerful. The firelight glowed over the countless squashy armchairs where people sat reading, talking, doing homework or, in the case of Fred and George Weasley, trying to find out what would happen if you fed a Filibuster firework to a salamander. Fred had  \\\"rescued \\\" the brilliant orange, fire-dwelling lizard from a Care of Magical Creatures class and it was now smouldering gently on a table surrounded by a knot of curious people.  Harry was at the point of telling Ron and Hermione about Filch and the Kwikspell course when the salamander suddenly whizzed into the air, emitting loud sparks and bangs as it whirled wildly round the room. The sight of Percy bellowing himself hoarse at Fred and George, the spectacular display of tangerine stars showering from the salamander \\'s mouth, and its escape into the fire, with accompanying explosions, drove both Filch and the Kwikspell envelope from Harry \\'s mind. By the time Halloween arrived, Harry was regretting his rash promise to go to the deathday party. The rest of the school was happily anticipating their Halloween feast; the Great Hall had been decorated with the usual live bats, Hagrid \\'s vast pumpkins had been carved into lanterns large enough for three men to sit in, and there were rumors that Dumbledore had booked a troupe of dancing skeletons for the entertainment.   \\\"A promise is a promise, \\\" Hermione reminded Harry bossily.  \\\"You said you \\'d go to the deathday party. \\\"  So at seven o \\'clock, Harry, Ron, and Hermione walked straight past the doorway to the packed Great Hall, which was glittering invitingly with gold plates and candles, and directed their steps instead toward the dungeons.  The passageway leading to Nearly Headless Nick \\'s party had been lined with candles, too, though the effect was far from cheerful: These were long, thin, jet-black tapers, all burning bright blue, casting a dim, ghostly light even over their own living faces. The temperature dropped with every step they took. As Harry shivered and drew his robes tightly around him, he heard what sounded like a thousand fingernails scraping an enormous blackboard.   \\\"Is that supposed to be music? \\\" Ron whispered. They turned a corner and saw Nearly Headless Nick standing at a doorway hung with black velvet drapes.   \\\"My dear friends, \\\" he said mournfully.  \\\"Welcome, welcome so pleased you could come. \\\"  He swept off his plumed hat and bowed them inside.  It was an incredible sight. The dungeon was full of hundreds of pearly-white, translucent people, mostly drifting around a crowded dance floor, waltzing to the dreadful, quavering sound of thirty musical saws, played by an orchestra on a raised, black-draped platform. A chandelier overhead blazed midnight-blue with a thousand more black candles. Their breath rose in a mist before them; it was like stepping into a freezer.   \\\"Shall we have a look around? \\\" Harry suggested, wanting to warm up his feet.   \\\"Careful not to walk through anyone, \\\" said Ron nervously, and they set off around the edge of the dance floor. They passed a group of gloomy nuns, a ragged man wearing chains, and the Fat Friar, a cheerful Hufflepuff ghost, who was talking to a knight with an arrow sticking out of his forehead. Harry wasn \\'t surprised to see that the Bloody Baron, a gaunt, staring Slytherin ghost covered in silver bloodstains, was being given a wide berth by the other ghosts.   \\\"Oh, no, \\\" said Hermione, stopping abruptly.  \\\"Turn back, turn back, I don \\'t want to talk to Moaning Myrtle - \\\"   \\\"Who? \\\" said Harry as they backtracked quickly.   \\\"She haunts one of the toilets in the girls \\' bathroom on the first floor, \\\" said Hermione.   \\\"She haunts a toilet? \\\"   \\\"Yes. It \\'s been out-of-order all year because she keeps having tantrums and flooding the place. I never went in there anyway if I could avoid it; it \\'s awful trying to have a pee with her wailing at you - \\\"  \\\"Look, food! \\\" said Ron.  On the other side of the dungeon was a long table, also covered in black velvet. They approached it eagerly but next moment had stopped in their tracks, horrified. The smell was quite disgusting. Large, rotten fish were laid on handsome silver platters; cakes, burned charcoal-black, were heaped on salvers; there was a great maggoty haggis, a slab of cheese covered in furry green mold and, in pride of place, an enormous gray cake in the shape of a tombstone, with tar-like icing forming the words, Sir Nicholas de Mimsy-Porpington died 31st October, 1492  Harry watched, amazed, as a portly ghost approached the table, crouched low, and walked through it, his mouth held wide so that it passed through one of the stinking salmon.   \\\"Can you taste it if you walk though it? \\\" Harry asked him.   \\\"Almost, \\\" said the ghost sadly, and he drifted away.   \\\"I expect they \\'ve let it rot to give it a stronger flavor, \\\" said Hermione knowledgeably, pinching her nose and leaning closer to look at the putrid haggis.   \\\"Can we move? I feel sick, \\\" said Ron.  They had barely turned around, however, when a little man swooped suddenly from under the table and came to a halt in midair before them.   \\\"Hello, Peeves, \\\" said Harry cautiously.  Unlike the ghosts around them, Peeves the Poltergeist was the very reverse of pale and transparent. He was wearing a bright orange party hat, a revolving bow tie, and a broad grin on his wide, wicked face.   \\\"Nibbles? \\\" he said sweetly, offering them a bowl of peanuts covered in fungus.   \\\"No thanks, \\\" said Hermione.   \\\"Heard you talking about poor Myrtle, \\\" said Peeves, his eyes dancing.  \\\"Rude you was about poor Myrtle. \\\" He took a deep breath and bellowed,  \\\"OY! MYRTLE! \\\"   \\\"Oh, no, Peeves, don \\'t tell her what I said, she \\'ll be really upset, \\\" Hermione whispered frantically.  \\\"I didn \\'t mean it, I don \\'t mind her - er, hello, Myrtle. \\\"  The squat ghost of a girl had glided over. She had the glummest face Harry had ever seen, half-hidden behind lank hair and thick, pearly spectacles.   \\\"What? \\\" she said sulkily.   \\\"How are you, Myrtle? \\\" said Hermione in a falsely bright voice.  \\\"It \\'s nice to see you out of the toilet. \\\"  Myrtle sniffed.   \\\"Miss Granger was just talking about you - \\\" said Peeves slyly in Myrtle \\'s ear.   \\\"Just saying - saying - how nice you look tonight, \\\" said Hermione, glaring at Peeves.  Myrtle eyed Hermione suspiciously.   \\\"You \\'re making fun of me, \\\" she said, silver tears welling rapidly in her small, see-through eyes.   \\\"No - honestly - didn \\'t I just say how nice Myrtle \\'s looking? \\\" said Hermione, nudging Harry and Ron painfully in the ribs.   \\\"Oh, yeah - \\\"   \\\"She did - \\\"   \\\"Don \\'t lie to me, \\\" Myrtle gasped, tears now flooding down her face, while Peeves chuckled happily over her shoulder.  \\\"D \\'you think I don \\'t know what people call me behind my back? Fat Myrtle! Ugly Myrtle! Miserable, moaning, moping Myrtle! \\\"   \\\"You \\'ve forgotten pimply, \\\" Peeves hissed in her ear.  Moaning Myrtle burst into anguished sobs and fled from the dungeon. Peeves shot after her, pelting her with moldy peanuts, yelling,  \\\"Pimply! Pimply! \\\"   \\\"Oh, dear, \\\" said Hermione sadly.  Nearly Headless Nick now drifted toward them through the crowd.   \\\"Enjoying yourselves? \\\"   \\\"Oh, yes, \\\" they lied.   \\\"Not a bad turnout, \\\" said Nearly Headless Nick proudly.  \\\"The Wailing Widow came all the way up from Kent. It \\'s nearly time for my speech, I \\'d better go and warn the orchestra. \\\"  The orchestra, however, stopped playing at that very moment. They, and everyone else in the dungeon, fell silent, looking around in excitement, as a hunting horn sounded.   \\\"Oh, here we go, \\\" said Nearly Headless Nick bitterly.  Through the dungeon wall burst a dozen ghost horses, each ridden by a headless horseman. The assembly clapped wildly; Harry started to clap, too, but stopped quickly at the sight of Nick \\'s face.  The horses galloped into the middle of the dance floor and halted, rearing and plunging. At the front of the pack was a large ghost who held his bearded head under his arm, from which position he was blowing the horn. The ghost leapt down, lifted his head high in the air so he could see over the crowd (everyone laughed), and strode over to Nearly Headless Nick, squashing his head back onto his neck.   \\\"Nick! \\\" he roared.  \\\"How are you? Head still hanging in there? \\\"  He gave a hearty guffaw and clapped Nearly Headless Nick on the shoulder.   \\\"Welcome, Patrick, \\\" said Nick stiffly.   \\\"Live  \\'uns! \\\" said Sir Patrick, spotting Harry, Ron, and Hermione and giving a huge, fake jump of astonishment, so that his head fell off again (the crowd howled with laughter).   \\\"Very amusing, \\\" said Nearly Headless Nick darkly.   \\\"Don \\'t mind Nick! \\\" shouted Sir Patrick \\'s head from the floor.  \\\"Still upset we won \\'t let him join the Hunt! But I mean to say - look at the fellow - \\\"   \\\"I think, \\\" said Harry hurriedly, at a meaningful look from Nick,  \\\"Nick \\'s very - frightening and - er - \\\"   \\\"Ha! \\\" yelled Sir Patrick \\'s head.  \\\"Bet he asked you to say that! \\\"   \\\"If I could have everyone \\'s attention, it \\'s time for my speech! \\\" said Nearly Headless Nick loudly, striding toward the podium and climbing into an icy blue spotlight.   \\\"My late lamented lords, ladies, and gentlemen, it is my great sorrow \\\"  But nobody heard much more. Sir Patrick and the rest of the Headless Hunt had just started a game of Head Hockey and the crowd were turning to watch. Nearly Headless Nick tried vainly to recapture his audience, but gave up as Sir Patrick \\'s head went sailing past him to loud cheers.  Harry was very cold by now, not to mention hungry.   \\\"I can \\'t stand much more of this, \\\" Ron muttered, his teeth chattering, as the orchestra ground back into action and the ghosts swept back onto the dance floor.   \\\"Let \\'s go, \\\" Harry agreed.  They backed toward the door, nodding and beaming at anyone who looked at them, and a minute later were hurrying back up the passageway full of black candles.   \\\"Pudding might not be finished yet, \\\" said Ron hopefully, leading the way toward the steps to the entrance hall.  And then Harry heard it.   \\\" rip tear kill \\\"  It was the same voice, the same cold, murderous voice he had heard in Lockhart \\'s office.  He stumbled to a halt, clutching at the stone wall, listening with all his might, looking around, squinting up and down the dimly lit passageway.   \\\"Harry, what \\'re you -? \\\"   \\\"It \\'s that voice again - shut up a minute - \\\"   \\\" soo hungry for so long \\\"   \\\"Listen! \\\" said Harry urgently, and Ron and Hermione froze, watching him.   \\\" kill time to kill \\\"  The voice was growing fainter. Harry was sure it was moving away - moving upward. A mixture of fear and excitement gripped him as he stared at the dark ceiling; how could it be moving upward? Was it a phantom, to whom stone ceilings didn \\'t matter?   \\\"This way, \\\" he shouted, and he began to run, up the stairs, into the entrance hall. It was no good hoping to hear anything here, the babble of talk from the Halloween feast was echoing out of the Great Hall. Harry sprinted up the marble staircase to the first floor, Ron and Hermione clattering behind him.   \\\"Harry, what \\'re we - \\\"   \\\"SHH! \\\"  Harry strained his ears. Distantly, from the floor above, and growing fainter still, he heard the voice:  \\\" I smell blood. I SMELL BLOOD! \\\"  His stomach lurched -   \\\"It \\'s going to kill someone! \\\" he shouted, and ignoring Ron \\'s and Hermione \\'s bewildered faces, he ran up the next flight of steps three at a time, trying to listen over his own pounding footsteps -  Harry hurtled around the whole of the second floor, Ron and Hermione panting behind him, not stopping until they turned a corner into the last, deserted passage.   \\\"Harry, what was that all about? \\\" said Ron, wiping sweat off his face.  \\\"I couldn \\'t hear anything. \\\"  But Hermione gave a sudden gasp, pointing down the corridor.   \\\"Look! \\\"  Something was shining on the wall ahead. They approached slowly, squinting through the darkness. Foot-high words had been daubed on the wall between two windows, shimmering in the light cast by the flaming torches. the chamber of secrets has been opened. enemies of the heir, beware.   \\\"What \\'s that thing - hanging underneath? \\\" said Ron, a slight quiver in his voice.  As they edged nearer, Harry almost slipped - there was a large puddle of water on the floor; Ron and Hermione grabbed him, and they inched toward the message, eyes fixed on a dark shadow beneath it. All three of them realized what it was at once, and leapt backward with a splash.  Mrs. Norris, the caretaker \\'s cat, was hanging by her tail from the torch bracket. She was stiff as a board, her eyes wide and staring.  For a few seconds, they didn \\'t move. Then Ron said,  \\\"Let \\'s get out of here. \\\"   \\\"Shouldn \\'t we try and help - \\\" Harry began awkwardly.   \\\"Trust me, \\\" said Ron.  \\\"We don \\'t want to be found here. \\\"  But it was too late. A rumble, as though of distant thunder, told them that the feast had just ended. From either end of the corridor where they stood came the sound of hundreds of feet climbing the stairs, and the loud, happy talk of well-fed people; next moment, students were crashing into the passage from both ends.  The chatter, the bustle, the noise died suddenly as the people in front spotted the hanging cat. Harry, Ron, and Hermione stood alone, in the middle of the corridor, as silence fell among the mass of students pressing forward to see the grisly sight.  Then someone shouted through the quiet.   \\\"Enemies of the Heir, beware! You \\'ll be next, Mudbloods! \\\"  It was Draco Malfoy. He had pushed to the front of the crowd, his cold eyes alive, his usually bloodless face flushed, as he grinned at the sight of the hanging, immobile cat. '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_text = sent_tokenize(CNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Occupy Boston protesters remained firmly entrenched in a downtown city square early Friday after a m'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pad_sentence(source_text[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Occupy Boston protesters remained firmly entrenched in a downtown city square early Friday after a mittle boaflus wouhd hear that she was not a coo of toenk of the tooe.\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(pad_sentence(source_text[0]), simple_small_model, char_to_int_simple, int_to_char_simple))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Occupy Boston protesters remained firmly entrenched in a downtown city square early Friday after a minute or two the was so this time she was now and then a rewp finished the sable, which say on the dourt.\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(pad_sentence(source_text[0]), simple_big_model, char_to_int_simple, int_to_char_simple))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Occupy Boston protesters remained firmly entrenched in a downtown city square early Friday after a mooe ttonne tooeen then the president ane the crumtry on the catiliac conrection of the countrys moreriin conneny continees th the president and the crumery caneoi to be a ronn oo that an inturle that the hornen of the cotntrys poaicr bod the ceal forte the soecel aod senered the porieat offece to\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(pad_sentence(source_text[0]), complex_small_model, char_to_int_complex, int_to_char_complex))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Occupy Boston protesters remained firmly entrenched in a downtown city square early Friday after a mande of the most country and the srump crisisi and a comment of the most on the country of the country of the country of the country of the country of the country of the country of the country of the country of the country of the country of the country of the country of the country of the country o\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(pad_sentence(source_text[0]), complex_big_model, char_to_int_complex, int_to_char_complex))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_name = ['cnn', 'usa-today', 'the-green-mile', 'harry-potter']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, raw_text in enumerate([CNN, USAToday, TheGreenMile, HarryPotter]):\n",
    "    source_text = sent_tokenize(raw_text)\n",
    "    simple_small = []\n",
    "    simple_big = []\n",
    "    complex_small = []\n",
    "    complex_big = []\n",
    "    for sent in source_text:\n",
    "        padded_sent = pad_sentence(sent)\n",
    "        \n",
    "        result = generate_text(padded_sent, simple_small_model, char_to_int_simple, int_to_char_simple)\n",
    "        simple_small.append(result)\n",
    "        \n",
    "        result = generate_text(padded_sent, simple_big_model, char_to_int_simple, int_to_char_simple)\n",
    "        simple_big.append(result)\n",
    "        \n",
    "        result = generate_text(padded_sent, complex_small_model, char_to_int_complex, int_to_char_complex)\n",
    "        complex_small.append(result)\n",
    "        \n",
    "        result = generate_text(padded_sent, complex_big_model, char_to_int_complex, int_to_char_complex)\n",
    "        complex_big.append(result)\n",
    "    simple_small = ' '.join(simple_small)\n",
    "    with open('build/generated-texts/{0}-char-based-simple-shallow.txt'.format(source_name[i]), 'w') as f:\n",
    "        f.write(simple_small)\n",
    "        \n",
    "    simple_big = ' '.join(simple_big)\n",
    "    with open('build/generated-texts/{0}-char-based-simple-deep.txt'.format(source_name[i]), 'w') as f:\n",
    "        f.write(simple_big)\n",
    "        \n",
    "    complex_small = ' '.join(complex_small)\n",
    "    with open('build/generated-texts/{0}-char-based-complex-shallow.txt'.format(source_name[i]), 'w') as f:\n",
    "        f.write(complex_small)\n",
    "        \n",
    "    complex_big = ' '.join(complex_big)\n",
    "    with open('build/generated-texts/{0}-char-based-complex-deep.txt'.format(source_name[i]), 'w') as f:\n",
    "        f.write(complex_big)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
