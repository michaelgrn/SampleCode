{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string, os\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import Sequential\n",
    "import keras.utils as ku\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from tqdm import tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "filename = 'data/wonderland.txt'\n",
    "with open(filename, 'r') as f:\n",
    "    raw_text = f.read().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "sentences = sent_tokenize(raw_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================0th sentence==============================\n",
      "alice’s adventures in wonderland\n",
      "\n",
      "lewis carroll\n",
      "\n",
      "the millennium fulcrum edition 3.0\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "chapter i. down the rabbit-hole\n",
      "\n",
      "alice was beginning to get very tired of sitting by her sister on the\n",
      "bank, and of having nothing to do: once or twice she had peeped into the\n",
      "book her sister was reading, but it had no pictures or conversations in\n",
      "it, ‘and what is the use of a book,’ thought alice ‘without pictures or\n",
      "conversations?’\n",
      "\n",
      "so she was considering in her own mind (as well as she could, for the\n",
      "hot day made her feel very sleepy and stupid), whether the pleasure\n",
      "of making a daisy-chain would be worth the trouble of getting up and\n",
      "picking the daisies, when suddenly a white rabbit with pink eyes ran\n",
      "close by her.\n",
      "==============================1th sentence==============================\n",
      "there was nothing so very remarkable in that; nor did alice think it so\n",
      "very much out of the way to hear the rabbit say to itself, ‘oh dear!\n",
      "==================================End===================================\n"
     ]
    }
   ],
   "source": [
    "for i, s in enumerate(sentences[:2]):\n",
    "    print(''.join(['=']*30 + ['{}th sentence'.format(i)] + ['=']*30))\n",
    "    print(s)\n",
    "print(''.join(['=']*34 + ['End'] + ['=']*35))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert sentences to ngrams for prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[318, 494],\n",
       " [318, 494, 11],\n",
       " [318, 494, 11, 889],\n",
       " [318, 494, 11, 889, 1595],\n",
       " [318, 494, 11, 889, 1595, 1596]]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize_sentences(sentences, tokenizer):\n",
    "    ## tokenization\n",
    "    tokenizer.fit_on_texts(sentences)\n",
    "    nwords = len(tokenizer.word_index) + 1\n",
    "    \n",
    "    # sequences of tokens \n",
    "    n_gram_sequences = []\n",
    "    for line in sentences:\n",
    "        token_list = tokenizer.texts_to_sequences([line])[0]\n",
    "        for i in range(1, len(token_list)):\n",
    "            n_gram_sequence = token_list[:i+1]\n",
    "            n_gram_sequences.append(n_gram_sequence)\n",
    "    return n_gram_sequences, nwords\n",
    "\n",
    "n_gram_sequences, nwords = tokenize_sentences(sentences, tokenizer)\n",
    "n_gram_sequences[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre-pad each sentence with zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def pad_sentences(n_gram_sequences, nwords):\n",
    "    max_lenghth = max(len(x) for x in n_gram_sequences)\n",
    "    padded_sequences = np.array(pad_sequences(n_gram_sequences, maxlen=max_lenghth, padding='pre'))\n",
    "    \n",
    "    input_sequences, labels = padded_sequences[:, :-1], padded_sequences[:, -1]\n",
    "    labels = ku.to_categorical(labels, num_classes=nwords)\n",
    "    return input_sequences, labels, max_lenghth\n",
    "\n",
    "seq_X, seq_y, max_sequence_len = pad_sentences(n_gram_sequences, nwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "277"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_sequence_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example X and y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text input: ['alice’s']  label: ['adventures']\n",
      "text input: ['alice’s adventures']  label: ['in']\n",
      "text input: ['alice’s adventures in']  label: ['wonderland']\n",
      "text input: [318]  label: 494\n",
      "text input: [318 494]  label: 11\n",
      "text input: [318 494  11]  label: 889\n"
     ]
    }
   ],
   "source": [
    "for s in n_gram_sequences[:3]:\n",
    "    print('text input:', tokenizer.sequences_to_texts([s[:-1]]), ' label:', tokenizer.sequences_to_texts([[s[-1]]]))\n",
    "    \n",
    "for i, s in enumerate(seq_X[:3]):\n",
    "    print('text input:', s[s.nonzero()], ' label:', seq_y[i].nonzero()[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def make_model(max_len, nwords, small=True):\n",
    "    input_len = max_len - 1\n",
    "    model = Sequential()\n",
    "    # add embedding layer\n",
    "    model.add(Embedding(nwords, 10, input_length=input_len))\n",
    "    if small:\n",
    "        model.add(LSTM(256))\n",
    "        model.add(Dropout(0.2))\n",
    "    else:\n",
    "        model.add(LSTM(256, return_sequences=True))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(LSTM(256))\n",
    "        model.add(Dropout(0.2))\n",
    "    model.add(Dense(nwords, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 276, 10)           29150     \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 256)               273408    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2915)              749155    \n",
      "=================================================================\n",
      "Total params: 1,051,713\n",
      "Trainable params: 1,051,713\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "26853/26853 [==============================] - 149s 6ms/step - loss: 6.2553\n",
      "\n",
      "Epoch 00001: loss improved from inf to 6.25534, saving model to build/word-based-weights-01-6.2553-simple-small.hdf5\n",
      "Epoch 2/50\n",
      "26853/26853 [==============================] - 150s 6ms/step - loss: 5.9032\n",
      "\n",
      "Epoch 00002: loss improved from 6.25534 to 5.90317, saving model to build/word-based-weights-02-5.9032-simple-small.hdf5\n",
      "Epoch 3/50\n",
      "26853/26853 [==============================] - 151s 6ms/step - loss: 5.6762\n",
      "\n",
      "Epoch 00003: loss improved from 5.90317 to 5.67617, saving model to build/word-based-weights-03-5.6762-simple-small.hdf5\n",
      "Epoch 4/50\n",
      "26853/26853 [==============================] - 150s 6ms/step - loss: 5.4661\n",
      "\n",
      "Epoch 00004: loss improved from 5.67617 to 5.46605, saving model to build/word-based-weights-04-5.4661-simple-small.hdf5\n",
      "Epoch 5/50\n",
      "26853/26853 [==============================] - 150s 6ms/step - loss: 5.2803\n",
      "\n",
      "Epoch 00005: loss improved from 5.46605 to 5.28032, saving model to build/word-based-weights-05-5.2803-simple-small.hdf5\n",
      "Epoch 6/50\n",
      "26853/26853 [==============================] - 150s 6ms/step - loss: 5.1046\n",
      "\n",
      "Epoch 00006: loss improved from 5.28032 to 5.10460, saving model to build/word-based-weights-06-5.1046-simple-small.hdf5\n",
      "Epoch 7/50\n",
      "26853/26853 [==============================] - 149s 6ms/step - loss: 4.9242\n",
      "\n",
      "Epoch 00007: loss improved from 5.10460 to 4.92416, saving model to build/word-based-weights-07-4.9242-simple-small.hdf5\n",
      "Epoch 8/50\n",
      "26853/26853 [==============================] - 149s 6ms/step - loss: 4.7343\n",
      "\n",
      "Epoch 00008: loss improved from 4.92416 to 4.73426, saving model to build/word-based-weights-08-4.7343-simple-small.hdf5\n",
      "Epoch 9/50\n",
      "26853/26853 [==============================] - 147s 5ms/step - loss: 4.5265\n",
      "\n",
      "Epoch 00009: loss improved from 4.73426 to 4.52653, saving model to build/word-based-weights-09-4.5265-simple-small.hdf5\n",
      "Epoch 10/50\n",
      "26853/26853 [==============================] - 144s 5ms/step - loss: 4.3242\n",
      "\n",
      "Epoch 00010: loss improved from 4.52653 to 4.32420, saving model to build/word-based-weights-10-4.3242-simple-small.hdf5\n",
      "Epoch 11/50\n",
      "26853/26853 [==============================] - 147s 5ms/step - loss: 4.1229\n",
      "\n",
      "Epoch 00011: loss improved from 4.32420 to 4.12286, saving model to build/word-based-weights-11-4.1229-simple-small.hdf5\n",
      "Epoch 12/50\n",
      "26853/26853 [==============================] - 145s 5ms/step - loss: 3.9261\n",
      "\n",
      "Epoch 00012: loss improved from 4.12286 to 3.92608, saving model to build/word-based-weights-12-3.9261-simple-small.hdf5\n",
      "Epoch 13/50\n",
      "26853/26853 [==============================] - 146s 5ms/step - loss: 3.7323\n",
      "\n",
      "Epoch 00013: loss improved from 3.92608 to 3.73227, saving model to build/word-based-weights-13-3.7323-simple-small.hdf5\n",
      "Epoch 14/50\n",
      "26853/26853 [==============================] - 147s 5ms/step - loss: 3.5540\n",
      "\n",
      "Epoch 00014: loss improved from 3.73227 to 3.55405, saving model to build/word-based-weights-14-3.5540-simple-small.hdf5\n",
      "Epoch 15/50\n",
      "26853/26853 [==============================] - 149s 6ms/step - loss: 3.3786\n",
      "\n",
      "Epoch 00015: loss improved from 3.55405 to 3.37856, saving model to build/word-based-weights-15-3.3786-simple-small.hdf5\n",
      "Epoch 16/50\n",
      "26853/26853 [==============================] - 148s 6ms/step - loss: 3.2229\n",
      "\n",
      "Epoch 00016: loss improved from 3.37856 to 3.22286, saving model to build/word-based-weights-16-3.2229-simple-small.hdf5\n",
      "Epoch 17/50\n",
      "26853/26853 [==============================] - 150s 6ms/step - loss: 3.0740\n",
      "\n",
      "Epoch 00017: loss improved from 3.22286 to 3.07399, saving model to build/word-based-weights-17-3.0740-simple-small.hdf5\n",
      "Epoch 18/50\n",
      "26853/26853 [==============================] - 148s 6ms/step - loss: 2.9331\n",
      "\n",
      "Epoch 00018: loss improved from 3.07399 to 2.93306, saving model to build/word-based-weights-18-2.9331-simple-small.hdf5\n",
      "Epoch 19/50\n",
      "26853/26853 [==============================] - 150s 6ms/step - loss: 2.7991\n",
      "\n",
      "Epoch 00019: loss improved from 2.93306 to 2.79908, saving model to build/word-based-weights-19-2.7991-simple-small.hdf5\n",
      "Epoch 20/50\n",
      "26853/26853 [==============================] - 148s 6ms/step - loss: 2.6862\n",
      "\n",
      "Epoch 00020: loss improved from 2.79908 to 2.68618, saving model to build/word-based-weights-20-2.6862-simple-small.hdf5\n",
      "Epoch 21/50\n",
      "26853/26853 [==============================] - 148s 6ms/step - loss: 2.5741\n",
      "\n",
      "Epoch 00021: loss improved from 2.68618 to 2.57415, saving model to build/word-based-weights-21-2.5741-simple-small.hdf5\n",
      "Epoch 22/50\n",
      "26853/26853 [==============================] - 149s 6ms/step - loss: 2.4661\n",
      "\n",
      "Epoch 00022: loss improved from 2.57415 to 2.46613, saving model to build/word-based-weights-22-2.4661-simple-small.hdf5\n",
      "Epoch 23/50\n",
      "26853/26853 [==============================] - 149s 6ms/step - loss: 2.3681\n",
      "\n",
      "Epoch 00023: loss improved from 2.46613 to 2.36811, saving model to build/word-based-weights-23-2.3681-simple-small.hdf5\n",
      "Epoch 24/50\n",
      "26853/26853 [==============================] - 149s 6ms/step - loss: 2.2767\n",
      "\n",
      "Epoch 00024: loss improved from 2.36811 to 2.27669, saving model to build/word-based-weights-24-2.2767-simple-small.hdf5\n",
      "Epoch 25/50\n",
      "26853/26853 [==============================] - 149s 6ms/step - loss: 2.1892\n",
      "\n",
      "Epoch 00025: loss improved from 2.27669 to 2.18923, saving model to build/word-based-weights-25-2.1892-simple-small.hdf5\n",
      "Epoch 26/50\n",
      "26853/26853 [==============================] - 149s 6ms/step - loss: 2.1059\n",
      "\n",
      "Epoch 00026: loss improved from 2.18923 to 2.10591, saving model to build/word-based-weights-26-2.1059-simple-small.hdf5\n",
      "Epoch 27/50\n",
      "26853/26853 [==============================] - 148s 6ms/step - loss: 2.0340\n",
      "\n",
      "Epoch 00027: loss improved from 2.10591 to 2.03395, saving model to build/word-based-weights-27-2.0340-simple-small.hdf5\n",
      "Epoch 28/50\n",
      "26853/26853 [==============================] - 147s 5ms/step - loss: 1.9653\n",
      "\n",
      "Epoch 00028: loss improved from 2.03395 to 1.96529, saving model to build/word-based-weights-28-1.9653-simple-small.hdf5\n",
      "Epoch 29/50\n",
      "26853/26853 [==============================] - 148s 6ms/step - loss: 1.8895\n",
      "\n",
      "Epoch 00029: loss improved from 1.96529 to 1.88946, saving model to build/word-based-weights-29-1.8895-simple-small.hdf5\n",
      "Epoch 30/50\n",
      "26853/26853 [==============================] - 148s 6ms/step - loss: 1.8276\n",
      "\n",
      "Epoch 00030: loss improved from 1.88946 to 1.82761, saving model to build/word-based-weights-30-1.8276-simple-small.hdf5\n",
      "Epoch 31/50\n",
      "26853/26853 [==============================] - 149s 6ms/step - loss: 1.7777\n",
      "\n",
      "Epoch 00031: loss improved from 1.82761 to 1.77766, saving model to build/word-based-weights-31-1.7777-simple-small.hdf5\n",
      "Epoch 32/50\n",
      "26853/26853 [==============================] - 149s 6ms/step - loss: 1.7113\n",
      "\n",
      "Epoch 00032: loss improved from 1.77766 to 1.71130, saving model to build/word-based-weights-32-1.7113-simple-small.hdf5\n",
      "Epoch 33/50\n",
      "26853/26853 [==============================] - 148s 6ms/step - loss: 1.6519\n",
      "\n",
      "Epoch 00033: loss improved from 1.71130 to 1.65194, saving model to build/word-based-weights-33-1.6519-simple-small.hdf5\n",
      "Epoch 34/50\n",
      "26853/26853 [==============================] - 148s 6ms/step - loss: 1.6033\n",
      "\n",
      "Epoch 00034: loss improved from 1.65194 to 1.60329, saving model to build/word-based-weights-34-1.6033-simple-small.hdf5\n",
      "Epoch 35/50\n",
      "26853/26853 [==============================] - 150s 6ms/step - loss: 1.5513\n",
      "\n",
      "Epoch 00035: loss improved from 1.60329 to 1.55135, saving model to build/word-based-weights-35-1.5513-simple-small.hdf5\n",
      "Epoch 36/50\n",
      "26853/26853 [==============================] - 149s 6ms/step - loss: 1.5039\n",
      "\n",
      "Epoch 00036: loss improved from 1.55135 to 1.50388, saving model to build/word-based-weights-36-1.5039-simple-small.hdf5\n",
      "Epoch 37/50\n",
      "26853/26853 [==============================] - 149s 6ms/step - loss: 1.4579\n",
      "\n",
      "Epoch 00037: loss improved from 1.50388 to 1.45794, saving model to build/word-based-weights-37-1.4579-simple-small.hdf5\n",
      "Epoch 38/50\n",
      "26853/26853 [==============================] - 148s 6ms/step - loss: 2.0228\n",
      "\n",
      "Epoch 00038: loss did not improve from 1.45794\n",
      "Epoch 39/50\n",
      "26853/26853 [==============================] - 149s 6ms/step - loss: 1.8114\n",
      "\n",
      "Epoch 00039: loss did not improve from 1.45794\n",
      "Epoch 40/50\n",
      "26853/26853 [==============================] - 149s 6ms/step - loss: 1.6022\n",
      "\n",
      "Epoch 00040: loss did not improve from 1.45794\n",
      "Epoch 41/50\n",
      "26853/26853 [==============================] - 148s 6ms/step - loss: 1.6173\n",
      "\n",
      "Epoch 00041: loss did not improve from 1.45794\n",
      "Epoch 42/50\n",
      "26853/26853 [==============================] - 150s 6ms/step - loss: 1.4572\n",
      "\n",
      "Epoch 00042: loss improved from 1.45794 to 1.45719, saving model to build/word-based-weights-42-1.4572-simple-small.hdf5\n",
      "Epoch 43/50\n",
      "26853/26853 [==============================] - 148s 6ms/step - loss: 1.3805\n",
      "\n",
      "Epoch 00043: loss improved from 1.45719 to 1.38052, saving model to build/word-based-weights-43-1.3805-simple-small.hdf5\n",
      "Epoch 44/50\n",
      "26853/26853 [==============================] - 149s 6ms/step - loss: 1.3337\n",
      "\n",
      "Epoch 00044: loss improved from 1.38052 to 1.33365, saving model to build/word-based-weights-44-1.3337-simple-small.hdf5\n",
      "Epoch 45/50\n",
      "26853/26853 [==============================] - 149s 6ms/step - loss: 1.2839\n",
      "\n",
      "Epoch 00045: loss improved from 1.33365 to 1.28387, saving model to build/word-based-weights-45-1.2839-simple-small.hdf5\n",
      "Epoch 46/50\n",
      "26853/26853 [==============================] - 149s 6ms/step - loss: 1.2509\n",
      "\n",
      "Epoch 00046: loss improved from 1.28387 to 1.25085, saving model to build/word-based-weights-46-1.2509-simple-small.hdf5\n",
      "Epoch 47/50\n",
      "26853/26853 [==============================] - 150s 6ms/step - loss: 1.2215\n",
      "\n",
      "Epoch 00047: loss improved from 1.25085 to 1.22149, saving model to build/word-based-weights-47-1.2215-simple-small.hdf5\n",
      "Epoch 48/50\n",
      "26853/26853 [==============================] - 148s 5ms/step - loss: 1.1842\n",
      "\n",
      "Epoch 00048: loss improved from 1.22149 to 1.18416, saving model to build/word-based-weights-48-1.1842-simple-small.hdf5\n",
      "Epoch 49/50\n",
      "26853/26853 [==============================] - 148s 6ms/step - loss: 1.1531\n",
      "\n",
      "Epoch 00049: loss improved from 1.18416 to 1.15306, saving model to build/word-based-weights-49-1.1531-simple-small.hdf5\n",
      "Epoch 50/50\n",
      "26853/26853 [==============================] - 148s 5ms/step - loss: 1.1499\n",
      "\n",
      "Epoch 00050: loss improved from 1.15306 to 1.14992, saving model to build/word-based-weights-50-1.1499-simple-small.hdf5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fd2288cec50>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_model = make_model(max_sequence_len, nwords)\n",
    "filepath=\"build/word-based-weights-{epoch:02d}-{loss:.4f}-simple-small.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]\n",
    "small_model.fit(seq_X, seq_y, epochs=50, batch_size=64, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 276, 10)           29150     \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 276, 256)          273408    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 276, 256)          0         \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 256)               525312    \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2915)              749155    \n",
      "=================================================================\n",
      "Total params: 1,577,025\n",
      "Trainable params: 1,577,025\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "26853/26853 [==============================] - 293s 11ms/step - loss: 6.4204\n",
      "\n",
      "Epoch 00001: loss improved from inf to 6.42044, saving model to build/word-based-weights-01-6.4204-simple-big.hdf5\n",
      "Epoch 2/50\n",
      "26853/26853 [==============================] - 291s 11ms/step - loss: 6.2409\n",
      "\n",
      "Epoch 00002: loss improved from 6.42044 to 6.24087, saving model to build/word-based-weights-02-6.2409-simple-big.hdf5\n",
      "Epoch 3/50\n",
      "26853/26853 [==============================] - 292s 11ms/step - loss: 6.2253\n",
      "\n",
      "Epoch 00003: loss improved from 6.24087 to 6.22525, saving model to build/word-based-weights-03-6.2253-simple-big.hdf5\n",
      "Epoch 4/50\n",
      "26853/26853 [==============================] - 295s 11ms/step - loss: 6.2132\n",
      "\n",
      "Epoch 00004: loss improved from 6.22525 to 6.21325, saving model to build/word-based-weights-04-6.2132-simple-big.hdf5\n",
      "Epoch 5/50\n",
      "26853/26853 [==============================] - 293s 11ms/step - loss: 6.2166\n",
      "\n",
      "Epoch 00005: loss did not improve from 6.21325\n",
      "Epoch 6/50\n",
      "26853/26853 [==============================] - 295s 11ms/step - loss: 6.2202\n",
      "\n",
      "Epoch 00006: loss did not improve from 6.21325\n",
      "Epoch 7/50\n",
      "26853/26853 [==============================] - 292s 11ms/step - loss: 6.2138\n",
      "\n",
      "Epoch 00007: loss did not improve from 6.21325\n",
      "Epoch 8/50\n",
      "26853/26853 [==============================] - 293s 11ms/step - loss: 6.2184\n",
      "\n",
      "Epoch 00008: loss did not improve from 6.21325\n",
      "Epoch 9/50\n",
      "26853/26853 [==============================] - 293s 11ms/step - loss: 6.2160\n",
      "\n",
      "Epoch 00009: loss did not improve from 6.21325\n",
      "Epoch 10/50\n",
      "26853/26853 [==============================] - 289s 11ms/step - loss: 6.2167\n",
      "\n",
      "Epoch 00010: loss did not improve from 6.21325\n",
      "Epoch 11/50\n",
      "26853/26853 [==============================] - 294s 11ms/step - loss: 6.2156\n",
      "\n",
      "Epoch 00011: loss did not improve from 6.21325\n",
      "Epoch 12/50\n",
      "26853/26853 [==============================] - 293s 11ms/step - loss: 6.2150\n",
      "\n",
      "Epoch 00012: loss did not improve from 6.21325\n",
      "Epoch 13/50\n",
      "26853/26853 [==============================] - 292s 11ms/step - loss: 6.2137\n",
      "\n",
      "Epoch 00013: loss did not improve from 6.21325\n",
      "Epoch 14/50\n",
      "26853/26853 [==============================] - 294s 11ms/step - loss: 6.2112\n",
      "\n",
      "Epoch 00014: loss improved from 6.21325 to 6.21121, saving model to build/word-based-weights-14-6.2112-simple-big.hdf5\n",
      "Epoch 15/50\n",
      "26853/26853 [==============================] - 293s 11ms/step - loss: 6.2161\n",
      "\n",
      "Epoch 00015: loss did not improve from 6.21121\n",
      "Epoch 16/50\n",
      "26853/26853 [==============================] - 293s 11ms/step - loss: 6.2151\n",
      "\n",
      "Epoch 00016: loss did not improve from 6.21121\n",
      "Epoch 17/50\n",
      "26853/26853 [==============================] - 291s 11ms/step - loss: 6.2143\n",
      "\n",
      "Epoch 00017: loss did not improve from 6.21121\n",
      "Epoch 18/50\n",
      "26853/26853 [==============================] - 292s 11ms/step - loss: 6.2179\n",
      "\n",
      "Epoch 00018: loss did not improve from 6.21121\n",
      "Epoch 19/50\n",
      "26853/26853 [==============================] - 296s 11ms/step - loss: 6.2151\n",
      "\n",
      "Epoch 00019: loss did not improve from 6.21121\n",
      "Epoch 20/50\n",
      "26853/26853 [==============================] - 293s 11ms/step - loss: 6.2178\n",
      "\n",
      "Epoch 00020: loss did not improve from 6.21121\n",
      "Epoch 21/50\n",
      "26853/26853 [==============================] - 295s 11ms/step - loss: 6.2125\n",
      "\n",
      "Epoch 00021: loss did not improve from 6.21121\n",
      "Epoch 22/50\n",
      "26853/26853 [==============================] - 291s 11ms/step - loss: 6.2175\n",
      "\n",
      "Epoch 00022: loss did not improve from 6.21121\n",
      "Epoch 23/50\n",
      "26853/26853 [==============================] - 292s 11ms/step - loss: 6.2177\n",
      "\n",
      "Epoch 00023: loss did not improve from 6.21121\n",
      "Epoch 24/50\n",
      "26853/26853 [==============================] - 292s 11ms/step - loss: 6.2185\n",
      "\n",
      "Epoch 00024: loss did not improve from 6.21121\n",
      "Epoch 25/50\n",
      "26853/26853 [==============================] - 292s 11ms/step - loss: 6.2172\n",
      "\n",
      "Epoch 00025: loss did not improve from 6.21121\n",
      "Epoch 26/50\n",
      "26853/26853 [==============================] - 290s 11ms/step - loss: 6.2174\n",
      "\n",
      "Epoch 00026: loss did not improve from 6.21121\n",
      "Epoch 27/50\n",
      "26853/26853 [==============================] - 293s 11ms/step - loss: 6.2154\n",
      "\n",
      "Epoch 00027: loss did not improve from 6.21121\n",
      "Epoch 28/50\n",
      "26853/26853 [==============================] - 291s 11ms/step - loss: 6.2154\n",
      "\n",
      "Epoch 00028: loss did not improve from 6.21121\n",
      "Epoch 29/50\n",
      "26853/26853 [==============================] - 292s 11ms/step - loss: 6.2148\n",
      "\n",
      "Epoch 00029: loss did not improve from 6.21121\n",
      "Epoch 30/50\n",
      "26853/26853 [==============================] - 292s 11ms/step - loss: 6.2092\n",
      "\n",
      "Epoch 00030: loss improved from 6.21121 to 6.20920, saving model to build/word-based-weights-30-6.2092-simple-big.hdf5\n",
      "Epoch 31/50\n",
      "26853/26853 [==============================] - 294s 11ms/step - loss: 6.2085\n",
      "\n",
      "Epoch 00031: loss improved from 6.20920 to 6.20849, saving model to build/word-based-weights-31-6.2085-simple-big.hdf5\n",
      "Epoch 32/50\n",
      "26853/26853 [==============================] - 295s 11ms/step - loss: 6.2118\n",
      "\n",
      "Epoch 00032: loss did not improve from 6.20849\n",
      "Epoch 33/50\n",
      "26853/26853 [==============================] - 292s 11ms/step - loss: 6.2094\n",
      "\n",
      "Epoch 00033: loss did not improve from 6.20849\n",
      "Epoch 34/50\n",
      "26853/26853 [==============================] - 295s 11ms/step - loss: 6.2071\n",
      "\n",
      "Epoch 00034: loss improved from 6.20849 to 6.20714, saving model to build/word-based-weights-34-6.2071-simple-big.hdf5\n",
      "Epoch 35/50\n",
      "26853/26853 [==============================] - 289s 11ms/step - loss: 6.2097\n",
      "\n",
      "Epoch 00035: loss did not improve from 6.20714\n",
      "Epoch 36/50\n",
      "26853/26853 [==============================] - 294s 11ms/step - loss: 6.2232\n",
      "\n",
      "Epoch 00036: loss did not improve from 6.20714\n",
      "Epoch 37/50\n",
      "26853/26853 [==============================] - 293s 11ms/step - loss: 6.2161\n",
      "\n",
      "Epoch 00037: loss did not improve from 6.20714\n",
      "Epoch 38/50\n",
      "26853/26853 [==============================] - 293s 11ms/step - loss: 6.2187\n",
      "\n",
      "Epoch 00038: loss did not improve from 6.20714\n",
      "Epoch 39/50\n",
      "26853/26853 [==============================] - 291s 11ms/step - loss: 6.2151\n",
      "\n",
      "Epoch 00039: loss did not improve from 6.20714\n",
      "Epoch 40/50\n",
      "26853/26853 [==============================] - 293s 11ms/step - loss: 6.2160\n",
      "\n",
      "Epoch 00040: loss did not improve from 6.20714\n",
      "Epoch 41/50\n",
      "26853/26853 [==============================] - 293s 11ms/step - loss: 6.2174\n",
      "\n",
      "Epoch 00041: loss did not improve from 6.20714\n",
      "Epoch 42/50\n",
      "26853/26853 [==============================] - 292s 11ms/step - loss: 6.2162\n",
      "\n",
      "Epoch 00042: loss did not improve from 6.20714\n",
      "Epoch 43/50\n",
      "26853/26853 [==============================] - 295s 11ms/step - loss: 6.2153\n",
      "\n",
      "Epoch 00043: loss did not improve from 6.20714\n",
      "Epoch 44/50\n",
      "26853/26853 [==============================] - 291s 11ms/step - loss: 6.2161\n",
      "\n",
      "Epoch 00044: loss did not improve from 6.20714\n",
      "Epoch 45/50\n",
      "26853/26853 [==============================] - 294s 11ms/step - loss: 6.2316\n",
      "\n",
      "Epoch 00045: loss did not improve from 6.20714\n",
      "Epoch 46/50\n",
      "26853/26853 [==============================] - 291s 11ms/step - loss: 6.2239\n",
      "\n",
      "Epoch 00046: loss did not improve from 6.20714\n",
      "Epoch 47/50\n",
      "26853/26853 [==============================] - 293s 11ms/step - loss: 6.2231\n",
      "\n",
      "Epoch 00047: loss did not improve from 6.20714\n",
      "Epoch 48/50\n",
      "26853/26853 [==============================] - 292s 11ms/step - loss: 6.2251\n",
      "\n",
      "Epoch 00048: loss did not improve from 6.20714\n",
      "Epoch 49/50\n",
      "26853/26853 [==============================] - 294s 11ms/step - loss: 6.2226\n",
      "\n",
      "Epoch 00049: loss did not improve from 6.20714\n",
      "Epoch 50/50\n",
      "26853/26853 [==============================] - 293s 11ms/step - loss: 6.2221\n",
      "\n",
      "Epoch 00050: loss did not improve from 6.20714\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fd210383c18>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "big_model = make_model(max_sequence_len, nwords, small=False)\n",
    "filepath=\"build/word-based-weights-{epoch:02d}-{loss:.4f}-simple-big.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]\n",
    "big_model.fit(seq_X, seq_y, epochs=50, batch_size=64, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train on news data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'data/nyt-snippet-subsets.txt'\n",
    "with open(filename, 'r') as f:\n",
    "    raw_text = f.read().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================0th sentence==============================\n",
      "sleep deprivation is linked to behavioral and mental health problems and car accident risk, experts say, and starting school later could help.\n",
      "==============================1th sentence==============================\n",
      "the musical — a love story set during the vietnam war — ignited a fierce debate over the casting of a white actor in a eurasian role.\n",
      "==================================End===================================\n"
     ]
    }
   ],
   "source": [
    "sentences = sent_tokenize(raw_text)\n",
    "\n",
    "for i, s in enumerate(sentences[:2]):\n",
    "    print(''.join(['=']*30 + ['{}th sentence'.format(i)] + ['=']*30))\n",
    "    print(s)\n",
    "print(''.join(['=']*34 + ['End'] + ['=']*35))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[2421, 2422],\n",
       " [2421, 2422, 8],\n",
       " [2421, 2422, 8, 1426],\n",
       " [2421, 2422, 8, 1426, 3],\n",
       " [2421, 2422, 8, 1426, 3, 2423]]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_gram_sequences, nwords = tokenize_sentences(sentences, tokenizer)\n",
    "n_gram_sequences[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text input: ['sleep']  label: ['deprivation']\n",
      "text input: ['sleep deprivation']  label: ['is']\n",
      "text input: ['sleep deprivation is']  label: ['linked']\n",
      "text input: [2421]  label: 2422\n",
      "text input: [2421 2422]  label: 8\n",
      "text input: [2421 2422    8]  label: 1426\n"
     ]
    }
   ],
   "source": [
    "seq_X, seq_y, max_sequence_len = pad_sentences(n_gram_sequences, nwords)\n",
    "\n",
    "for s in n_gram_sequences[:3]:\n",
    "    print('text input:', tokenizer.sequences_to_texts([s[:-1]]), ' label:', tokenizer.sequences_to_texts([[s[-1]]]))\n",
    "    \n",
    "for i, s in enumerate(seq_X[:3]):\n",
    "    print('text input:', s[s.nonzero()], ' label:', seq_y[i].nonzero()[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 64, 10)            63300     \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 256)               273408    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 6330)              1626810   \n",
      "=================================================================\n",
      "Total params: 1,963,518\n",
      "Trainable params: 1,963,518\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "22620/22620 [==============================] - 41s 2ms/step - loss: 7.5416\n",
      "\n",
      "Epoch 00001: loss improved from inf to 7.54164, saving model to build/word-based-weights-01-7.5416-complex-small.hdf5\n",
      "Epoch 2/50\n",
      "22620/22620 [==============================] - 34s 2ms/step - loss: 7.3256\n",
      "\n",
      "Epoch 00002: loss improved from 7.54164 to 7.32561, saving model to build/word-based-weights-02-7.3256-complex-small.hdf5\n",
      "Epoch 3/50\n",
      "22620/22620 [==============================] - 34s 2ms/step - loss: 7.0095\n",
      "\n",
      "Epoch 00003: loss improved from 7.32561 to 7.00953, saving model to build/word-based-weights-03-7.0095-complex-small.hdf5\n",
      "Epoch 4/50\n",
      "22620/22620 [==============================] - 35s 2ms/step - loss: 6.8610\n",
      "\n",
      "Epoch 00004: loss improved from 7.00953 to 6.86096, saving model to build/word-based-weights-04-6.8610-complex-small.hdf5\n",
      "Epoch 5/50\n",
      "22620/22620 [==============================] - 37s 2ms/step - loss: 6.6976\n",
      "\n",
      "Epoch 00005: loss improved from 6.86096 to 6.69758, saving model to build/word-based-weights-05-6.6976-complex-small.hdf5\n",
      "Epoch 6/50\n",
      "22620/22620 [==============================] - 37s 2ms/step - loss: 6.5364\n",
      "\n",
      "Epoch 00006: loss improved from 6.69758 to 6.53640, saving model to build/word-based-weights-06-6.5364-complex-small.hdf5\n",
      "Epoch 7/50\n",
      "22620/22620 [==============================] - 36s 2ms/step - loss: 6.3651\n",
      "\n",
      "Epoch 00007: loss improved from 6.53640 to 6.36508, saving model to build/word-based-weights-07-6.3651-complex-small.hdf5\n",
      "Epoch 8/50\n",
      "22620/22620 [==============================] - 37s 2ms/step - loss: 6.1557\n",
      "\n",
      "Epoch 00008: loss improved from 6.36508 to 6.15575, saving model to build/word-based-weights-08-6.1557-complex-small.hdf5\n",
      "Epoch 9/50\n",
      "22620/22620 [==============================] - 37s 2ms/step - loss: 5.9184\n",
      "\n",
      "Epoch 00009: loss improved from 6.15575 to 5.91838, saving model to build/word-based-weights-09-5.9184-complex-small.hdf5\n",
      "Epoch 10/50\n",
      "22620/22620 [==============================] - 37s 2ms/step - loss: 5.6532\n",
      "\n",
      "Epoch 00010: loss improved from 5.91838 to 5.65320, saving model to build/word-based-weights-10-5.6532-complex-small.hdf5\n",
      "Epoch 11/50\n",
      "22620/22620 [==============================] - 37s 2ms/step - loss: 5.3718\n",
      "\n",
      "Epoch 00011: loss improved from 5.65320 to 5.37182, saving model to build/word-based-weights-11-5.3718-complex-small.hdf5\n",
      "Epoch 12/50\n",
      "22620/22620 [==============================] - 36s 2ms/step - loss: 5.0878\n",
      "\n",
      "Epoch 00012: loss improved from 5.37182 to 5.08777, saving model to build/word-based-weights-12-5.0878-complex-small.hdf5\n",
      "Epoch 13/50\n",
      "22620/22620 [==============================] - 36s 2ms/step - loss: 4.8092\n",
      "\n",
      "Epoch 00013: loss improved from 5.08777 to 4.80916, saving model to build/word-based-weights-13-4.8092-complex-small.hdf5\n",
      "Epoch 14/50\n",
      "22620/22620 [==============================] - 37s 2ms/step - loss: 4.5431\n",
      "\n",
      "Epoch 00014: loss improved from 4.80916 to 4.54313, saving model to build/word-based-weights-14-4.5431-complex-small.hdf5\n",
      "Epoch 15/50\n",
      "22620/22620 [==============================] - 37s 2ms/step - loss: 4.2850\n",
      "\n",
      "Epoch 00015: loss improved from 4.54313 to 4.28500, saving model to build/word-based-weights-15-4.2850-complex-small.hdf5\n",
      "Epoch 16/50\n",
      "22620/22620 [==============================] - 37s 2ms/step - loss: 4.0394\n",
      "\n",
      "Epoch 00016: loss improved from 4.28500 to 4.03941, saving model to build/word-based-weights-16-4.0394-complex-small.hdf5\n",
      "Epoch 17/50\n",
      "22620/22620 [==============================] - 38s 2ms/step - loss: 3.8144\n",
      "\n",
      "Epoch 00017: loss improved from 4.03941 to 3.81438, saving model to build/word-based-weights-17-3.8144-complex-small.hdf5\n",
      "Epoch 18/50\n",
      "22620/22620 [==============================] - 37s 2ms/step - loss: 3.5948\n",
      "\n",
      "Epoch 00018: loss improved from 3.81438 to 3.59477, saving model to build/word-based-weights-18-3.5948-complex-small.hdf5\n",
      "Epoch 19/50\n",
      "22620/22620 [==============================] - 37s 2ms/step - loss: 3.3960\n",
      "\n",
      "Epoch 00019: loss improved from 3.59477 to 3.39599, saving model to build/word-based-weights-19-3.3960-complex-small.hdf5\n",
      "Epoch 20/50\n",
      "22620/22620 [==============================] - 37s 2ms/step - loss: 3.2207\n",
      "\n",
      "Epoch 00020: loss improved from 3.39599 to 3.22069, saving model to build/word-based-weights-20-3.2207-complex-small.hdf5\n",
      "Epoch 21/50\n",
      "22620/22620 [==============================] - 37s 2ms/step - loss: 3.0594\n",
      "\n",
      "Epoch 00021: loss improved from 3.22069 to 3.05942, saving model to build/word-based-weights-21-3.0594-complex-small.hdf5\n",
      "Epoch 22/50\n",
      "22620/22620 [==============================] - 37s 2ms/step - loss: 2.8993\n",
      "\n",
      "Epoch 00022: loss improved from 3.05942 to 2.89929, saving model to build/word-based-weights-22-2.8993-complex-small.hdf5\n",
      "Epoch 23/50\n",
      "22620/22620 [==============================] - 37s 2ms/step - loss: 2.7649\n",
      "\n",
      "Epoch 00023: loss improved from 2.89929 to 2.76490, saving model to build/word-based-weights-23-2.7649-complex-small.hdf5\n",
      "Epoch 24/50\n",
      "22620/22620 [==============================] - 37s 2ms/step - loss: 2.6364\n",
      "\n",
      "Epoch 00024: loss improved from 2.76490 to 2.63641, saving model to build/word-based-weights-24-2.6364-complex-small.hdf5\n",
      "Epoch 25/50\n",
      "22620/22620 [==============================] - 37s 2ms/step - loss: 2.5133\n",
      "\n",
      "Epoch 00025: loss improved from 2.63641 to 2.51327, saving model to build/word-based-weights-25-2.5133-complex-small.hdf5\n",
      "Epoch 26/50\n",
      "22620/22620 [==============================] - 36s 2ms/step - loss: 2.4087\n",
      "\n",
      "Epoch 00026: loss improved from 2.51327 to 2.40871, saving model to build/word-based-weights-26-2.4087-complex-small.hdf5\n",
      "Epoch 27/50\n",
      "22620/22620 [==============================] - 37s 2ms/step - loss: 2.3103\n",
      "\n",
      "Epoch 00027: loss improved from 2.40871 to 2.31030, saving model to build/word-based-weights-27-2.3103-complex-small.hdf5\n",
      "Epoch 28/50\n",
      "22620/22620 [==============================] - 37s 2ms/step - loss: 2.2105\n",
      "\n",
      "Epoch 00028: loss improved from 2.31030 to 2.21051, saving model to build/word-based-weights-28-2.2105-complex-small.hdf5\n",
      "Epoch 29/50\n",
      "22620/22620 [==============================] - 36s 2ms/step - loss: 2.1201\n",
      "\n",
      "Epoch 00029: loss improved from 2.21051 to 2.12011, saving model to build/word-based-weights-29-2.1201-complex-small.hdf5\n",
      "Epoch 30/50\n",
      "22620/22620 [==============================] - 37s 2ms/step - loss: 2.3926\n",
      "\n",
      "Epoch 00030: loss did not improve from 2.12011\n",
      "Epoch 31/50\n",
      "22620/22620 [==============================] - 37s 2ms/step - loss: 2.1487\n",
      "\n",
      "Epoch 00031: loss did not improve from 2.12011\n",
      "Epoch 32/50\n",
      "22620/22620 [==============================] - 38s 2ms/step - loss: 1.9588\n",
      "\n",
      "Epoch 00032: loss improved from 2.12011 to 1.95877, saving model to build/word-based-weights-32-1.9588-complex-small.hdf5\n",
      "Epoch 33/50\n",
      "22620/22620 [==============================] - 37s 2ms/step - loss: 1.8869\n",
      "\n",
      "Epoch 00033: loss improved from 1.95877 to 1.88692, saving model to build/word-based-weights-33-1.8869-complex-small.hdf5\n",
      "Epoch 34/50\n",
      "22620/22620 [==============================] - 37s 2ms/step - loss: 1.8161\n",
      "\n",
      "Epoch 00034: loss improved from 1.88692 to 1.81611, saving model to build/word-based-weights-34-1.8161-complex-small.hdf5\n",
      "Epoch 35/50\n",
      "22620/22620 [==============================] - 37s 2ms/step - loss: 1.7238\n",
      "\n",
      "Epoch 00035: loss improved from 1.81611 to 1.72384, saving model to build/word-based-weights-35-1.7238-complex-small.hdf5\n",
      "Epoch 36/50\n",
      "22620/22620 [==============================] - 37s 2ms/step - loss: 1.6782\n",
      "\n",
      "Epoch 00036: loss improved from 1.72384 to 1.67819, saving model to build/word-based-weights-36-1.6782-complex-small.hdf5\n",
      "Epoch 37/50\n",
      "22620/22620 [==============================] - 37s 2ms/step - loss: 1.6211\n",
      "\n",
      "Epoch 00037: loss improved from 1.67819 to 1.62106, saving model to build/word-based-weights-37-1.6211-complex-small.hdf5\n",
      "Epoch 38/50\n",
      "22620/22620 [==============================] - 38s 2ms/step - loss: 1.6032\n",
      "\n",
      "Epoch 00038: loss improved from 1.62106 to 1.60321, saving model to build/word-based-weights-38-1.6032-complex-small.hdf5\n",
      "Epoch 39/50\n",
      "22620/22620 [==============================] - 37s 2ms/step - loss: 1.5807\n",
      "\n",
      "Epoch 00039: loss improved from 1.60321 to 1.58067, saving model to build/word-based-weights-39-1.5807-complex-small.hdf5\n",
      "Epoch 40/50\n",
      "22620/22620 [==============================] - 37s 2ms/step - loss: 1.4836\n",
      "\n",
      "Epoch 00040: loss improved from 1.58067 to 1.48363, saving model to build/word-based-weights-40-1.4836-complex-small.hdf5\n",
      "Epoch 41/50\n",
      "22620/22620 [==============================] - 37s 2ms/step - loss: 1.4423\n",
      "\n",
      "Epoch 00041: loss improved from 1.48363 to 1.44233, saving model to build/word-based-weights-41-1.4423-complex-small.hdf5\n",
      "Epoch 42/50\n",
      "22620/22620 [==============================] - 37s 2ms/step - loss: 1.3947\n",
      "\n",
      "Epoch 00042: loss improved from 1.44233 to 1.39466, saving model to build/word-based-weights-42-1.3947-complex-small.hdf5\n",
      "Epoch 43/50\n",
      "22620/22620 [==============================] - 37s 2ms/step - loss: 1.3581\n",
      "\n",
      "Epoch 00043: loss improved from 1.39466 to 1.35814, saving model to build/word-based-weights-43-1.3581-complex-small.hdf5\n",
      "Epoch 44/50\n",
      "22620/22620 [==============================] - 38s 2ms/step - loss: 1.3157\n",
      "\n",
      "Epoch 00044: loss improved from 1.35814 to 1.31571, saving model to build/word-based-weights-44-1.3157-complex-small.hdf5\n",
      "Epoch 45/50\n",
      "22620/22620 [==============================] - 37s 2ms/step - loss: 1.2805\n",
      "\n",
      "Epoch 00045: loss improved from 1.31571 to 1.28053, saving model to build/word-based-weights-45-1.2805-complex-small.hdf5\n",
      "Epoch 46/50\n",
      "22620/22620 [==============================] - 37s 2ms/step - loss: 1.2429\n",
      "\n",
      "Epoch 00046: loss improved from 1.28053 to 1.24287, saving model to build/word-based-weights-46-1.2429-complex-small.hdf5\n",
      "Epoch 47/50\n",
      "22620/22620 [==============================] - 38s 2ms/step - loss: 1.2076\n",
      "\n",
      "Epoch 00047: loss improved from 1.24287 to 1.20762, saving model to build/word-based-weights-47-1.2076-complex-small.hdf5\n",
      "Epoch 48/50\n",
      "22620/22620 [==============================] - 37s 2ms/step - loss: 1.1717\n",
      "\n",
      "Epoch 00048: loss improved from 1.20762 to 1.17172, saving model to build/word-based-weights-48-1.1717-complex-small.hdf5\n",
      "Epoch 49/50\n",
      "22620/22620 [==============================] - 37s 2ms/step - loss: 1.1437\n",
      "\n",
      "Epoch 00049: loss improved from 1.17172 to 1.14365, saving model to build/word-based-weights-49-1.1437-complex-small.hdf5\n",
      "Epoch 50/50\n",
      "22620/22620 [==============================] - 36s 2ms/step - loss: 1.1208\n",
      "\n",
      "Epoch 00050: loss improved from 1.14365 to 1.12080, saving model to build/word-based-weights-50-1.1208-complex-small.hdf5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f017073e908>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_model = make_model(max_sequence_len, nwords)\n",
    "filepath=\"build/word-based-weights-{epoch:02d}-{loss:.4f}-complex-small.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]\n",
    "small_model.fit(seq_X, seq_y, epochs=50, batch_size=64, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 64, 10)            63300     \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 64, 256)           273408    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 64, 256)           0         \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 256)               525312    \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 6330)              1626810   \n",
      "=================================================================\n",
      "Total params: 2,488,830\n",
      "Trainable params: 2,488,830\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "22620/22620 [==============================] - 67s 3ms/step - loss: 7.9981\n",
      "\n",
      "Epoch 00001: loss improved from inf to 7.99813, saving model to build/word-based-weights-01-7.9981-complex-big.hdf5\n",
      "Epoch 2/50\n",
      "22620/22620 [==============================] - 64s 3ms/step - loss: 7.6884\n",
      "\n",
      "Epoch 00002: loss improved from 7.99813 to 7.68836, saving model to build/word-based-weights-02-7.6884-complex-big.hdf5\n",
      "Epoch 3/50\n",
      "22620/22620 [==============================] - 67s 3ms/step - loss: 7.6412\n",
      "\n",
      "Epoch 00003: loss improved from 7.68836 to 7.64120, saving model to build/word-based-weights-03-7.6412-complex-big.hdf5\n",
      "Epoch 4/50\n",
      "22620/22620 [==============================] - 65s 3ms/step - loss: 7.6220\n",
      "\n",
      "Epoch 00004: loss improved from 7.64120 to 7.62195, saving model to build/word-based-weights-04-7.6220-complex-big.hdf5\n",
      "Epoch 5/50\n",
      "22620/22620 [==============================] - 67s 3ms/step - loss: 7.6252\n",
      "\n",
      "Epoch 00005: loss did not improve from 7.62195\n",
      "Epoch 6/50\n",
      "22620/22620 [==============================] - 67s 3ms/step - loss: 7.6338\n",
      "\n",
      "Epoch 00006: loss did not improve from 7.62195\n",
      "Epoch 7/50\n",
      "22620/22620 [==============================] - 66s 3ms/step - loss: 7.6454\n",
      "\n",
      "Epoch 00007: loss did not improve from 7.62195\n",
      "Epoch 8/50\n",
      "22620/22620 [==============================] - 65s 3ms/step - loss: 7.6404\n",
      "\n",
      "Epoch 00008: loss did not improve from 7.62195\n",
      "Epoch 9/50\n",
      "22620/22620 [==============================] - 67s 3ms/step - loss: 7.6421\n",
      "\n",
      "Epoch 00009: loss did not improve from 7.62195\n",
      "Epoch 10/50\n",
      "22620/22620 [==============================] - 67s 3ms/step - loss: 7.6440\n",
      "\n",
      "Epoch 00010: loss did not improve from 7.62195\n",
      "Epoch 11/50\n",
      "22620/22620 [==============================] - 66s 3ms/step - loss: 7.6431\n",
      "\n",
      "Epoch 00011: loss did not improve from 7.62195\n",
      "Epoch 12/50\n",
      "22620/22620 [==============================] - 68s 3ms/step - loss: 7.6392\n",
      "\n",
      "Epoch 00012: loss did not improve from 7.62195\n",
      "Epoch 13/50\n",
      "22620/22620 [==============================] - 64s 3ms/step - loss: 7.6432\n",
      "\n",
      "Epoch 00013: loss did not improve from 7.62195\n",
      "Epoch 14/50\n",
      "22620/22620 [==============================] - 66s 3ms/step - loss: 7.6420\n",
      "\n",
      "Epoch 00014: loss did not improve from 7.62195\n",
      "Epoch 15/50\n",
      "22620/22620 [==============================] - 67s 3ms/step - loss: 7.6463\n",
      "\n",
      "Epoch 00015: loss did not improve from 7.62195\n",
      "Epoch 16/50\n",
      "22620/22620 [==============================] - 65s 3ms/step - loss: 7.6403\n",
      "\n",
      "Epoch 00016: loss did not improve from 7.62195\n",
      "Epoch 17/50\n",
      "22620/22620 [==============================] - 65s 3ms/step - loss: 7.6347\n",
      "\n",
      "Epoch 00017: loss did not improve from 7.62195\n",
      "Epoch 18/50\n",
      "22620/22620 [==============================] - 65s 3ms/step - loss: 7.6419\n",
      "\n",
      "Epoch 00018: loss did not improve from 7.62195\n",
      "Epoch 19/50\n",
      "22620/22620 [==============================] - 67s 3ms/step - loss: 7.6413\n",
      "\n",
      "Epoch 00019: loss did not improve from 7.62195\n",
      "Epoch 20/50\n",
      "22620/22620 [==============================] - 66s 3ms/step - loss: 7.6345\n",
      "\n",
      "Epoch 00020: loss did not improve from 7.62195\n",
      "Epoch 21/50\n",
      "22620/22620 [==============================] - 66s 3ms/step - loss: 7.6375\n",
      "\n",
      "Epoch 00021: loss did not improve from 7.62195\n",
      "Epoch 22/50\n",
      "22620/22620 [==============================] - 66s 3ms/step - loss: 7.6391\n",
      "\n",
      "Epoch 00022: loss did not improve from 7.62195\n",
      "Epoch 23/50\n",
      "22620/22620 [==============================] - 67s 3ms/step - loss: 7.6414\n",
      "\n",
      "Epoch 00023: loss did not improve from 7.62195\n",
      "Epoch 24/50\n",
      "22620/22620 [==============================] - 65s 3ms/step - loss: 7.6359\n",
      "\n",
      "Epoch 00024: loss did not improve from 7.62195\n",
      "Epoch 25/50\n",
      "22620/22620 [==============================] - 67s 3ms/step - loss: 7.6396\n",
      "\n",
      "Epoch 00025: loss did not improve from 7.62195\n",
      "Epoch 26/50\n",
      "22620/22620 [==============================] - 66s 3ms/step - loss: 7.6423\n",
      "\n",
      "Epoch 00026: loss did not improve from 7.62195\n",
      "Epoch 27/50\n",
      "22620/22620 [==============================] - 66s 3ms/step - loss: 7.6417\n",
      "\n",
      "Epoch 00027: loss did not improve from 7.62195\n",
      "Epoch 28/50\n",
      "22620/22620 [==============================] - 66s 3ms/step - loss: 7.6433\n",
      "\n",
      "Epoch 00028: loss did not improve from 7.62195\n",
      "Epoch 29/50\n",
      "22620/22620 [==============================] - 67s 3ms/step - loss: 7.6400\n",
      "\n",
      "Epoch 00029: loss did not improve from 7.62195\n",
      "Epoch 30/50\n",
      "22620/22620 [==============================] - 67s 3ms/step - loss: 7.6379\n",
      "\n",
      "Epoch 00030: loss did not improve from 7.62195\n",
      "Epoch 31/50\n",
      "22620/22620 [==============================] - 67s 3ms/step - loss: 7.6376\n",
      "\n",
      "Epoch 00031: loss did not improve from 7.62195\n",
      "Epoch 32/50\n",
      "22620/22620 [==============================] - 66s 3ms/step - loss: 7.6394\n",
      "\n",
      "Epoch 00032: loss did not improve from 7.62195\n",
      "Epoch 33/50\n",
      "22620/22620 [==============================] - 67s 3ms/step - loss: 7.6394\n",
      "\n",
      "Epoch 00033: loss did not improve from 7.62195\n",
      "Epoch 34/50\n",
      "22620/22620 [==============================] - 65s 3ms/step - loss: 7.6369\n",
      "\n",
      "Epoch 00034: loss did not improve from 7.62195\n",
      "Epoch 35/50\n",
      "22620/22620 [==============================] - 65s 3ms/step - loss: 7.6445\n",
      "\n",
      "Epoch 00035: loss did not improve from 7.62195\n",
      "Epoch 36/50\n",
      "22620/22620 [==============================] - 66s 3ms/step - loss: 7.6368\n",
      "\n",
      "Epoch 00036: loss did not improve from 7.62195\n",
      "Epoch 37/50\n",
      "22620/22620 [==============================] - 65s 3ms/step - loss: 7.6371\n",
      "\n",
      "Epoch 00037: loss did not improve from 7.62195\n",
      "Epoch 38/50\n",
      "22620/22620 [==============================] - 64s 3ms/step - loss: 7.6395\n",
      "\n",
      "Epoch 00038: loss did not improve from 7.62195\n",
      "Epoch 39/50\n",
      "22620/22620 [==============================] - 65s 3ms/step - loss: 7.6366\n",
      "\n",
      "Epoch 00039: loss did not improve from 7.62195\n",
      "Epoch 40/50\n",
      "22620/22620 [==============================] - 64s 3ms/step - loss: 7.6442\n",
      "\n",
      "Epoch 00040: loss did not improve from 7.62195\n",
      "Epoch 41/50\n",
      "22620/22620 [==============================] - 65s 3ms/step - loss: 7.6412\n",
      "\n",
      "Epoch 00041: loss did not improve from 7.62195\n",
      "Epoch 42/50\n",
      "22620/22620 [==============================] - 65s 3ms/step - loss: 7.6358\n",
      "\n",
      "Epoch 00042: loss did not improve from 7.62195\n",
      "Epoch 43/50\n",
      "22620/22620 [==============================] - 65s 3ms/step - loss: 7.6412\n",
      "\n",
      "Epoch 00043: loss did not improve from 7.62195\n",
      "Epoch 44/50\n",
      "22620/22620 [==============================] - 63s 3ms/step - loss: 7.6390\n",
      "\n",
      "Epoch 00044: loss did not improve from 7.62195\n",
      "Epoch 45/50\n",
      "22620/22620 [==============================] - 65s 3ms/step - loss: 7.6459\n",
      "\n",
      "Epoch 00045: loss did not improve from 7.62195\n",
      "Epoch 46/50\n",
      "22620/22620 [==============================] - 65s 3ms/step - loss: 7.6415\n",
      "\n",
      "Epoch 00046: loss did not improve from 7.62195\n",
      "Epoch 47/50\n",
      "22620/22620 [==============================] - 65s 3ms/step - loss: 7.6417\n",
      "\n",
      "Epoch 00047: loss did not improve from 7.62195\n",
      "Epoch 48/50\n",
      "22620/22620 [==============================] - 63s 3ms/step - loss: 7.6461\n",
      "\n",
      "Epoch 00048: loss did not improve from 7.62195\n",
      "Epoch 49/50\n",
      "22620/22620 [==============================] - 66s 3ms/step - loss: 7.6430\n",
      "\n",
      "Epoch 00049: loss did not improve from 7.62195\n",
      "Epoch 50/50\n",
      "22620/22620 [==============================] - 65s 3ms/step - loss: 7.6423\n",
      "\n",
      "Epoch 00050: loss did not improve from 7.62195\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f015e74b390>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "big_model = make_model(max_sequence_len, nwords, small=False)\n",
    "filepath=\"build/word-based-weights-{epoch:02d}-{loss:.4f}-complex-big.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]\n",
    "big_model.fit(seq_X, seq_y, epochs=50, batch_size=64, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_sentences(sentences, tokenizer):\n",
    "    ## tokenization\n",
    "    tokenizer.fit_on_texts(sentences)\n",
    "    nwords = len(tokenizer.word_index) + 1\n",
    "    \n",
    "    # sequences of tokens \n",
    "    n_gram_sequences = []\n",
    "    for line in sentences:\n",
    "        token_list = tokenizer.texts_to_sequences([line])[0]\n",
    "        for i in range(1, len(token_list)):\n",
    "            n_gram_sequence = token_list[:i+1]\n",
    "            n_gram_sequences.append(n_gram_sequence)\n",
    "    return n_gram_sequences, nwords\n",
    "\n",
    "def pad_sentences(n_gram_sequences, nwords):\n",
    "    max_lenghth = max(len(x) for x in n_gram_sequences)\n",
    "    padded_sequences = np.array(pad_sequences(n_gram_sequences,\n",
    "                                              maxlen=max_lenghth,\n",
    "                                              padding='pre'))\n",
    "    \n",
    "    input_sequences, labels = padded_sequences[:, :-1], padded_sequences[:, -1]\n",
    "    labels = ku.to_categorical(labels, num_classes=nwords)\n",
    "    return input_sequences, labels, max_lenghth\n",
    "\n",
    "def make_model(max_len, nwords, small=True):\n",
    "    input_len = max_len - 1\n",
    "    model = Sequential()\n",
    "    # add embedding layer\n",
    "    model.add(Embedding(nwords, 10, input_length=input_len))\n",
    "    if small:\n",
    "        model.add(LSTM(256))\n",
    "        model.add(Dropout(0.2))\n",
    "    else:\n",
    "        model.add(LSTM(256, return_sequences=True))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(LSTM(256))\n",
    "        model.add(Dropout(0.2))\n",
    "    model.add(Dense(nwords, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load models -- models trained on simple texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 276, 10)           29150     \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 256)               273408    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2915)              749155    \n",
      "=================================================================\n",
      "Total params: 1,051,713\n",
      "Trainable params: 1,051,713\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 276, 10)           29150     \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 276, 256)          273408    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 276, 256)          0         \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 256)               525312    \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2915)              749155    \n",
      "=================================================================\n",
      "Total params: 1,577,025\n",
      "Trainable params: 1,577,025\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "filename = 'data/wonderland.txt'\n",
    "with open(filename, 'r') as f:\n",
    "    raw_text = f.read().lower()\n",
    "sentences = sent_tokenize(raw_text)\n",
    "tokenizer_simple = Tokenizer()\n",
    "n_gram_sequences, nwords = tokenize_sentences(sentences, tokenizer_simple)\n",
    "seq_X, seq_y, max_len_simple = pad_sentences(n_gram_sequences, nwords)\n",
    "\n",
    "simple_small_model = make_model(max_len_simple, nwords)\n",
    "filename = \"build/word-based-weights-50-1.1499-simple-small.hdf5\"\n",
    "simple_small_model.load_weights(filename)\n",
    "simple_small_model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "simple_big_model = make_model(max_len_simple, nwords, small=False)\n",
    "filename = \"build/word-based-weights-34-6.2071-simple-big.hdf5\"\n",
    "simple_big_model.load_weights(filename)\n",
    "simple_big_model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Models trained on complex texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 64, 10)            63300     \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 256)               273408    \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 6330)              1626810   \n",
      "=================================================================\n",
      "Total params: 1,963,518\n",
      "Trainable params: 1,963,518\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, 64, 10)            63300     \n",
      "_________________________________________________________________\n",
      "lstm_5 (LSTM)                (None, 64, 256)           273408    \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 64, 256)           0         \n",
      "_________________________________________________________________\n",
      "lstm_6 (LSTM)                (None, 256)               525312    \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 6330)              1626810   \n",
      "=================================================================\n",
      "Total params: 2,488,830\n",
      "Trainable params: 2,488,830\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "filename = 'data/nyt-snippet-subsets.txt'\n",
    "with open(filename, 'r') as f:\n",
    "    raw_text = f.read().lower()\n",
    "sentences = sent_tokenize(raw_text)\n",
    "tokenizer_complex = Tokenizer()\n",
    "n_gram_sequences, nwords = tokenize_sentences(sentences, tokenizer_complex)\n",
    "seq_X, seq_y, max_len_complex = pad_sentences(n_gram_sequences, nwords)\n",
    "\n",
    "complex_small_model = make_model(max_len_complex, nwords)\n",
    "filename = \"build/word-based-weights-50-1.1208-complex-small.hdf5\"\n",
    "complex_small_model.load_weights(filename)\n",
    "complex_small_model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "complex_big_model = make_model(max_len_complex, nwords, False)\n",
    "filename = \"build/word-based-weights-04-7.6220-complex-big.hdf5\"\n",
    "complex_big_model.load_weights(filename)\n",
    "complex_big_model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AAiW\n",
      "Unique words: 2914 the total number of words: 27827\n",
      "NYT\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Unique words: 6329 the total number of words: 24049'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('AAiW')\n",
    "print('Unique words: {0} the total number of words: {1}'.format(len(tokenizer_simple.word_counts), sum(x for x in tokenizer_simple.word_counts.values())))\n",
    "print('NYT')\n",
    "'Unique words: {0} the total number of words: {1}'.format(len(tokenizer_complex.word_counts), sum(x for x in tokenizer_complex.word_counts.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text generation\n",
    "- generate texts by trained four models using the following four texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "CNN = 'Occupy Boston protesters remained firmly entrenched in a downtown city square early Friday after a midnight deadline passed for them to clear out or face eviction.  Demonstrators cleared trash and some of the more than 100 tents from the area, but most stayed put. Police presence was light around Dewey Square. Superior Court Judge Frances A. McIntyre ruled Wednesday that demonstrators\\' First Amendment rights do not extend to seizing and holding areas on which they sit. Authorities are \\\"obligated by law to preserve Dewey Square as a space open to the public,\\\" McIntyre added. Boston Mayor Thomas M. Menino, who in the past has appeared to tolerate his city\\'s chapter of the nationwide movement, recently signaled that the park could be a safety hazard as winter weather rolls in. \\\"We\\'re asking them to leave, according to their own will and volition,\\\" Menino told CNN affiliate WCVB. \\\"After that, we\\'ll make decisions about how we\\'ll clear off the site in the future.\\\" Protesters have been encamped in the square since late September.  On Thursday, police could be seen handing out fliers to protesters in an apparent effort to inform them that they soon had to leave.  \\\"It\\'s like telling you to get out of your apartment by midnight,\\\" protester Philip O\\'Connell told the affiliate station. By Thursday evening, comments on an Occupy Boston Twitter feed were focused on the impending deadline.  \\\"Some tents may fold, but an idea cannot be evicted,\\\" read one post.  \\\"We have occupied Dewey Square because Wall Street has occupied our government,\\\" read another. In October, 129 people were arrested during a demonstration in which protesters allegedly blocked traffic and refused to disperse, according to police spokesman Eddy Chrispin. They were arrested mostly for \\\"unlawful assembly and trespassing,\\\" he said.  The movement, which first sprang up in a Lower Manhattan park, seeks to highlight what it sees as corruption and growing income disparities between the nation\\'s richest 1% and the rest of the country. In a move similar to McIntyre\\'s ruling, a New York Supreme Court announced last month that Occupy protesters would be allowed to return to Zuccotti Park -- considered a home base for demonstrators -- but would be restricted from camping overnight.  Police in riot gear cleared them out in the early-morning hours in mid-November, a move that attorneys for the demonstrators have said was unlawful. Thousands later deluged the city\\'s financial district in a show of strength echoed nationwide as part of the group\\'s so-called \\\"mass day of action.\\\" Scores were arrested across the city, and several police officers were reported injured, as thousands of others gathered in places such as St. Louis, San Francisco, Denver and Milwaukee to respond to the \\\"day of action\\\" plea. Last month, police in Los Angeles and Philadelphia dismantled tents and arrested Occupy protesters who refused to leave city areas.  In Philadelphia, police arrested 52 people after scuffles broke out when authorities ordered some protesters to clear the street.  Encampments have largely remained in a handful of cities, including San Francisco; Asheville, North Carolina; Oklahoma City; and Washington, according to media reports and group websites. '\n",
    "USAToday = 'Shopping carts sit outside the Wal-Mart store in Mayfield Hts., Ohio., on Nov. 14, 2011. The retailer offered few details about the investigation in a quarterly report filed Thursday with the Securities and Exchange Commission. The company said that it opened the investigation after reviewing policies, procedures and internal controls tied to its global anti-corruption program. It said that it is taking \\\"appropriate remedial measures.\\\" \\\"We are taking a deep look at our policies and procedures in every country in which we operate,\\\" said Wal-Mart spokesman Dave Tovar in an email. \\\"As a result of information obtained during that review and from other sources, we have begun an internal investigation related to compliance with the\\\" Foreign Corrupt Practice Act. All companies doing business overseas must comply with the U.S. Foreign Corrupt Practices Act, which broadly deals with bribery and accounting rules. Wal-Mart Stores (WMT) has hired outside lawyers and other advisers and has notified the U.S. Department of Justice and Securities and Exchange Commission. \\\"Our investigation is currently focused on discrete incidents in specific areas,\\\" Tovar said. \\\"We intend to keep federal authorities apprised of what we learn.\\\" The issue will not have a material impact on business, the company said. The world\\'s largest retailer — which operates in 28 countries, including China, Mexico and Japan — brought in $419 billion in revenue last year. Company shares fell 31 cents or 0.5% to $58.29 on Friday. '\n",
    "TheGreenMile = 'This happened in 1932, when the state penitentiary was still at Cold Mountain. And the electric chair was there, too, of course.   The inmates made jokes about the chair the way people always make jokes about things that frighten them but can\\'t be gotten away from. They called it Old Sparky, or the Big Juicy. They made cracks about the Power bill, and how Warden Moores would cook his Thanksgiving dinner that fall, with his wife, Melinda, too sick to cook.   But for the ones who actually had to sit down in that chair, the humor went out of the situation in a hurry I presided over seventy-eight executions during my time at Cold Mountain (that\\'s one figure I\\'ve never been confused about; I\\'ll remember it on my deathbed), and I think that, for most of those men, the truth of what was happening to them finally hit all the way home when their ankles were being damped to the stout oak of \\\"Old Sparky\\'s\\\" legs. The realization came then (you would see it rising in their eyes, a kind of cold dismay) that their, own legs had finished their careers. The blood still ran in them, the muscles were still strong, but they were finished, all the same; they were never going to walk another country mile or dance with a girl at a barn-raising. Old Sparky\\'s clients came to a knowledge of their deaths from the ankles up. There was a black silk bag that went over their heads after they had finished their rambling and mostly disjointed last remarks. It was supposed to be for them, but I always thought it was really for us, to keep us from seeing the awful tide of dismay in their eyes as they realized they were going to die with their knees bent.   There was no death row at Cold Mountain, only E Block, set apart from the other four and about a quarter their size, brick instead of wood, with a horrible bare metal roof that glared in the summer sun like a delirious eyeball. Six cells inside, three on each side of a wide center aisle, each almost twice as big as the cells in the other four blocks. Singles, too. Great accommodations for a prison (especially in the thirties), but the inmates would have traded for cells in any of the other four. Believe me, they would have traded.   There was never a time during my years as block superintendent when all six cells were occupied at one time -- thank God for small favors. Four was the most, mixed black and white (at Cold Mountain, there was no segregation among the walking dead), and that was a little piece of hell. One was a woman, Beverly McCall. She was black as the ace of spades and as beautiful as the sin you never had nerve enough to commit. She put up with six years of her husband beating her, but wouldn\\'t put up with his creeping around for a single day. On the evening after she found out he was cheating, she stood waiting for the unfortunate Lester McCall, known to his pals (and, presumably, to his extremely short-term mistress) as Cutter, at the top of the stairs leading to the apartment over his barber shop. She waited until he got his overcoat half off, then dropped his cheating guts onto his tu-tone shoes. Used one of Cutter\\'s own razors to do it. Two nights before she was due to sit in Old Sparky, she called me to her cell and said she had been visited by her African spirit-father in a dream. He told her to discard her slave-name and to die under her free name, Matuomi. That was her request, that her deathwarrant should be read under the name of Beverly Matuomi. I guess her spirit-father didn\\'t give her any first name, or one she could make out, anyhow. I said yes, okay, fine. One thing those years serving as the bull-goose screw taught me was never to refuse the condemned unless I absolutely had to. In the case of Beverly Matuomi, it made no difference, anyway. The governor called the next day around three in the afternoon, commuting her sentence to life in the Grassy Valley Penal Facility for Women -- all penal and no penis, we used to say back then. I was glad to see Bev\\'s round ass going left instead of right when she got to the duty desk, let me tell you. Thirty-five years or so later -- had to be at least thirty-five -- I saw that name on the obituary page of the paper, under a picture of a skinny-faced black lady with a cloud of white hair and glasses with rhinestones at the comers. It was Beverly. She\\'d spent the last ten years of her life a free woman, the obituary said, and had rescued the small-town library of Raines Falls pretty much single-handed. She had also taught Sunday school and had been much loved in that little backwater. LIBRARIAN DIES OF HEART FAILURE, the headline said, and below that, in smaller type, almost as an afterthought: Served Over Two Decades in Prison for Murder. Only the eyes, wide and blazing behind the glasses with the rhinestones at the comers, were the same. They were the eyes of a woman who even at seventy-whatever would not hesitate to pluck a safety razor from its blue jar of disinfectant, if the urge seemed pressing. You know murderers, even if they finish up as old lady librarians in dozey little towns. At least you do if you\\'ve spent as much time minding murderers as I did. There was only one time I ever had a question about the nature of my job. That, I reckon, is why I\\'m writing this.   The wide corridor up the center of E Block was floored with linoleum the color of tired old limes, and so what was called the Last Mile at other prisons was called the Green Mile at Cold Mountain. It ran, I guess, sixty long paces from south to north, bottom to top. At the bottom was the restraint room. At the top end was a T-junction. A left turn meant life -- if you called what went on in the sunbaked exercise yard life, and many did; many lived it for years, with no apparent ill effects. Thieves and arsonists and sex criminals, all talking their talk and walking their walk and making their little deals.   A right turn, though -- that was different. First you went into my office (where the carpet was also green, a thing I kept meaning to change and not getting around to), and crossed in front of my desk, which was flanked by the American flag on the left and the state flag on the right. On the far side were two doors. One led into the small W.C. that I and the E Block guards (sometimes even Warden Moores) used; the other opened on a kind of storage shed. This was where you ended up when you walked the Green Mile.   It was a small door -- I had to duck my head when I went through, and John Coffey actually had to sit and scoot. You came out on a little landing, then went down three cement steps to a board floor. It was a miserable room without heat and with a metal roof, just like the one on the block to which it was an adjunct. It was cold enough in there to see your breath during the winter, and stifling in the summer. At the execution of Elmer Manfred -- in July or August of \\'30, that one was, I believe -- we had nine witnesses pass out.   On the left side of the storage shed -- again -- there was life. Tools (all locked down in frames crisscrossed with chains, as if they were carbine rifles instead of spades and pickaxes), dry goods, sacks of seeds for spring planting in the prison gardens, boxes of toilet paper, pallets cross-loaded with blanks for the prison plate-shop...even bags of lime for marking out the baseball diamond and the football gridiron -- the cons played in what was known as The Pasture, and fall afternoons were greatly looked forward to at Cold Mountain.   On the right -- once again -- death. Old Sparky his ownself, sitting up on a plank platform at the southeast comer of the storeroom, stout oak legs, broad oak arms that had absorbed the terrorized sweat of scores of men in the last few minutes of their lives, and the metal cap, usually hung jauntily on the back of the chair, like some robot kid\\'s beanie in a Buck Rogers comic-strip. A cord ran from it and through a gasket-circled hole in the cinderblock wall behind the chair. Off to one side was a galvanized tin bucket. If you looked inside it, you would see a circle of sponge, cut just right to fit the metal cap. Before executions, it was soaked in brine to better conduct the charge of direct-current electricity that ran through the wire, through the sponge, and into the condemned man\\'s brain.'\n",
    "HarryPotter = 'October arrived, spreading a damp chill over the grounds and into the castle. Madam Pomfrey, the nurse, was kept busy by a sudden spate of colds among the staff and students. Her Pepperup potion worked instantly, though it left the drinker smoking at the ears for several hours afterward. Ginny Weasley, who had been looking pale, was bullied into taking some by Percy. The steam pouring from under her vivid hair gave the impression that her whole head was on fire.  Raindrops the size of bullets thundered on the castle windows for days on end; the lake rose, the flower beds turned into muddy streams, and Hagrid \\'s pumpkins swelled to the size of garden sheds. Oliver Wood \\'s enthusiasm for regular training sessions, however, was not dampened, which was why Harry was to be found, late one stormy Saturday afternoon a few days before Halloween, returning to Gryffindor Tower, drenched to the skin and splattered with mud.  Even aside from the rain and wind it hadn \\'t been a happy practice session. Fred and George, who had been spying on the Slytherin team, had seen for themselves the speed of those new Nimbus Two Thousand and Ones. They reported that the Slytherin team was no more than seven greenish blurs, shooting through the air like missiles.  As Harry squelched along the deserted corridor he came across somebody who looked just as preoccupied as he was. Nearly Headless Nick, the ghost of Gryffindor Tower, was staring morosely out of a window, muttering under his breath,  \\\"… don \\'t fulfill their requirements… half an inch, if that… \\\"   \\\"Hello, Nick, \\\" said Harry.   \\\"Hello, hello, \\\" said Nearly Headless Nick, starting and looking round. He wore a dashing, plumed hat on his long curly hair, and a tunic with a ruff, which concealed the fact that his neck was almost completely severed. He was pale as smoke, and Harry could see right through him to the dark sky and torrential rain outside.   \\\"You look troubled, young Potter, \\\" said Nick, folding a transparent letter as he spoke and tucking it inside his doublet.   \\\"So do you, \\\" said Harry.   \\\"Ah, \\\" Nearly Headless Nick waved an elegant hand,  \\\"a matter of no importance.… It \\'s not as though I really wanted to join.… Thought I \\'d apply, but apparently I  \\'don \\'t fulfill requirements \\' - \\\"  In spite of his airy tone, there was a look of great bitterness on his face.   \\\"But you would think, wouldn \\'t you, \\\" he erupted suddenly, pulling the letter back out of his pocket,  \\\"that getting hit forty-five times in the neck with a blunt axe would qualify you to join the Headless Hunt? \\\"   \\\"Oh - yes, \\\" said Harry, who was obviously supposed to agree.   \\\"I mean, nobody wishes more than I do that it had all been quick and clean, and my head had come off properly, I mean, it would have saved me a great deal of pain and ridicule. However - \\\" Nearly Headless Nick shook his letter open and read furiously:  \\\" \\'We can only accept huntsmen whose heads have parted company with their bodies. You will appreciate that it would be impossible otherwise for members to participate in hunt activities such as Horseback Head-Juggling and Head Polo. It is with the greatest regret, therefore, that I must inform you that you do not fulfill our requirements. With very best wishes, Sir Patrick Delaney-Podmore. \\' \\\"  Fuming, Nearly Headless Nick stuffed the letter away.   \\\"Half an inch of skin and sinew holding my neck on, Harry! Most people would think that \\'s good and beheaded, but oh, no, it \\'s not enough for Sir Properly Decapitated-Podmore. \\\"  Nearly Headless Nick took several deep breaths and then said, in a far calmer tone,  \\\"So - what \\'s bothering you? Anything I can do? \\\"   \\\"No, \\\" said Harry.  \\\"Not unless you know where we can get seven free Nimbus Two Thousand and Ones for our match against Sly - \\\"  The rest of Harry \\'s sentence was drowned out by a high-pitched mewling from somewhere near his ankles. He looked down and found himself gazing into a pair of lamp-like yellow eyes. It was Mrs. Norris, the skeletal gray cat who was used by the caretaker, Argus Filch, as a sort of deputy in his endless battle against students.   \\\"You \\'d better get out of here, Harry, \\\" said Nick quickly.  \\\"Filch isn \\'t in a good mood - he \\'s got the flu and some third years accidentally plastered frog brains all over the ceiling in dungeon five. He \\'s been cleaning all morning, and if he sees you dripping mud all over the place - \\\"   \\\"Right, \\\" said Harry, backing away from the accusing stare of Mrs. Norris, but not quickly enough. Drawn to the spot by the mysterious power that seemed to connect him with his foul cat, Argus Filch burst suddenly through a tapestry to Harry \\'s right, wheezing and looking wildly about for the rule-breaker. There was a thick tartan scarf bound around his head, and his nose was unusually purple.   \\\"Filth! \\\" he shouted, his jowls aquiver, his eyes popping alarmingly as he pointed at the muddy puddle that had dripped from Harry \\'s Quidditch robes.  \\\"Mess and muck everywhere! I \\'ve had enough of it, I tell you! Follow me, Potter! \\\"  So Harry waved a gloomy good-bye to Nearly Headless Nick and followed Filch back downstairs, doubling the number of muddy footprints on the floor.  Harry had never been inside Filch \\'s office before; it was a place most students avoided. The room was dingy and windowless, lit by a single oil lamp dangling from the low ceiling. A faint smell of fried fish lingered about the place. Wooden filing cabinets stood around the walls; from their labels, Harry could see that they contained details of every pupil Filch had ever punished. Fred and George Weasley had an entire drawer to themselves. A highly polished collection of chains and manacles hung on the wall behind Filch \\'s desk. It was common knowledge that he was always begging Dumbledore to let him suspend students by their ankles from the ceiling.  Filch grabbed a quill from a pot on his desk and began shuffling around looking for parchment.   \\\"Dung, \\\" he muttered furiously,  \\\"great sizzling dragon bogies… frog brains… rat intestines… I \\'ve had enough of it… make an example… where \\'s the form… yes… \\\"  He retrieved a large roll of parchment from his desk drawer and stretched it out in front of him, dipping his long black quill into the ink pot.   \\\"Name… Harry Potter. Crime… \\\"   \\\"It was only a bit of mud! \\\" said Harry.   \\\"It \\'s only a bit of mud to you, boy, but to me it \\'s an extra hour scrubbing! \\\" shouted Filch, a drip shivering unpleasantly at the end of his bulbous nose.  \\\"Crime… befouling the castle… suggested sentence… \\\"  Dabbing at his streaming nose, Filch squinted unpleasantly at Harry who waited with bated breath for his sentence to fall.  But as Filch lowered his quill, there was a great BANG! on the ceiling of the office, which made the oil lamp rattle.   \\\"PEEVES! \\\" Filch roared, flinging down his quill in a transport of rage.  \\\"I \\'ll have you this time, I \\'ll have you! \\\"  And without a backward glance at Harry, Filch ran flat-footed from the office, Mrs. Norris streaking alongside him.  Peeves was the school poltergeist, a grinning, airborne menace who lived to cause havoc and distress. Harry didn \\'t much like Peeves, but couldn \\'t help feeling grateful for his timing. Hopefully, whatever Peeves had done (and it sounded as though he \\'d wrecked something very big this time) would distract Filch from Harry.  Thinking that he should probably wait for Filch to come back, Harry sank into a moth-eaten chair next to the desk. There was only one thing on it apart from his half-completed form: a large, glossy, purple envelope with silver lettering on the front. With a quick glance at the door to check that Filch wasn \\'t on his way back, Harry picked up the envelope and read: kwikspell A Correspondence Course in Beginners \\' Magic  p>Intrigued, Harry flicked the envelope open and pulled out the sheaf of parchment inside. More curly silver writing on the front page said: Feel out of step in the world of modern magic? Find yourself making excuses not to perform simple spells? Ever been taunted for your woeful wandwork? There is an answer! Kwikspell is an all-new, fail-safe, quick-result, easy-learn course. Hundreds of witches and wizards have benefited from the Kwikspell method! Madam Z. Nettles of Topsham writes:  \\\"I had no memory for incantations and my potions were a family joke! Now, after a Kwikspell course, I am the center of attention at parties and friends beg for the recipe of my Scintillation Solution! \\\" Warlock D. J. Prod of Didsbury says:  \\\"My wife used to sneer at my feeble charms, but one month into your fabulous Kwikspell course and I succeeded in turning her into a yak! Thank you, Kwikspell! \\\"    Fascinated, Harry thumbed through the rest of the envelope \\'s contents. Why on earth did Filch want a Kwikspell course? Did this mean he wasn \\'t a proper wizard? Harry was just reading  \\\"Lesson One: Holding Your Wand (Some Useful Tips) \\\" when shuffling footsteps outside told him Filch was coming back. Stuffing the parchment back into the envelope, Harry threw it back onto the desk just as the door opened.  Filch was looking triumphant.   \\\"That vanishing cabinet was extremely valuable! \\\" he was saying gleefully to Mrs. Norris.  \\\"We \\'ll have Peeves out this time, my sweet - \\\"  His eyes fell on Harry and then darted to the Kwikspell envelope, which, Harry realized too late, was lying two feet away from where it had started.  Filch \\'s pasty face went brick red. Harry braced himself for a tidal wave of fury. Filch hobbled across to his desk, snatched up the envelope, and threw it into a drawer.   \\\"Have you - did you read -? \\\" he sputtered.   \\\"No, \\\" Harry lied quickly.  Filch \\'s knobbly hands were twisting together.   \\\"If I thought you \\'d read my private - not that it \\'s mine - for a friend - be that as it may - however - \\\"  Harry was staring at him, alarmed; Filch had never looked madder. His eyes were popping, a tic was going in one of his pouchy cheeks, and the tartan scarf didn \\'t help.   \\\"Very well - go - and don \\'t breathe a word - not that - however, if you didn \\'t read - go now, I have to write up Peeves \\' report - go - \\\"  Amazed at his luck, Harry sped out of the office, up the corridor, and back upstairs. To escape from Filch \\'s office without punishment was probably some kind of school record.   \\\"Harry! Harry! Did it work? \\\"  Nearly Headless Nick came gliding out of a classroom. Behind him, Harry could see the wreckage of a large black-and-gold cabinet that appeared to have been dropped from a great height.   \\\"I persuaded Peeves to crash it right over Filch \\'s office, \\\" said Nick eagerly.  \\\"Thought it might distract him - \\\"   \\\"Was that you? \\\" said Harry gratefully.  \\\"Yeah, it worked, I didn \\'t even get detention. Thanks, Nick! \\\"  They set off up the corridor together. Nearly Headless Nick, Harry noticed, was still holding Sir Patrick \\'s rejection letter.   \\\"I wish there was something I could do for you about the Headless Hunt, \\\" Harry said.    Nearly Headless Nick stopped in his tracks and Harry walked right through him. He wished he hadn \\'t; it was like stepping through an icy shower.   \\\"But there is something you could do for me, \\\" said Nick excitedly.  \\\"Harry - would I be asking too much - but no, you wouldn \\'t want - \\\"   \\\"What is it? \\\" said Harry.   \\\"Well, this Halloween will be my five hundredth deathday, \\\" said Nearly Headless Nick, drawing himself up and looking dignified.   \\\"Oh, \\\" said Harry, not sure whether he should look sorry or happy about this.  \\\"Right. \\\"   \\\"I \\'m holding a party down in one of the roomier dungeons. Friends will be coming from all over the country. It would be such an honor if you would attend. Mr. Weasley and Miss Granger would be most welcome, too, of course - but I daresay you \\'d rather go to the school feast? \\\" He watched Harry on tenterhooks.   \\\"No, \\\" said Harry quickly,  \\\"I \\'ll come - \\\"   \\\"My dear boy! Harry Potter, at my deathday party! And \\\" - he hesitated, looking excited -  \\\"do you think you could possibly mention to Sir Patrick how very frightening and impressive you find me? \\\"   \\\"Of - of course, \\\" said Harry.  Nearly Headless Nick beamed at him.  \\\"A deathday party? \\\" said Hermione keenly when Harry had changed at last and joined her and Ron in the common room.  \\\"I bet there aren \\'t many living people who can say they \\'ve been to one of those - it \\'ll be fascinating! \\\"   \\\"Why would anyone want to celebrate the day they died? \\\" said Ron, who was halfway through his Potions homework and grumpy.  \\\"Sounds dead depressing to me.… \\\"  Rain was still lashing the windows, which were now inky black, but inside all looked bright and cheerful. The firelight glowed over the countless squashy armchairs where people sat reading, talking, doing homework or, in the case of Fred and George Weasley, trying to find out what would happen if you fed a Filibuster firework to a salamander. Fred had  \\\"rescued \\\" the brilliant orange, fire-dwelling lizard from a Care of Magical Creatures class and it was now smouldering gently on a table surrounded by a knot of curious people.  Harry was at the point of telling Ron and Hermione about Filch and the Kwikspell course when the salamander suddenly whizzed into the air, emitting loud sparks and bangs as it whirled wildly round the room. The sight of Percy bellowing himself hoarse at Fred and George, the spectacular display of tangerine stars showering from the salamander \\'s mouth, and its escape into the fire, with accompanying explosions, drove both Filch and the Kwikspell envelope from Harry \\'s mind. By the time Halloween arrived, Harry was regretting his rash promise to go to the deathday party. The rest of the school was happily anticipating their Halloween feast; the Great Hall had been decorated with the usual live bats, Hagrid \\'s vast pumpkins had been carved into lanterns large enough for three men to sit in, and there were rumors that Dumbledore had booked a troupe of dancing skeletons for the entertainment.   \\\"A promise is a promise, \\\" Hermione reminded Harry bossily.  \\\"You said you \\'d go to the deathday party. \\\"  So at seven o \\'clock, Harry, Ron, and Hermione walked straight past the doorway to the packed Great Hall, which was glittering invitingly with gold plates and candles, and directed their steps instead toward the dungeons.  The passageway leading to Nearly Headless Nick \\'s party had been lined with candles, too, though the effect was far from cheerful: These were long, thin, jet-black tapers, all burning bright blue, casting a dim, ghostly light even over their own living faces. The temperature dropped with every step they took. As Harry shivered and drew his robes tightly around him, he heard what sounded like a thousand fingernails scraping an enormous blackboard.   \\\"Is that supposed to be music? \\\" Ron whispered. They turned a corner and saw Nearly Headless Nick standing at a doorway hung with black velvet drapes.   \\\"My dear friends, \\\" he said mournfully.  \\\"Welcome, welcome… so pleased you could come.… \\\"  He swept off his plumed hat and bowed them inside.  It was an incredible sight. The dungeon was full of hundreds of pearly-white, translucent people, mostly drifting around a crowded dance floor, waltzing to the dreadful, quavering sound of thirty musical saws, played by an orchestra on a raised, black-draped platform. A chandelier overhead blazed midnight-blue with a thousand more black candles. Their breath rose in a mist before them; it was like stepping into a freezer.   \\\"Shall we have a look around? \\\" Harry suggested, wanting to warm up his feet.   \\\"Careful not to walk through anyone, \\\" said Ron nervously, and they set off around the edge of the dance floor. They passed a group of gloomy nuns, a ragged man wearing chains, and the Fat Friar, a cheerful Hufflepuff ghost, who was talking to a knight with an arrow sticking out of his forehead. Harry wasn \\'t surprised to see that the Bloody Baron, a gaunt, staring Slytherin ghost covered in silver bloodstains, was being given a wide berth by the other ghosts.   \\\"Oh, no, \\\" said Hermione, stopping abruptly.  \\\"Turn back, turn back, I don \\'t want to talk to Moaning Myrtle - \\\"   \\\"Who? \\\" said Harry as they backtracked quickly.   \\\"She haunts one of the toilets in the girls \\' bathroom on the first floor, \\\" said Hermione.   \\\"She haunts a toilet? \\\"   \\\"Yes. It \\'s been out-of-order all year because she keeps having tantrums and flooding the place. I never went in there anyway if I could avoid it; it \\'s awful trying to have a pee with her wailing at you - \\\"  \\\"Look, food! \\\" said Ron.  On the other side of the dungeon was a long table, also covered in black velvet. They approached it eagerly but next moment had stopped in their tracks, horrified. The smell was quite disgusting. Large, rotten fish were laid on handsome silver platters; cakes, burned charcoal-black, were heaped on salvers; there was a great maggoty haggis, a slab of cheese covered in furry green mold and, in pride of place, an enormous gray cake in the shape of a tombstone, with tar-like icing forming the words, Sir Nicholas de Mimsy-Porpington died 31st October, 1492  Harry watched, amazed, as a portly ghost approached the table, crouched low, and walked through it, his mouth held wide so that it passed through one of the stinking salmon.   \\\"Can you taste it if you walk though it? \\\" Harry asked him.   \\\"Almost, \\\" said the ghost sadly, and he drifted away.   \\\"I expect they \\'ve let it rot to give it a stronger flavor, \\\" said Hermione knowledgeably, pinching her nose and leaning closer to look at the putrid haggis.   \\\"Can we move? I feel sick, \\\" said Ron.  They had barely turned around, however, when a little man swooped suddenly from under the table and came to a halt in midair before them.   \\\"Hello, Peeves, \\\" said Harry cautiously.  Unlike the ghosts around them, Peeves the Poltergeist was the very reverse of pale and transparent. He was wearing a bright orange party hat, a revolving bow tie, and a broad grin on his wide, wicked face.   \\\"Nibbles? \\\" he said sweetly, offering them a bowl of peanuts covered in fungus.   \\\"No thanks, \\\" said Hermione.   \\\"Heard you talking about poor Myrtle, \\\" said Peeves, his eyes dancing.  \\\"Rude you was about poor Myrtle. \\\" He took a deep breath and bellowed,  \\\"OY! MYRTLE! \\\"   \\\"Oh, no, Peeves, don \\'t tell her what I said, she \\'ll be really upset, \\\" Hermione whispered frantically.  \\\"I didn \\'t mean it, I don \\'t mind her - er, hello, Myrtle. \\\"  The squat ghost of a girl had glided over. She had the glummest face Harry had ever seen, half-hidden behind lank hair and thick, pearly spectacles.   \\\"What? \\\" she said sulkily.   \\\"How are you, Myrtle? \\\" said Hermione in a falsely bright voice.  \\\"It \\'s nice to see you out of the toilet. \\\"  Myrtle sniffed.   \\\"Miss Granger was just talking about you - \\\" said Peeves slyly in Myrtle \\'s ear.   \\\"Just saying - saying - how nice you look tonight, \\\" said Hermione, glaring at Peeves.  Myrtle eyed Hermione suspiciously.   \\\"You \\'re making fun of me, \\\" she said, silver tears welling rapidly in her small, see-through eyes.   \\\"No - honestly - didn \\'t I just say how nice Myrtle \\'s looking? \\\" said Hermione, nudging Harry and Ron painfully in the ribs.   \\\"Oh, yeah - \\\"   \\\"She did - \\\"   \\\"Don \\'t lie to me, \\\" Myrtle gasped, tears now flooding down her face, while Peeves chuckled happily over her shoulder.  \\\"D \\'you think I don \\'t know what people call me behind my back? Fat Myrtle! Ugly Myrtle! Miserable, moaning, moping Myrtle! \\\"   \\\"You \\'ve forgotten pimply, \\\" Peeves hissed in her ear.  Moaning Myrtle burst into anguished sobs and fled from the dungeon. Peeves shot after her, pelting her with moldy peanuts, yelling,  \\\"Pimply! Pimply! \\\"   \\\"Oh, dear, \\\" said Hermione sadly.  Nearly Headless Nick now drifted toward them through the crowd.   \\\"Enjoying yourselves? \\\"   \\\"Oh, yes, \\\" they lied.   \\\"Not a bad turnout, \\\" said Nearly Headless Nick proudly.  \\\"The Wailing Widow came all the way up from Kent.… It \\'s nearly time for my speech, I \\'d better go and warn the orchestra.… \\\"  The orchestra, however, stopped playing at that very moment. They, and everyone else in the dungeon, fell silent, looking around in excitement, as a hunting horn sounded.   \\\"Oh, here we go, \\\" said Nearly Headless Nick bitterly.  Through the dungeon wall burst a dozen ghost horses, each ridden by a headless horseman. The assembly clapped wildly; Harry started to clap, too, but stopped quickly at the sight of Nick \\'s face.  The horses galloped into the middle of the dance floor and halted, rearing and plunging. At the front of the pack was a large ghost who held his bearded head under his arm, from which position he was blowing the horn. The ghost leapt down, lifted his head high in the air so he could see over the crowd (everyone laughed), and strode over to Nearly Headless Nick, squashing his head back onto his neck.   \\\"Nick! \\\" he roared.  \\\"How are you? Head still hanging in there? \\\"  He gave a hearty guffaw and clapped Nearly Headless Nick on the shoulder.   \\\"Welcome, Patrick, \\\" said Nick stiffly.   \\\"Live  \\'uns! \\\" said Sir Patrick, spotting Harry, Ron, and Hermione and giving a huge, fake jump of astonishment, so that his head fell off again (the crowd howled with laughter).   \\\"Very amusing, \\\" said Nearly Headless Nick darkly.   \\\"Don \\'t mind Nick! \\\" shouted Sir Patrick \\'s head from the floor.  \\\"Still upset we won \\'t let him join the Hunt! But I mean to say - look at the fellow - \\\"   \\\"I think, \\\" said Harry hurriedly, at a meaningful look from Nick,  \\\"Nick \\'s very - frightening and - er - \\\"   \\\"Ha! \\\" yelled Sir Patrick \\'s head.  \\\"Bet he asked you to say that! \\\"   \\\"If I could have everyone \\'s attention, it \\'s time for my speech! \\\" said Nearly Headless Nick loudly, striding toward the podium and climbing into an icy blue spotlight.   \\\"My late lamented lords, ladies, and gentlemen, it is my great sorrow… \\\"  But nobody heard much more. Sir Patrick and the rest of the Headless Hunt had just started a game of Head Hockey and the crowd were turning to watch. Nearly Headless Nick tried vainly to recapture his audience, but gave up as Sir Patrick \\'s head went sailing past him to loud cheers.  Harry was very cold by now, not to mention hungry.   \\\"I can \\'t stand much more of this, \\\" Ron muttered, his teeth chattering, as the orchestra ground back into action and the ghosts swept back onto the dance floor.   \\\"Let \\'s go, \\\" Harry agreed.  They backed toward the door, nodding and beaming at anyone who looked at them, and a minute later were hurrying back up the passageway full of black candles.   \\\"Pudding might not be finished yet, \\\" said Ron hopefully, leading the way toward the steps to the entrance hall.  And then Harry heard it.   \\\"… rip… tear… kill… \\\"  It was the same voice, the same cold, murderous voice he had heard in Lockhart \\'s office.  He stumbled to a halt, clutching at the stone wall, listening with all his might, looking around, squinting up and down the dimly lit passageway.   \\\"Harry, what \\'re you -? \\\"   \\\"It \\'s that voice again - shut up a minute - \\\"   \\\"… soo hungry… for so long… \\\"   \\\"Listen! \\\" said Harry urgently, and Ron and Hermione froze, watching him.   \\\"… kill… time to kill… \\\"  The voice was growing fainter. Harry was sure it was moving away - moving upward. A mixture of fear and excitement gripped him as he stared at the dark ceiling; how could it be moving upward? Was it a phantom, to whom stone ceilings didn \\'t matter?   \\\"This way, \\\" he shouted, and he began to run, up the stairs, into the entrance hall. It was no good hoping to hear anything here, the babble of talk from the Halloween feast was echoing out of the Great Hall. Harry sprinted up the marble staircase to the first floor, Ron and Hermione clattering behind him.   \\\"Harry, what \\'re we - \\\"   \\\"SHH! \\\"  Harry strained his ears. Distantly, from the floor above, and growing fainter still, he heard the voice:  \\\"… I smell blood.… I SMELL BLOOD! \\\"  His stomach lurched -   \\\"It \\'s going to kill someone! \\\" he shouted, and ignoring Ron \\'s and Hermione \\'s bewildered faces, he ran up the next flight of steps three at a time, trying to listen over his own pounding footsteps -  Harry hurtled around the whole of the second floor, Ron and Hermione panting behind him, not stopping until they turned a corner into the last, deserted passage.   \\\"Harry, what was that all about? \\\" said Ron, wiping sweat off his face.  \\\"I couldn \\'t hear anything.… \\\"  But Hermione gave a sudden gasp, pointing down the corridor.   \\\"Look! \\\"  Something was shining on the wall ahead. They approached slowly, squinting through the darkness. Foot-high words had been daubed on the wall between two windows, shimmering in the light cast by the flaming torches. the chamber of secrets has been opened. enemies of the heir, beware.   \\\"What \\'s that thing - hanging underneath? \\\" said Ron, a slight quiver in his voice.  As they edged nearer, Harry almost slipped - there was a large puddle of water on the floor; Ron and Hermione grabbed him, and they inched toward the message, eyes fixed on a dark shadow beneath it. All three of them realized what it was at once, and leapt backward with a splash.  Mrs. Norris, the caretaker \\'s cat, was hanging by her tail from the torch bracket. She was stiff as a board, her eyes wide and staring.  For a few seconds, they didn \\'t move. Then Ron said,  \\\"Let \\'s get out of here. \\\"   \\\"Shouldn \\'t we try and help - \\\" Harry began awkwardly.   \\\"Trust me, \\\" said Ron.  \\\"We don \\'t want to be found here. \\\"  But it was too late. A rumble, as though of distant thunder, told them that the feast had just ended. From either end of the corridor where they stood came the sound of hundreds of feet climbing the stairs, and the loud, happy talk of well-fed people; next moment, students were crashing into the passage from both ends.  The chatter, the bustle, the noise died suddenly as the people in front spotted the hanging cat. Harry, Ron, and Hermione stood alone, in the middle of the corridor, as silence fell among the mass of students pressing forward to see the grisly sight.  Then someone shouted through the quiet.   \\\"Enemies of the Heir, beware! You \\'ll be next, Mudbloods! \\\"  It was Draco Malfoy. He had pushed to the front of the crowd, his cold eyes alive, his usually bloodless face flushed, as he grinned at the sight of the hanging, immobile cat. '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_name = ['cnn', 'usa-today', 'the-green-mile', 'harry-potter']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(input_words, n_output_words, model, tokenizer, max_len=277):\n",
    "    index_word = dict((i, w) for w, i in tokenizer.word_index.items())\n",
    "    for _ in range(n_output_words):\n",
    "        input_seq = tokenizer.texts_to_sequences([input_words])[0]\n",
    "        input_seq = input_seq[:(max_len-1)]\n",
    "        padded_seq = pad_sequences([input_seq], maxlen=max_len-1, padding='pre')\n",
    "        prediction = model.predict_classes(padded_seq, verbose=0)\n",
    "        next_word = index_word.get(prediction[0])\n",
    "        input_words += \" \" + next_word\n",
    "    return input_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8904abec8844b10b87d4da4ab3648d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=24), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08132ac23e684ffba20e3538d9e3df03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=13), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5630720e5a640448ee4f6bd6bd27a3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=68), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc5ff38c927f4ee0be4f4ae9f86eea9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=307), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for i, raw_text in enumerate([CNN, USAToday, TheGreenMile, HarryPotter]):\n",
    "    source_text = sent_tokenize(raw_text)\n",
    "    simple_small = []\n",
    "    simple_big = []\n",
    "    complex_small = []\n",
    "    complex_big = []\n",
    "    for sent in tqdm_notebook(source_text):        \n",
    "        result = generate_text(sent, 100, simple_small_model, tokenizer_simple, max_len_simple)\n",
    "        simple_small.append(result)\n",
    "        \n",
    "        result = generate_text(sent, 100, simple_big_model, tokenizer_simple, max_len_simple)\n",
    "        simple_big.append(result)\n",
    "        \n",
    "        result = generate_text(sent, 100, complex_small_model, tokenizer_complex, max_len_complex)\n",
    "        complex_small.append(result)\n",
    "        \n",
    "        result = generate_text(sent, 100, complex_big_model, tokenizer_complex, max_len_complex)\n",
    "        complex_big.append(result)\n",
    "    simple_small = ' '.join(simple_small)\n",
    "    with open('build/generated-texts/{0}-word-based-simple-shallow.txt'.format(source_name[i]), 'w') as f:\n",
    "        f.write(simple_small)\n",
    "        \n",
    "    simple_big = ' '.join(simple_big)\n",
    "    with open('build/generated-texts/{0}-word-based-simple-deep.txt'.format(source_name[i]), 'w') as f:\n",
    "        f.write(simple_big)\n",
    "        \n",
    "    complex_small = ' '.join(complex_small)\n",
    "    with open('build/generated-texts/{0}-word-based-complex-shallow.txt'.format(source_name[i]), 'w') as f:\n",
    "        f.write(complex_small)\n",
    "        \n",
    "    complex_big = ' '.join(complex_big)\n",
    "    with open('build/generated-texts/{0}-word-based-complex-deep.txt'.format(source_name[i]), 'w') as f:\n",
    "        f.write(complex_big)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
