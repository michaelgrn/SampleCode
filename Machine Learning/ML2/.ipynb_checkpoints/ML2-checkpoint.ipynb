{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VuWANcq7m1KQ"
   },
   "source": [
    "# CS534 Homework 2\n",
    "\n",
    "\n",
    "Please provide an answer to the following question:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D9_7-Wkkm1KR"
   },
   "source": [
    "# Question 1 (15 pts)\n",
    "\n",
    "Implement the ADABoost algorithm by using the scikit implementation of the logistic regression. Evaluate the result on a real dataset between a single logistic regression and AdaBoost (use K-Fold cross validation).\n",
    "\n",
    "This links can be helpful: http://rob.schapire.net/papers/explaining-adaboost.pdf and https://en.wikipedia.org/wiki/AdaBoost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.cluster import DBSCAN, KMeans\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "\n",
    "def ADABoost(X,y,T):\n",
    "    weights = np.empty(len(X))\n",
    "    weights.fill(1/len(X))\n",
    "    listWeights = []\n",
    "    listPredictions = []\n",
    "    sigs = []\n",
    "    models = []\n",
    "    \n",
    "    for t in range(T):\n",
    "        logisticRegr = LogisticRegression(solver='liblinear')\n",
    "        logisticRegr.fit(X, y, weights)\n",
    "        models.append(logisticRegr)\n",
    "        results =  logisticRegr.predict(X)\n",
    "        ##get all indices that were wrong\n",
    "        wrongInd = (results!=y)\n",
    "        ##sum their weight\n",
    "        totalError = (wrongInd *weights).sum()\n",
    "        signifigance = .5 * np.log((1-totalError)/totalError)\n",
    "        sigs.append(signifigance)\n",
    "        ##get new weights\n",
    "        newWeights = weights * np.exp(-signifigance*y*results)\n",
    "        ##normalize them\n",
    "        newWeights = newWeights*(1/newWeights.sum()) \n",
    "        weights = newWeights\n",
    "    return models, sigs\n",
    "\n",
    "\n",
    "def ADABoostPredict(M,S,X,y):\n",
    "    listPredictions = []\n",
    "    \n",
    "    for x in range(len(M)):\n",
    "        prediction = S[x]*M[x].predict(X)\n",
    "        listPredictions.append(prediction)\n",
    "    \n",
    "    listPredictions = np.array(listPredictions)\n",
    "    newY = []\n",
    "\n",
    "    for x in range(len(y)):\n",
    "        newY.append(np.sign((listPredictions[:,x]).sum()))\n",
    "    \n",
    "    accuracy = (accuracy_score(y, newY))   \n",
    "    \n",
    "    return accuracy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Average Accuracy:\n",
      "0.9733333333333333\n",
      "AdaBoost Average Accuracy:\n",
      "0.9466666666666667\n"
     ]
    }
   ],
   "source": [
    "##Testing for the Iris DataSet\n",
    "\n",
    "data = pd.read_csv('iris.csv',sep=',').values\n",
    "X=data[:,0:4]\n",
    "y=data[:,4]\n",
    "\n",
    "def val(s):\n",
    "    if s=='Iris-virginica':\n",
    "        return 1\n",
    "    return -1\n",
    "y=np.array([val(x) for x in y])\n",
    "\n",
    "\n",
    "skf = StratifiedKFold(n_splits=10, shuffle=True)\n",
    "logAcc = 0\n",
    "adaAcc = 0\n",
    "\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "    X_train=X[train_index]\n",
    "    X_test=X[test_index]\n",
    "    y_train=y[train_index]\n",
    "    y_test=y[test_index]\n",
    "    logisticRegr = LogisticRegression(solver='liblinear')\n",
    "    logisticRegr.fit(X_train, y_train)\n",
    "    results =  logisticRegr.predict(X_test)\n",
    "    #print(accuracy_score(y_test, results))\n",
    "    logAcc += accuracy_score(y_test, results)\n",
    "    \n",
    "for train_index, test_index in skf.split(X, y):\n",
    "    X_train=X[train_index]\n",
    "    X_test=X[test_index]\n",
    "    y_train=y[train_index]\n",
    "    y_test=y[test_index]\n",
    "    M,S = ADABoost(X_train,y_train,1000)\n",
    "    acc = ADABoostPredict(M,S,X_test,y_test)\n",
    "    #print(acc)\n",
    "    adaAcc += acc\n",
    "    \n",
    "print(\"Logistic Regression Average Accuracy:\")\n",
    "print(logAcc/10)\n",
    "print(\"AdaBoost Average Accuracy:\")\n",
    "print(adaAcc/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Average Accuracy:\n",
      "0.98978631122395\n",
      "AdaBoost Average Accuracy:\n",
      "0.9832328361366761\n"
     ]
    }
   ],
   "source": [
    "##Testing for the Banknote Dataset\n",
    "\n",
    "data = pd.read_csv('data_banknote_authentication.txt',sep=',').values\n",
    "\n",
    "X = data[:,0:3]\n",
    "y = data[:,4]\n",
    "\n",
    "def val(s):\n",
    "    if s==1:\n",
    "        return 1\n",
    "    return -1\n",
    "y=np.array([val(x) for x in y])\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "skf = StratifiedKFold(n_splits=10, shuffle=True)\n",
    "logAcc = 0\n",
    "adaAcc = 0\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "    X_train=X[train_index]\n",
    "    X_test=X[test_index]\n",
    "    y_train=y[train_index]\n",
    "    y_test=y[test_index]\n",
    "    logisticRegr = LogisticRegression(solver='liblinear')\n",
    "    logisticRegr.fit(X_train, y_train)\n",
    "    results =  logisticRegr.predict(X_test)\n",
    "    #print(accuracy_score(y_test, results))\n",
    "    logAcc += accuracy_score(y_test, results)\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "    X_train=X[train_index]\n",
    "    X_test=X[test_index]\n",
    "    y_train=y[train_index]\n",
    "    y_test=y[test_index]\n",
    "    M,S = ADABoost(X_train,y_train,1000)\n",
    "    acc = ADABoostPredict(M,S,X_test,y_test)\n",
    "    #print(acc)\n",
    "    adaAcc += acc\n",
    "print(\"Logistic Regression Average Accuracy:\")\n",
    "print(logAcc/10)\n",
    "print(\"AdaBoost Average Accuracy:\")\n",
    "print(adaAcc/10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Found it difficult to consistently have the AdaBoost outperform the Logistic Regression Classifier on its own. Most the time it would be close, but not quite. Only had one instance where AdaBoost had a higher accuracy than the Logistic Regression Classifier. Maybe that is the expected outcome? Tried on more than one dataset as well. Similar results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cBCQhHD-m1KS"
   },
   "source": [
    "## Question 2 (10 pts)\n",
    "Use DBscan (try with different parameters) and K-means (K=3) on IRIS Dataset and discuss/compare the results with the iris ground truth.\n",
    "Please provide an explanation of why K-fold cross validation is not required for the comparison among these different algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When looking at both DBscan and K-means, it is easy to see that K-means provides the best accuracy, even when reducing the Iris dataset to binary dataset. This is most likely correlated to how DBscan works, which is establishing clusters based on distance between points (eps), as well as how many points must be connected to form that cluster (min_samples). Since DBscan breaks it's dataset down into two clusters, with any outliers of those clusters being classified as noise, it becomes difficult to establish the proper distance between cluster centers to include those outliers. If the distance is set high enough, then eventually DBscan will only return one cluster, which really defeats the purpose of clustering in the first place.\n",
    "\n",
    "The reason the K-fold cross validation is not required for the comparison is because we are testing on the data that we trained. There is no over fitting in terms of clustering because these approaches are algorithmic.\n",
    "\n",
    "References:\n",
    "\n",
    "https://www.dummies.com/programming/big-data/data-science/how-to-create-an-unsupervised-learning-model-with-dbscan/\n",
    "https://heartbeat.fritz.ai/k-means-clustering-using-sklearn-and-python-4a054d67b187"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('iris.csv',sep=',').values\n",
    "X=data[:,0:4]\n",
    "y=data[:,4]\n",
    "\n",
    "def val(s):\n",
    "    if s=='Iris-virginica':\n",
    "        return 1\n",
    "    return 0\n",
    "y=np.array([val(x) for x in y])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5906040268456376"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dbscan = DBSCAN()\n",
    "dbscan.fit(X)\n",
    "dbscan.labels_\n",
    "accuracy_score(y, dbscan.labels_) ##default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6510067114093959"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dbscan = DBSCAN(eps=1, min_samples=15)\n",
    "dbscan.fit(X)\n",
    "dbscan.labels_\n",
    "accuracy_score(y, dbscan.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6644295302013423"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dbscan = DBSCAN(eps=3, min_samples=2)\n",
    "dbscan.fit(X)\n",
    "dbscan.labels_\n",
    "accuracy_score(y, dbscan.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8926174496644296"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "data = pd.read_csv('iris.csv',sep=',').values\n",
    "X=data[:,0:4]\n",
    "y=data[:,4]\n",
    "\n",
    "def val(s):\n",
    "    if s=='Iris-setosa':\n",
    "        return 1\n",
    "    elif s =='Iris-versicolor':\n",
    "        return 0\n",
    "    else:\n",
    "        return 2\n",
    "    \n",
    "y=np.array([val(x) for x in y])\n",
    "\n",
    "kmeans = KMeans(n_clusters=3)\n",
    "y_kmeans = kmeans.fit_predict(X)\n",
    "accuracy_score(y, y_kmeans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 2, 2, 2, 2, 0, 2, 2, 2, 2,\n",
       "       2, 2, 0, 0, 2, 2, 2, 2, 0, 2, 0, 2, 0, 2, 2, 0, 0, 2, 2, 2, 2, 2,\n",
       "       0, 2, 2, 2, 2, 0, 2, 2, 2, 0, 2, 2, 2, 0, 2, 2, 0], dtype=int32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_kmeans"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "PS4.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
