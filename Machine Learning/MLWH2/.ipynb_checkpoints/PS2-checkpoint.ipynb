{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "czVETi-U8I0h"
   },
   "source": [
    "# CS534 Homework 2\n",
    "\n",
    "Put your homework in the directory with your name. Please mentionin this file the names of any students with whom you collaborated. If you didn't collaborate with anyone, mark your collaborators as \"None.\" Remember, your goal is to communicate. Full credit will be given only to correct solutions which are described clearly. Convoluted and obtuse descriptions will receive low marks. To complete your homework, you may ONLY consult the following material:\n",
    "\n",
    "lecture slides\n",
    "course notes you or others took during lecture.\n",
    "the required text (CLRS)\n",
    "websites that may clarify the concepts covered in the material but do not in any way provide complete solutions to the problems.\n",
    "Deadline 3/4/2020\n",
    "\n",
    "Please provide an answer to the following question: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "byP0-Jsv8I0i"
   },
   "source": [
    "# Question 1 (15 pts)\n",
    "Implement the fit and predict procedures for the logistic regression (scikit is not allowed) with norm 2 regularization function (and Lambda parameter).\n",
    "Use as the imput parameters of the gradient ascent the maximum number of iterations (just a constant e.g 100) and the learning factor (e.g. 0.01).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1541,
     "status": "ok",
     "timestamp": 1551824422727,
     "user": {
      "displayName": "Oghenemaro Anuyah",
      "photoUrl": "https://lh3.googleusercontent.com/-aTogxnURc28/AAAAAAAAAAI/AAAAAAAAAAs/eZdkQxssXhk/s64/photo.jpg",
      "userId": "10208576858792423227"
     },
     "user_tz": 420
    },
    "id": "zdKTz3TD8I0j",
    "outputId": "9cc49e94-08cb-4ce3-87b1-5ef840250d74"
   },
   "outputs": [],
   "source": [
    "#This is the sigmoid function. \n",
    "#w = weights\n",
    "#x = data\n",
    "#b= bias\n",
    "\n",
    "def prob(w,x,b):\n",
    "    return 1/(1+np.exp(-((np.dot(w,x)+b))))\n",
    "\n",
    "#Logistic Regression Gradient Ascent\n",
    "#features = the array of data without classes\n",
    "#target = the array of classes correlately to the features\n",
    "#maxIter = number of epochs to train\n",
    "#learn = the learning factor\n",
    "#c = lambda, the weight of the l2 regularization\n",
    "\n",
    "def LRGA(features, target, maxIter, learn, c):\n",
    "    w = np.zeros(features.shape[1]) #takes the shape of the features\n",
    "    b = 0 #the bias\n",
    "    for epoch in range(0,maxIter): #training loop\n",
    "        gradw = np.zeros(features.shape[1]) #resets the gradient for weights to zero\n",
    "        gradb = 0 #resets the gradient for the bias to zero\n",
    "        for x in range (len(features)): #goes through each item to train on\n",
    "            gradw = gradw + (features[x] * (target[x] - prob(w,features[x],b))) #builds the gradient for the weights\n",
    "            gradb = gradb + (target[x] - prob(w,features[x],b)) #builds the gradient for the bias\n",
    "            \n",
    "        w = w + (gradw * learn) + (1/c * w) #applies regularization to the weights, and builds the weight from the gradient based on the learning rate\n",
    "        b = b + (gradb * learn) #builds the bias with respect to the learning rate and gradient\n",
    "    return(w,b) #returns weight and bias\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lyWdLh3B8I0l"
   },
   "source": [
    "# Question 2 (20 pts)\n",
    "Use the iris dataset (just the binary class Iris Setosa vs others), the K-fold cross validation, metrics(accuracy, precision, recall, F1-score) and the logistic regression with L2 regularization.\n",
    "You can use scikit.\n",
    "Please estimate the best parameter C(the inverse of lambda) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Averages for c at 10000\n",
      "Accuracy= 1.0\n",
      "Precision= 1.0\n",
      "Recall= 1.0\n",
      "F1= 1.0\n",
      "\n",
      "Averages for c at 100\n",
      "Accuracy= 1.0\n",
      "Precision= 1.0\n",
      "Recall= 1.0\n",
      "F1= 1.0\n",
      "\n",
      "Averages for c at 10\n",
      "Accuracy= 1.0\n",
      "Precision= 1.0\n",
      "Recall= 1.0\n",
      "F1= 1.0\n",
      "\n",
      "Averages for c at 1\n",
      "Accuracy= 1.0\n",
      "Precision= 1.0\n",
      "Recall= 1.0\n",
      "F1= 1.0\n",
      "\n",
      "Averages for c at 0.1\n",
      "Accuracy= 1.0\n",
      "Precision= 1.0\n",
      "Recall= 1.0\n",
      "F1= 1.0\n",
      "\n",
      "Averages for c at 0.01\n",
      "Accuracy= 1.0\n",
      "Precision= 1.0\n",
      "Recall= 1.0\n",
      "F1= 1.0\n",
      "\n",
      "Averages for c at 0.001\n",
      "Accuracy= 0.6714285714285715\n",
      "Precision= 0.32857142857142857\n",
      "Recall= 0.0\n",
      "F1= 0.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, f1_score, recall_score, average_precision_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.exceptions import UndefinedMetricWarning\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UndefinedMetricWarning) ##used to suppress warnings about F1 scores being zero\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    \n",
    "    #reads in the iris data and preprocesses it into features (X) and classes (y)\n",
    "    data = pd.read_csv('iris.csv',sep=',').values\n",
    "    X=data[:,0:4]\n",
    "    y=data[:,4]\n",
    "    \n",
    "    #processes the data into binary values for the classes\n",
    "    def val(s):\n",
    "        if s=='Iris-setosa':\n",
    "            return 1\n",
    "        return 0\n",
    "    y=np.array([val(x) for x in y])\n",
    "  \n",
    "    #splits the data into k-folds\n",
    "    skf = StratifiedKFold(n_splits=10,random_state=10,shuffle=True)\n",
    "    C = [10000,100,10, 1, .1, .01, .001]\n",
    "    \n",
    "    #test value of C for each set of folds\n",
    "    for c in C:\n",
    "        \n",
    "        print()\n",
    "        fo=1\n",
    "        accuracy = 0\n",
    "        precision = 0\n",
    "        recall = 0\n",
    "        f1 = 0\n",
    "        \n",
    "        #trains, tests, and computes metrics for each fold \n",
    "        for train_index, test_index in skf.split(X, y):\n",
    "            #print('fold=',fo)\n",
    "            X_train=X[train_index]\n",
    "            X_test=X[test_index]\n",
    "            y_train=y[train_index]\n",
    "            y_test=y[test_index]\n",
    "            clf = clf = LogisticRegression(penalty='l2', C=c, solver='liblinear')\n",
    "            clf.fit(X_train,y_train)\n",
    "            pred=clf.predict(X_test)\n",
    "            average_precision = average_precision_score(y_test, pred)\n",
    "            #print('Accuracy', accuracy_score(y_test, pred))\n",
    "            accuracy += accuracy_score(y_test, pred)\n",
    "            #print('Precision:', average_precision_score(y_test, pred))\n",
    "            precision += average_precision_score(y_test, pred)\n",
    "            #print('Recall:',  recall_score(y_test, pred))\n",
    "            recall += recall_score(y_test, pred)\n",
    "            #print('F1-Score:', f1_score(y_test, pred))\n",
    "            f1+= f1_score(y_test, pred)\n",
    "            fo += 1\n",
    "            \n",
    "        #prints the averages of each metric for the folds with lambda of c\n",
    "        print(\"Averages for c at\", c)\n",
    "        print(\"Accuracy=\",accuracy/10)\n",
    "        print(\"Precision=\",precision/10)\n",
    "        print(\"Recall=\",recall/10)\n",
    "        print(\"F1=\", f1/10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I would estimate the best paramater of C for this dataset, using this form of regularization, at some where between the values of .01 and .001. I suspect that anything greater than this value starts to run the risk of over fitting, especially when considering how effective the classifier seems to be. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "PS2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
